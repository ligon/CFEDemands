%2multibyte Version: 5.50.0.2960 CodePage: 1251
%\newtheorem{theorem}{Theorem}
%\newtheorem{axiom}[theorem]{Axiom}
%\newtheorem{conjecture}[theorem]{Conjecture}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{example}[theorem]{Example}
%\newtheorem{exercise}[theorem]{Exercise}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{remark}[theorem]{Remark}


\documentclass[a4paper,notitlepage,thmsb,11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{sw20lart}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{TCIstyle=article/art4.lat,lart,article}

%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{Codepage=1251}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Thu May 13 14:30:27 1999}
%TCIDATA{LastRevised=Tuesday, June 18, 2013 17:12:47}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}
%TCIDATA{CSTFile=article.cst}
%TCIDATA{PageSetup=72,72,72,72,0}
%TCIDATA{Counters=arabic,1}
%TCIDATA{AllPages=
%H=36
%F=36
%}


\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][P
roof]{\textbf{#1.} }{\  \rule{0.5em}{0.5em}}
\input{tcilatex}
\setlength{\evensidemargin}{0.in}
\setlength{\oddsidemargin}{0.in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}

\begin{document}


\begin{center}
{\huge Econometric Model with no endogeneity and using OLS}

%TCIMACRO{\TeXButton{vskip1cm}{\vskip1cm}}%
%BeginExpansion
\vskip1cm%
%EndExpansion
\end{center}

\section{The Model}

\subsection{Econometric specification}

Our economic model is:

\begin{eqnarray}
\log (x_{it}+\phi _{it}) &=&\alpha _{it}+\frac{1}{\gamma _{it}}\log (\lambda
_{it})+\frac{1}{\gamma _{it}}\log (p_{it})  \label{eco_model} \\
&=&\alpha _{it}+\frac{1}{\gamma _{it}}\log (\lambda _{it})+\dsum\limits_{k}%
\frac{1}{k\gamma _{it}}\log (p_{it}^{k}),  \notag
\end{eqnarray}

with already defined notations. We have already imposed some restrictions on
various parameters. 

This lead us to be interested in estimating the parameters ($\alpha _{0}$
and $\phi _{0}$) in the following structural equation, written with
convential econometric notations, with $i=1,...,N;t=1,...,T$ :

\begin{equation}
g(y_{it}+\phi _{0})=x_{it}^{\prime }\alpha _{0}+\eta _{i}+\mu
_{t}+\varepsilon _{it}=x_{it}^{\prime }\alpha _{0}+u_{it}  \label{struct}
\end{equation}

where $g$ is a strictly increasing and strictly concave function of type $%
C^{2}$, $y_{it}$ is an endogenous variables, $x_{it}^{\prime }$ is a $K$ row
vector of exogenous variables, $\eta _{i}$ is an individual random term
stable over time, $\mu _{t}$ is a time random term fixed across individuals
and $\varepsilon _{it}$ is a general error term. The composite error term is
denoted $u_{it}$. The parameter $\alpha _{0}$ can be seen as including
relevant measures of `excess demand elasticities'.

\subsection{First stage}

We now specify the data generating process. To obtain the asymptotic
distribution of our estimator, we will need the following usual regularity
conditions.

%TCIMACRO{\TeXButton{vskip0.5cm}{\vskip0.5cm}}%
%BeginExpansion
\vskip0.5cm%
%EndExpansion

%TCIMACRO{\TeXButton{noindent}{\noindent}}%
%BeginExpansion
\noindent%
%EndExpansion
\textbf{Assumption 1.} (i) The sequence $\{(x_{it}^{\prime },u_{it})\}$\ is $%
\alpha -$mixing with mixing numbers\ $\{\alpha (s)\}$\ of size $-2\left(
4K+1\right) \left( K+1\right) $.\footnote{%
The sequence $\{W_{t}\}$ of random variables is $\alpha -$mixing if $\alpha
(s)$ decreases towards $0$ as $s\rightarrow \infty $, where $\alpha
(s)=\sup_{t}\sup_{A\in F_{-\infty }^{t};B\in F_{t+s}^{\infty }}|P(A\cap
B)-P(A)P(B)|$ for $s\geq 1$ and $F_{s}^{t}$ denote the $\sigma $-field
generated by $(W_{s},\ldots ,W_{t})$ for $-\infty \leq s\leq t\leq \infty $.
The sequence is called $\alpha -$mixing of size $-a$ if $\alpha
(s)=O(s^{-a-\varepsilon })$ for some $\varepsilon >0$.}

\textbf{In fact we may want to start with a much stronger assumption to
explore, for example iid}

%TCIMACRO{\TeXButton{noindent}{\noindent}}%
%BeginExpansion
\noindent%
%EndExpansion
(ii) The sequences $\{(x_{it}^{\prime },u_{it})\}$ are independent across
individuals.\textbf{to relax later to account for stratification and
clustering}

%TCIMACRO{\TeXButton{noindent}{\noindent}}%
%BeginExpansion
\noindent%
%EndExpansion
(iii) Let $F_{it}(.|z)$\ be the conditional cumulative distribution function
(CDF) and $f_{it}(.|)$\ be the conditional probability density function
(PDF) of $u_{it}$. The conditional PDF $f_{it}(\cdot |x)$\ is assumed to be
Lipschitz continuous for all $x$, strictly positive\ and bounded by a
constant $f_{0}$ (i.e., $f_{it}(\cdot |x)<f_{0}$, for all $x)$.

%TCIMACRO{\TeXButton{noindent}{\noindent}}%
%BeginExpansion
\noindent%
%EndExpansion
(iv) $E(u_{it}|x_{it})=0$, for all $i$ and $t$.

%TCIMACRO{\TeXButton{noindent}{\noindent}}%
%BeginExpansion
\noindent%
%EndExpansion
(v)\ The matrix $Q=\lim\limits_{\substack{ T\rightarrow \infty  \\ %
N\rightarrow \infty }}E\left[ \frac{1}{NT}\sum\limits_{i=1}^{N}\sum%
\limits_{t=1}^{T}x_{it}x_{it}^{\prime }\right] $\ is finite and positive
definite.

%TCIMACRO{\TeXButton{noindent}{\noindent}}%
%BeginExpansion
\noindent%
%EndExpansion
(vi) There exists a positive number $C>0$\ such that $E(\left\Vert
x_{it}\right\Vert ^{3})<C<\infty $\ for any $t$.

\textbf{It is probably possible to relax this, in particular when there is
no serial correlations}

%TCIMACRO{\TeXButton{vskip0.5cm}{\vskip0.5cm}}%
%BeginExpansion
\vskip0.5cm%
%EndExpansion

Assumption 1(i) aims at considering as general as possible stationary
dynamic processes. In most cases, much milder conditions are enough

Assumption 1(ii) avoids mixing spatial issues with typical econometric
assumptions. It may be generalised once the main structures of the
asymptotics have been investigated. \textbf{I am not sure it really matters
much}

Assumption 1(iii) simplifies the demonstration of convergence of remainder
terms to zero for the calculation of the asymptotic representation. \textbf{%
Probably possible to relax Lispschitz contitnuity}

Assumption 1(iv) is the assumption that zero is the mean of the conditional
distribution of $u_{it}$. It identifies the coefficients of the model. 

Assumption 1(v) is akin to the usual condition for OLS that the sample
second moment matrix of the regressor vectors converges towards a finite
positive definite matrix. This condition \textbf{may be }necessary for
consistency and for the inversion of the relevant empirical process to
establish the asymptotic normality in the most general considered cases. 
\textbf{It can probably be relaxed for many panel estimators, for example
one may just need the the convergence with respect to i}

Assumption 1(vi), the moment condition on the exogenous variables, is
necessary for the stochastic equicontinuity of our empirical process in the
dependent case, which is used for the asymptotic representation. We also use
it to bound the asymptotic covariance matrix of the parameter estimators.%
\textbf{\ to relax. I am just ensuring a case where I can work safely}

The first stage consists in fixing a value for $\phi _{0}$ and in estimating
(\ref{struct}) using a second-stage estimator $\hat{\alpha}(\phi _{0})$\ for 
$\alpha _{0}$.

%TCIMACRO{\TeXButton{noindent}{\noindent}}%
%BeginExpansion
\noindent%
%EndExpansion
\textbf{Assumption 3} : $\hat{\alpha}(\phi _{0})$ is the OLS estimator in
equation (\ref{struct}).

Under assumption 3, we have

\begin{equation*}
\hat{\alpha}(\phi _{0})=\left[ X^{\prime }X\right] ^{-1}X^{\prime }Y(\phi
_{0}).
\end{equation*}

where $X=\left[ 
\begin{array}{ccc}
x_{11}^{1} & \cdots  & x_{it}^{K} \\ 
\vdots  &  & \vdots  \\ 
x_{1T}^{1} &  &  \\ 
x_{21}^{1} &  &  \\ 
\vdots  &  & \vdots  \\ 
x_{NT}^{1} & \cdots  & x_{NT}^{1}%
\end{array}%
\right] $.and $Y(\phi _{0})=\left[ 
\begin{array}{c}
g(y_{11}+\phi _{0}) \\ 
\vdots  \\ 
g(y_{1T}+\phi _{0}) \\ 
g(y_{21}+\phi _{0}) \\ 
\vdots  \\ 
g(y_{NT}+\phi _{0})%
\end{array}%
\right] $.

Let us now consider the quantile of order $\theta $ of the dependent
variable $g(y_{it}+\phi _{0})\equiv Y_{it}(\phi _{0})$. Assume that, for
each of such quantile, a subsample of size $N$ is drawn from an infinite
subjacent population. Let $Y(\phi _{0},\theta )=Y_{\theta }(\phi _{0})$
denote the corresponding vector of dependent variables, and $X_{\theta }$
are the corresponding matrices of independent variables in the structural
model.

\textbf{NB6: IMPORTANT: I DO NOT INDICATE IT FOR EVITING NOTATION CLUSTER,
BUT THE QUANTILE INDEX }$\theta $\textbf{\ DEPENDS ON THE GIVEN LEVEL OF }$%
\phi $\textbf{. However, this would help a lot the econometric formulae once
we want to looks at slection of quantiles seriously.}

We can then obtain the following estimators by applying OLS to (\ref{struct}%
) for each quantile respectively.

\begin{equation}
\hat{\alpha}(\phi _{0},\theta )=\left[ X_{\theta }^{\prime }X_{\theta }%
\right] ^{-1}X_{\theta }^{\prime }Y(\phi _{0},\theta ).  \label{alpha_theta}
\end{equation}

For a given $\phi _{0}$, this estimator is consistent under A1(iv)-(v). The
corresponding FOCs are first under matrix form:

\begin{equation*}
\left[ X_{\theta }^{\prime }X_{\theta }\right] \hat{\alpha}(\phi _{0},\theta
)=X_{\theta }^{\prime }Y(\phi _{0},\theta ).
\end{equation*}

and then at the observation level:

\begin{center}
$\sum_{t=1}^{T}\sum_{i=1}^{N}x_{\theta it}(Y_{\theta it}(\phi
_{0})-x_{\theta it}^{\prime }\hat{\alpha}(\phi _{0},\theta ))=0$,
\end{center}

i.e. 
\begin{equation*}
\sum_{t=1}^{T}\sum_{i=1}^{N}x_{\theta it}u_{\theta it}(\phi _{0})=0,
\end{equation*}

\bigskip where $u_{\theta it}(\phi _{0})$ is the error term for the quantile 
$\theta $ for a given $(\phi _{0})$\textit{\ .}

\bigskip 

\textbf{Lemma 1: }\textit{Inder Assumption 4(i})\textit{, and other
conditions, the corresponding asymptotic Bahadur representation of the
Pooled-OLS given }$\phi _{0}$ \textit{is}

$T^{1/2}(\hat{\alpha}(\phi _{0},\theta )-\alpha _{0})=Q_{X\theta
}^{-1}\{(NT)^{-1/2}\sum_{t=1}^{T}\sum_{i=1}^{N}x_{\theta it}u_{\theta
it}\}+o_{p}(1)$\textit{,}

\textit{where the matrix }$Q_{Z\theta }=\lim\limits_{\substack{ T\rightarrow
\infty  \\ N\rightarrow \infty }}E\left[ \frac{1}{NT}\sum\limits_{i=1}^{N}%
\sum\limits_{t=1}^{T}x_{\theta it}x_{\theta it}^{\prime }\right] $\textit{\
is assumed to be finite and positive definite and }$u_{\theta it}$ is the
error term for the quantile $\theta $\textit{\ .}

\textbf{to give a quick proof-argument of the inversion of the Focs}

\bigskip

\textbf{to give later: the expansion under various panel conditions, notably
with T fixed and N going to infinity}

\textbf{\bigskip }

\textbf{Lemma 2: }\textit{Under the simple case of iid, the
variance-covariance matrix of }$\hat{\alpha}(\phi _{0},\theta )$\textit{\ is 
}

\begin{equation*}
\mathit{V}\left( \hat{\alpha}(\phi ,\theta )\right) \mathit{=\sigma }^{_{2}}%
\mathit{Q}_{X\theta }^{-1}
\end{equation*}

\textit{A simple estimator of this matrix is the unbiased estimator}

\begin{equation*}
\hat{V}\left( \hat{\alpha}(\phi ,\theta )\right) =\hat{\sigma}^{_{2}}\hat{Q}%
_{X\theta }{}^{-1},
\end{equation*}

\textit{where}%
\begin{equation*}
\hat{\sigma}^{_{2}}=\frac{1}{NT-K}\sum\limits_{i=1}^{N}\sum%
\limits_{t=1}^{T}x_{\theta it}x_{\theta it}^{\prime }
\end{equation*}%
\textit{\ and }%
\begin{equation*}
\hat{Q}_{X\theta }=\frac{1}{NT}\sum\limits_{i=1}^{N}\sum\limits_{t=1}^{T}x_{%
\theta it}x_{\theta it}^{\prime }.
\end{equation*}

\bigskip

\bigskip

\subsection{Second stage}

The parameter $\phi _{0}$ is estimated using%
\begin{equation}
\hat{\phi}=\arg \min_{\phi }\left[ \hat{\alpha}(\phi ,\theta )~-\hat{\alpha}%
(\phi ,\theta ^{\prime })~\right] ^{\prime }\left[ \hat{\alpha}(\phi ,\theta
)~-\hat{\alpha}(\phi ,\theta ^{\prime })\right] ,  \label{eq7}
\end{equation}

The FOCs of (\ref{eq7}), still in matrix form, are:

\begin{equation}
\left[ \hat{\alpha}(\phi ,\theta )~-\hat{\alpha}(\phi ,\theta ^{\prime })~%
\right] ^{\prime }\left[ \frac{\partial \hat{\alpha}(\phi ,\theta )-\hat{%
\alpha}(\phi ,\theta ^{\prime })~}{\partial \phi }\right] =0.
\label{FOC_2nd_stage}
\end{equation}

That is

\begin{equation*}
\sum\limits_{k=1}^{k}\left[ \hat{\alpha}_{k}(\phi ,\theta )~-\hat{\alpha}%
_{k}(\phi ,\theta ^{\prime })~\right] \frac{\partial \left[ \hat{\alpha}%
_{k}(\phi ,\theta )-\hat{\alpha}_{k}(\phi ,\theta ^{\prime })\right] ~}{%
\partial \phi }=0.
\end{equation*}

Now, from (\ref{alpha_theta}), we can calculate

$\frac{\partial \hat{\alpha}(\phi ,\theta )~}{\partial \phi }=\left[
X_{\theta }^{\prime }X_{\theta }\right] ^{-1}X_{\theta }^{\prime }\frac{%
\partial Y(\phi ,\theta )}{\partial \phi }=\left[ X_{\theta }^{\prime
}X_{\theta }\right] ^{-1}X_{\theta }^{\prime }\frac{\partial \left[ 
\begin{array}{c}
g(y_{11}+\phi ) \\ 
\vdots  \\ 
g(y_{11}+\phi )%
\end{array}%
\right] }{\partial \phi }=\left[ X_{\theta }^{\prime }X_{\theta }\right]
^{-1}X_{\theta }^{\prime }\left[ 
\begin{array}{c}
g^{\prime }(y_{11}+\phi ) \\ 
\vdots  \\ 
g^{\prime }(y_{11}+\phi )%
\end{array}%
\right] =\left[ X_{\theta }^{\prime }X_{\theta }\right] ^{-1}X_{\theta
}^{\prime }\nabla g_{\theta }$,

where $g^{\prime }$ denotes the derivative of $g$ and $\nabla g_{\theta }$
its gradient vector. Beware: the gradient of $g$ depends on $\theta $ here
since the$\ Y(\phi ,\theta )$ (or the $y_{it}^{\prime }s)$ correspond to the
quantile $\theta $.

Alternatively if the nuisance parameter $\phi _{0}$ is not additive in $g$,
one has the second partial derivative $g_{2}(y_{11},\phi )$ instead.

\bigskip

Then, (\ref{FOC_2nd_stage}) becomes%
\begin{equation}
\left[ \left[ X_{\theta }^{\prime }X_{\theta }\right] ^{-1}X_{\theta
}^{\prime }Y(\phi ,\theta )-\left[ X_{\theta ^{\prime }}^{\prime }X_{\theta
^{\prime }}\right] ^{-1}X_{\theta ^{\prime }}^{\prime }Y(\phi ,\theta
^{\prime })\right] ^{\prime }\left[ \left[ X_{\theta }^{\prime }X_{\theta }%
\right] ^{-1}X_{\theta }^{\prime }\nabla g_{\theta }-\left[ X_{\theta
^{\prime }}^{\prime }X_{\theta ^{\prime }}\right] ^{-1}X_{\theta ^{\prime
}}^{\prime }\nabla g_{\theta \prime }\right] =0.  \label{2FOC_MAT}
\end{equation}

That is:

$\sum\limits_{k=1}^{k}\left[ \left[ x_{\theta }^{k\prime }x_{\theta }^{k}%
\right] ^{-1}x_{\theta }^{k\prime }Y(\phi ,\theta )-\left[ x_{\theta
^{\prime }}^{k\prime }x_{\theta ^{\prime }}^{k}\right] ^{-1}x_{\theta
^{\prime }}^{k\prime }Y(\phi ,\theta ^{\prime })\right] =0$

i.e.,

$\sum\limits_{k=1}^{k}\left[ \left[ x_{\theta }^{k\prime }x_{\theta }^{k}%
\right] ^{-1}x_{\theta }^{k\prime }Y(\phi _{0},\theta )-\left[ x_{\theta
^{\prime }}^{k\prime }x_{\theta ^{\prime }}^{k}\right] ^{-1}x_{\theta
^{\prime }}^{k\prime }Y(\phi _{0},\theta ^{\prime })\right] =0$

\begin{equation*}
\sum_{t=1}^{T}\sum_{i=1}^{N}\sum\limits_{k=1}^{k}\left[ \left[ x_{it\theta
}^{k}x_{it\theta }^{k}\right] ^{-1}x_{it\theta }^{k\prime }Y_{it}(\phi
,\theta )-\left[ x_{it\theta ^{\prime }}^{k\prime }x_{it\theta ^{\prime
}}^{k}\right] ^{-1}x_{it\theta ^{\prime }}^{k\prime }Y_{it}(\phi ,\theta
^{\prime })\right] =0
\end{equation*}

even choosing\textbf{\ a good form for }$g$\textbf{, that is }$Y\,$\textbf{,
does not allow some explicit solution for }$\phi $\textbf{.}

\bigskip However, we may want to restratify the two samples so that the
values of $X$ are made identical. The restratification looks like
endogenous. But that is still part of our selection/sampling issues. This
will imply at least a last stage of reweighing and correction of the
variance-covariance matrix at the end.

We would obtain:

\begin{equation}
\left[ \left[ X^{\prime }X\right] ^{-1}X^{\prime }\left( Y(\phi ,\theta
)-Y(\phi ,\theta ^{\prime })\right) \right] ^{\prime }\left[ \left[
X^{\prime }X\right] ^{-1}X^{\prime }\left( \nabla g_{\theta }-\nabla
g_{\theta \prime }\right) \right] =0.
\end{equation}

\bigskip 

That is:

\begin{equation}
\left( Y(\phi ,\theta )-Y(\phi ,\theta ^{\prime })\right) ^{\prime }X\left[
X^{\prime }X\right] ^{-1}\left[ X^{\prime }X\right] ^{-1}X^{\prime }\left(
\nabla g_{\theta }-\nabla g_{\theta \prime }\right) =0.
\end{equation}%
\bigskip 

\textbf{\bigskip It is not simplifying: there must be an error to check
later on}

\textbf{I must look at the dimensions of matrices more carefully}

\textbf{\bigskip }%
\begin{equation}
\left( Y(\phi ,\theta )-Y(\phi ,\theta ^{\prime })\right) ^{\prime }X\left[
X^{\prime }X\right] ^{-2}X^{\prime }\left( \nabla g_{\theta }-\nabla
g_{\theta \prime }\right) =0.
\end{equation}

\bigskip 

\bigskip 

\textbf{I must have missed a transposed that would give the follsing
expression after simplificatio}\bigskip 

\begin{equation}
\left( \nabla g_{\theta }-\nabla g_{\theta \prime }\right) ^{\prime }\left[
X^{\prime }X\right] ^{-1}\left( Y(\phi ,\theta )-Y(\phi ,\theta ^{\prime
})\right) =0.
\end{equation}

In any case, once I'll find the mistake we shall end up with

\begin{equation}
\left( Y(\phi ,\theta )-Y(\phi ,\theta ^{\prime })\right) ^{\prime }\Omega
\left( \nabla g_{\theta }-\nabla g_{\theta \prime }\right) =0.
\end{equation}

where $\Omega $\ \ is a block-diagonal matrix that can be computed from the
observations of the independent variable.

When using our economic functional form, we have:

\begin{equation}
\left[ 
\begin{array}{c}
\log (\frac{y_{\theta 11}+\phi }{y_{\theta ^{\prime }11}+\phi }) \\ 
\vdots  \\ 
\log (\frac{y_{\theta NT}+\phi }{y_{\theta ^{\prime }NT}+\phi })%
\end{array}%
\right] ^{\prime }\Omega \left[ 
\begin{array}{c}
\frac{1}{y_{\theta 11}+\phi }-\frac{1}{y_{\theta ^{\prime }11}+\phi } \\ 
\vdots  \\ 
\frac{1}{y_{\theta NT}+\phi }-\frac{1}{y_{\theta ^{\prime }NT}+\phi }%
\end{array}%
\right] =0,
\end{equation}

which yields by DROPPING THE TIME INDEX to simplify formulae:%
\begin{equation}
C=\sum\limits_{i=1}^{N}w_{ij}\log (\frac{y_{\theta i}+\phi }{y_{\theta
^{\prime }i}+\phi })\left( \frac{1}{y_{\theta i}+\phi }-\frac{1}{y_{\theta
^{\prime }i}+\phi }\right) =0.  \label{explicit_focs}
\end{equation}

where the weights $w_{ij}$ are extracted from matrix $\Omega $. 

\textbf{NB: Later on, we'll have a series of equations for all the
coefficients of variables explaining }$\phi $

One interest of (\ref{explicit_focs}), is that the gradient and the Hessian
matrix can be easily calculated explicitly. Then, the search for the root of
(\ref{explicit_focs}), can be based on an explicit Newton method. In that
case, each iteration of the Newton method would correspond to solving a
quadratic equation, which can be done explicitly. Moroever, it should be
possible to express the solution of this quadratic equation as the solution
of an artificial OLS problem. This would allow the use of standard
regression programs to provide solutions at each iteration step.

If one iteration gives good results, which is sometimes the case in such
estimation problems, then we would have an explicit formula for the
estimator (the 'one-step estimator approach'). Simulations may clarify if
one iteration is enough. One may also study if this estimator is
asymptotically equivalent to solving (\ref{explicit_focs}).

The calculus yields

$\nabla C=\frac{\partial C}{\partial \phi }=\sum\limits_{i=1}^{N}w_{ij}\left%
\{ \left( \frac{1}{y_{\theta i}+\phi }-\frac{1}{y_{\theta ^{\prime }i}+\phi }%
\right) ^{2}+\log (\frac{y_{\theta i}+\phi }{y_{\theta ^{\prime }i}+\phi }%
)\left( \frac{1}{(y_{\theta ^{\prime }i}+\phi )^{2}}-\frac{1}{(y_{\theta
i}+\phi )^{2}}\right) \right\} .$

$HC=\sum\limits_{i=1}^{N}w_{ij}\left\{ 2\left( \frac{1}{y_{\theta i}+\phi }-%
\frac{1}{y_{\theta ^{\prime }i}+\phi }\right) \left( \frac{1}{(y_{\theta
^{\prime }i}+\phi )^{2}}-\frac{1}{(y_{\theta i}+\phi )^{2}}\right) +\left( 
\frac{1}{y_{\theta i}+\phi }-\frac{1}{y_{\theta ^{\prime }i}+\phi }\right)
\left( \frac{1}{(y_{\theta ^{\prime }i}+\phi )^{2}}-\frac{1}{(y_{\theta
i}+\phi )^{2}}\right) +2\log (\frac{y_{\theta i}+\phi }{y_{\theta ^{\prime
}i}+\phi })\left( \frac{1}{(y_{\theta i}+\phi )^{3}}-\frac{1}{(y_{\theta
^{\prime }i}+\phi )^{3}}\right) \right\} .$

$=\sum\limits_{i=1}^{N}w_{ij}\left\{ 3\left( \frac{1}{y_{\theta i}+\phi }-%
\frac{1}{y_{\theta ^{\prime }i}+\phi }\right) \left( \frac{1}{(y_{\theta
^{\prime }i}+\phi )^{2}}-\frac{1}{(y_{\theta i}+\phi )^{2}}\right) +2\log (%
\frac{y_{\theta i}+\phi }{y_{\theta ^{\prime }i}+\phi })\left( \frac{1}{%
(y_{\theta i}+\phi )^{3}}-\frac{1}{(y_{\theta ^{\prime }i}+\phi )^{3}}%
\right) \right\} $

$=\sum\limits_{i=1}^{N}w_{ij}\left\{ 3\left( \frac{1}{y_{\theta i}+\phi }-%
\frac{1}{y_{\theta ^{\prime }i}+\phi }\right) \left( \frac{1}{(y_{\theta
^{\prime }i}+\phi )^{2}}-\frac{1}{(y_{\theta i}+\phi )^{2}}\right) +2\log (%
\frac{y_{\theta i}+\phi }{y_{\theta ^{\prime }i}+\phi })\left( \frac{1}{%
(y_{\theta i}+\phi )^{3}}-\frac{1}{(y_{\theta ^{\prime }i}+\phi )^{3}}%
\right) \right\} $

and $\left( \frac{1}{y_{\theta i}+\phi }-\frac{1}{y_{\theta ^{\prime
}i}+\phi }\right) $ can even be factored in each curly bracket if needed.

A second-order Taylor expansion of (\ref{explicit_focs}) around an initial
iteration value $\phi _{n}$ gives the condition for calculation the next
value $\phi _{n+1}$.

\bigskip 

$C_{\phi _{n+1}}-C_{\phi _{n}}\simeq \nabla C(\phi _{n+1}-\phi _{n})+(\phi
_{n+1}-\phi _{n})^{\prime }H(\phi _{n+1}-\phi _{n})\simeq 0$, where the
gradient and the Hessian at calculated at $\phi _{n}$. We obtain the
recurrence relation $\phi _{n+1}=\phi _{n}-H^{-1}\nabla C$.

\bigskip 

Then, a `one-step estimator' is 

\begin{equation}
\hat{\phi}=\phi _{0}-\frac{\sum\limits_{i=1}^{N}w_{ij}\left\{ \left( \frac{1%
}{y_{\theta i}+\phi _{0}}-\frac{1}{y_{\theta ^{\prime }i}+\phi _{0}}\right)
^{2}+\log (\frac{y_{\theta i}+\phi _{0}}{y_{\theta ^{\prime }i}+\phi _{0}}%
)\left( \frac{1}{(y_{\theta ^{\prime }i}+\phi _{0})^{2}}-\frac{1}{(y_{\theta
i}+\phi _{0})^{2}}\right) \right\} }{\sum\limits_{i=1}^{N}w_{ij}\left\{
3\left( \frac{1}{y_{\theta i}+\phi _{0}}-\frac{1}{y_{\theta ^{\prime
}i}+\phi _{0}}\right) \left( \frac{1}{(y_{\theta ^{\prime }i}+\phi _{0})^{2}}%
-\frac{1}{(y_{\theta i}+\phi _{0})^{2}}\right) +2\log (\frac{y_{\theta
i}+\phi _{0}}{y_{\theta ^{\prime }i}+\phi _{0}})\left( \frac{1}{(y_{\theta
i}+\phi _{0})^{3}}-\frac{1}{(y_{\theta ^{\prime }i}+\phi _{0})^{3}}\right)
\right\} },  \label{1step_estimator}
\end{equation}

where $\phi _{0}$ is an inital value, for example based on the value of
nutritional minima.

The 'several steps estimator' is denoted $\tilde{\phi}$. 

\textbf{I use matrix notation to prepare the passage in matrices later}

\textbf{Il faudra tout generaliser en matriciel avec un phi incluant
plusieurs variables}

\subsubsection{Contrafactual-based estimation\protect\bigskip }

Instead of using post-stratification, which may be hard to deal with if the
post-sampling has endogenous characteristics, one may want instead to
consider contra-factual situations defined by given values for $X$. In that
case, the estimators $\hat{\alpha}(\phi ,\theta )$ and $\hat{\alpha}(\phi
,\theta ^{\prime })$ still correspond to the two chosen quantiles. 

\textbf{We need an argument to say that these estimators are consistent. For
example, there is a two-stage Heckman estimator instead of OLS}

An implicit restriction is that $\alpha _{0}$ does not depend on the
quantiles. However, additionally we want the error terms $u_{it}$ not to be
correlated with $y_{it}$, to ensure the consistency of the two estimators to
compare. If that is the case, we assume that the model includes through the $%
x_{it}$ a control function that is sufficient to deal with such
endogeneity/selectivity issues. Endogeneity issues will be dealt with in
detail later through instrumental variable methods.

Under these assumptions, instead of comparing the two estimators $\hat{\alpha%
}(\phi ,\theta )$ and $\hat{\alpha}(\phi ,\theta ^{\prime })$ directly, we
compare the respective model predictions for a contra-factual matrix $\bar{X}
$. That is we consider the following minimal-distance problem:

\begin{equation}
\hat{\phi}=\arg \min_{\phi }\left[ \hat{\alpha}(\phi ,\theta )~-\hat{\alpha}%
(\phi ,\theta ^{\prime })~\right] ^{\prime }\bar{X}^{\prime }\bar{X}\left[ 
\hat{\alpha}(\phi ,\theta )~-\hat{\alpha}(\phi ,\theta ^{\prime })\right] ,
\label{Mindistance_contrafact}
\end{equation}

\textbf{NB: There may be some metric that would make the influence of X's
disappear. For example the OLS difference variance may incorporate }$\left(
X^{\prime }X\right) ^{-1}$\textbf{, as a Core component. Then, since we are
in the case of the scalar we could use the circular invariance of the Trace
to eliminate the X's. To check}

If the X's are eliminated we are back to our previous estimation procedure.

If the X's do not disappear, we can calculate the new estimator explicitly
too. This is likely to change the weights $w_{ij}$ only. \textbf{To check}

In that case, we shall be able to use this counter-factual anchor $\bar{X}$\
as an additional freedom of choice in our problem.

\subsection{Third stage}

Once $\hat{\phi}$ is obtained, one use again the OLS with this initial
estimate to obtain

\begin{equation*}
\tilde{\alpha}(\phi _{0})=\left[ X^{\prime }X\right] ^{-1}X^{\prime }Y(\hat{%
\phi}).
\end{equation*}

We now study the asymptotic properties of our estimator in the next section.

\section{The Asymptotic Representation}

\textbf{The difficulty is to plug the asymptotic representation of }$\hat{%
\phi}$\textbf{\ in that of the OLS. For this, we need to construct a
pass-through device using Empirical processes.}

To derive the asymptotic representation of our estimator, we define the
following empirical process.

\begin{equation*}
M_{N,T}(\Delta
)=(NT)^{-1/2}~\sum\limits_{i=1}^{N}\sum_{t=1}^{T}x_{it}h_{\theta
}(u_{it}-T^{-1/2}\Delta ),
\end{equation*}

where $\Delta $ is a ($K_{1}+G)\times 1$ vector. 

\section{The Introduction of Measurement Errors}

\begin{eqnarray}
g(y_{it}+\phi _{0}+\zeta _{it}) &=&x_{1it}^{\prime }\beta
_{0}+Y_{it}^{\prime }\gamma _{0}+u_{it} \\
&=&z_{1it}^{\prime }\alpha _{0}+\eta _{i}+\mu _{t}+\varepsilon _{it}  \notag
\end{eqnarray}

\bigskip 

where $\zeta _{it}$ is an asymmetric measurement error, possibly correlated
over time. This would considerably complicate the theoretical analysis of
the estimator. Therefore, we shall deal with it in an ad hoc manner.

\bigskip 

\bigskip 

\textbf{However, simulated moment methods would allow us to extend the
estimation techniques to the introduction of measurement errors.\ The fact
that the error is asymmetric may be an issue}

\bigskip 

\end{document}
