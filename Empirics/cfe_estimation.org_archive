#    -*- mode: org -*-


Archived entries from file /home/ligon/Research/VESDemand/Empirics/neediness.org


* COMMENT Brand (2006) algorithm for iterative rank-1 SVD updates
  :PROPERTIES:
  :ARCHIVE_TIME: 2017-01-17 Tue 08:03
  :ARCHIVE_FILE: ~/Research/VESDemand/Empirics/neediness.org
  :ARCHIVE_OLPATH: Rank 1 SVD with Missing Data
  :ARCHIVE_CATEGORY: neediness
  :END:

This is supposed to handle rank one updates to an SVD decomposition of
X, with usv.T=X.  However, its calculation of the right-singular
vectors v seems entirely broken, and its use otherwise suspect (this
is probably the  fault of the implementation rather than of the
algorithm). 
#+name: brand2006
#+begin_src python :noweb no-export :results output :tangle neediness.py
  """
  Adapted from code in version 0.7.4 of gensim
  (https://pypi.python.org/pypi/gensim/0.7.4).  That code is licensed
  under the LGPL (http://www.gnu.org/licenses/lgpl.html).  I assert that
  this notice satisfies the requirements imposed on on this work
  enumerated in Section 5 ("Combined Libraries") of the LGPL.
  """ 

  import logging 
  import numpy
  np=numpy

  logger = logging.getLogger('lsimodel')
  logger.setLevel(logging.INFO)



  def svdUpdate(U, S, V, a, b):
      """
      Update SVD of an (m x n) matrix `X = U * S * V^T` so that
      `[X + a * b^T] = U' * S' * V'^T`
      and return `U'`, `S'`, `V'`.

      The original matrix X is not needed at all, so this function implements one-pass
      streaming rank-1 updates to an existing decomposition. 

      `a` and `b` are (m, 1) and (n, 1) matrices.

      You can set V to None if you're not interested in the right singular
      vectors. In that case, the returned V' will also be None (saves memory).

        This is the rank-1 update as described in
      ,**Brand, 2006: Fast low-rank modifications of the thin singular value decomposition**,
      but without separating the basis from rotations.
      """

      def fixmiss(c,S,U):
          """Interpolate to deal with missing values in vector c."""

          miss=np.isnan(c).nonzero()[0]
          nonmiss=(~np.isnan(c)).nonzero()[0]

          B=S*np.linalg.pinv(U[nonmiss,:]*S)*c[nonmiss]
          p = c[nonmiss] - U[nonmiss,:]*B
          Ra = np.linalg.norm(p)
          if len(miss)>0:
            chat=U[miss,:]*S*B
          else:
            chat = c

      return chat,p,Ra

      # convert input to matrices (no copies of data made if already numpy.ndarray or numpy.matrix)
      S = numpy.asmatrix(S)
      U = numpy.asmatrix(U)
      if V is not None:
          V = numpy.asmatrix(V)

    
      b = numpy.asmatrix(b).reshape(b.size, 1)

      rank = S.shape[0]

      # eq (6)
      a,p,Ra = fixmiss(numpy.asmatrix(a).reshape(a.size, 1),S,U)
      #m = U.T * a   # These are for the non-missing case
      #p = a - U * m
      #Ra = numpy.sqrt(p.T * p)
      if float(Ra) < 1e-10:
          logger.debug("input already contained in a subspace of U; skipping update")
          return U, S, V
      P = (1.0 / float(Ra)) * p

      if V is not None:
          # eq (7)
          n = V.T * b
          q = b - V * n
          Rb = numpy.sqrt(q.T * q)
          if float(Rb) < 1e-10:
              logger.debug("input already contained in a subspace of V; skipping update")
              return U, S, V
          Q = (1.0 / float(Rb)) * q
      else:
          n = numpy.matrix(numpy.zeros((rank, 1)))
          Rb = numpy.matrix([[1.0]])    

      if float(Ra) > 1.0 or float(Rb) > 1.0:
          logger.debug("insufficient target rank (Ra=%.3f, Rb=%.3f); this update will result in major loss of information"
                        % (float(Ra), float(Rb)))

      # eq (8)
      K = numpy.matrix(numpy.diag(list(numpy.diag(S)) + [0.0])) + numpy.bmat('m ; Ra') * numpy.bmat('n ; Rb').T

      # eq (5)
      u, s, vt = numpy.linalg.svd(K, full_matrices = False)
      tUp = numpy.matrix(u[:, :rank])
      tVp = numpy.matrix(vt.T[:, :rank])
      tSp = numpy.matrix(numpy.diag(s[: rank]))
      Up = numpy.bmat('U P') * tUp
      if V is not None:
          Vp = numpy.bmat('V Q') * tVp
      else:
          Vp = None
      Sp = tSp

      return Up, Sp, Vp
#+end_src



* Modified IncPACK algorithm 
  :PROPERTIES:
  :ARCHIVE_TIME: 2017-02-15 Wed 10:14
  :ARCHIVE_FILE: ~/Research/VESDemand/Empirics/neediness.org
  :ARCHIVE_OLPATH: Rank 1 SVD with Missing Data/Rank 1 SVD Approximation to Matrix with Missing Data
  :ARCHIVE_CATEGORY: neediness
  :END:
This relies on a modification to the interative SVD algorithm =IncPACK=.
#+name: svd_missing
#+begin_src python :noweb no-export :results output :tangle svd_missing.py 
  import numpy as np
  from oct2py import Oct2Py, Oct2PyError
  octave=Oct2Py()
  octave.addpath('../utils/IncPACK/')
  octave.addpath('../utils/nan-3.1.1/')

  def mysvd(X):
      """Wrap np.linalg.svd so that output is "thin" and X=usv.T.
      """
      u,s,vt = np.linalg.svd(X,full_matrices=False)
      s=np.diag(s)
      v = vt.T
      return u,s,v

  def svd_missing(X):
      [u,s,v]=octave.svd_missing(X.as_matrix())
      s=np.matrix(s)
      u=np.matrix(u)
      v=np.matrix(v)
      return u,s,v

#+end_src

* Order matters when data is missing!                              :noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2017-03-14 Tue 15:39
  :ARCHIVE_FILE: ~/Research/CFEDemands/Empirics/cfe_estimation.org
  :ARCHIVE_OLPATH: Rank 1 SVD with Missing Data/Rank 1 SVD Approximation to Matrix with Missing Data
  :ARCHIVE_CATEGORY: cfe_estimation
  :END:
By moving observations with  more missing data to the "end" of the
thin matrix, we (invariably?) obtain better approximations if we use
the updating SVD approach.

#+BEGIN_SRC python :var percent_missing=0.2 :var N=20 :tangle foo.py
  <<df_utils>>
  <<svd_rank1_approximation_with_missing_data>>

  def random_rank1_matrix(n=100,m=2):
      a=np.random.random(size=(n,1))
      a=a/np.linalg.norm(a)
      b=np.random.random(size=(m,1)).T
      b=b/np.linalg.norm(b)

      return a.dot(b)

  np.random.seed(0)
  d=[]
  
  for i in range(N):
      #print i
      X0=pd.DataFrame(random_rank1_matrix(n=100,m=4))
      X=X0.copy()
      X.iloc[np.random.random_sample(X.shape)<percent_missing]=np.nan

      Xhat0=svd_rank1_approximation_with_missing_data(X,return_usv=False,VERBOSE=False,MISSLAST=False)
      Xhat1=svd_rank1_approximation_with_missing_data(X,return_usv=False,VERBOSE=False,MISSLAST=True)

      d.append(df_norm(Xhat0-X0)/np.linalg.norm(X0) - df_norm(Xhat1-X0)/np.linalg.norm(X0))

  d=np.array(d)
  print("Proportion (out of %d) for which approximation is better with missing values last is %5.4f." % (N,np.mean(d>0)))

#+END_SRC

#+results:
: Proportion (out of 20) for which approximation is better with missing values last is 1.0000.

    

* Tuning truncation parameter for missing-value svds
  :PROPERTIES:
  :ARCHIVE_TIME: 2017-03-14 Tue 15:40
  :ARCHIVE_FILE: ~/Research/CFEDemands/Empirics/cfe_estimation.org
  :ARCHIVE_OLPATH: Extraction of Frisch Elasticities and Neediness
  :ARCHIVE_CATEGORY: cfe_estimation
  :END:

By extracting estimates of \phi\beta from the covariance matrix, we
ensure that those estimates depend on missing data only to the extent
that the covariance matrix itself depends on  those data.  If we
assume that data is missing at random, then the naive estimator
implemented in =pandas= will be consistent, though in finite samples
it may not be positive definite.  

#+BEGIN_SRC python :tangle missing_svd_tuning.py
  <<svd_rank1_approximation_with_missing_data>>
  <<get_loglambdas>>

  x=pd.read_pickle('../Results/Uganda/Eig/ce.df') # Residuals from reduced form

  B=[pd.read_pickle('../Results/Uganda/goods.df')[r'$\phi\beta_i$']]
  L=[pd.read_pickle('../Results/Uganda/loglambda.df').stack()]
  for r in range(1,11):
      print("Max rank for missing inference: %d" % r)
      b,l=get_loglambdas(x,TEST=True,max_rank=r)
      B.append(b)
      L.append(l)
  
  L=pd.concat(L,axis=1,names=range(11))
  B=pd.concat(B,axis=1,names=range(11))

#+END_SRC


* xarray-based Data Generating Process (replaced by pandas version)
  :PROPERTIES:
  :ARCHIVE_TIME: 2026-01-30
  :ARCHIVE_FILE: ~/Research/CFEDemands/Empirics/cfe_estimation.org
  :ARCHIVE_OLPATH: Monte Carlo Data Generating Process
  :ARCHIVE_CATEGORY: cfe_estimation
  :END:

The following xarray-based DGP code was replaced by a pandas-only
implementation to remove the xarray dependency.

** lambdas_dgp
#+BEGIN_SRC python :tangle no
from scipy.stats.distributions import lognorm
import numpy as np

import warnings
with warnings.catch_warnings():
    warnings.filterwarnings("ignore",category=UserWarning)
    import xarray as xr


def geometric_brownian(sigma=1.):
    return lognorm(s=sigma,scale=np.exp(-(sigma**2)/2))

def lambdabar(T,M,Fbar):
    return xr.DataArray(np.cumprod(Fbar.rvs(size=(T,M)),axis=0),
                        dims=('t','m'),
                        coords={'t':range(T),'m':range(M)})

def lambdas(N,T,M=1,G0=lognorm(.5),Fbar=geometric_brownian(.1),F=geometric_brownian(.2)):

    L0 = xr.DataArray(G0.rvs(size=(N,1,M)),dims=('j','t','m'),
                    coords={'j':range(N),'t':range(1),'m':range(M)})
    innov = xr.DataArray(F.rvs(size=(N,T-1,M)),dims=('j','t','m'),
                             coords={'j':range(N),'t':range(1,T),'m':range(M)})

    L = xr.concat((L0,innov),dim='t').transpose('j','t','m')

    return L*lambdabar(T,M,Fbar=Fbar)
#+END_SRC

** characteristics_dgp
#+BEGIN_SRC python :tangle no
def characteristics(N,T,M=1):
    z = lambdas(N,T,M,Fbar=geometric_brownian(.05),F=geometric_brownian(0.1))
    return z
#+END_SRC

** prices_dgp
#+BEGIN_SRC python :tangle no
def prices(T,M,n,G0=lognorm(.5),Fbar=geometric_brownian(.05),F=geometric_brownian(.2)):

    P0 = xr.DataArray(G0.rvs(size=(n,1,M)),dims=('i','t','m'),
                      coords={'i':range(n),'t':range(1),'m':range(M)})
    innov = xr.DataArray(F.rvs(size=(n,T-1,M)),dims=('i','t','m'),
                               coords={'i':range(n),'t':range(1,T),'m':range(M)})

    P = xr.concat((P0,innov),dim='t').transpose('t','m','i')

    return P*lambdabar(T,M,Fbar=Fbar)
#+END_SRC

** measurement_error_dgp
#+BEGIN_SRC python :tangle no
import pandas as pd
from scipy.stats import distributions
import numpy as np

def measurement_error(N,T,M,n,mu_phi=0.,sigma_phi=0.,mu_eps=0.,sigma_eps=1.):
    def additive_error(N=N,T=T,M=M,n=n,sigma=sigma_phi):
        return xr.DataArray(distributions.norm.rvs(scale=sigma,size=(N,T,M,n)) + mu_phi,dims=('j','t','m','i'))

    def multiplicative_error(N=N,T=T,M=M,n=n,sigma=sigma_eps):
        return xr.DataArray(np.exp(distributions.norm.rvs(loc=-sigma/2.,scale=sigma,size=(N,T,M,n)) + mu_eps),dims=('j','t','m','i'))

    phi=additive_error(N,T,M,n,sigma=sigma_phi)
    eps=multiplicative_error(N,T,M,n,sigma=sigma_eps)

    return phi,eps
#+END_SRC

** expenditures_dgp
#+BEGIN_SRC python :tangle no
def expenditures(N,T,M,n,k,beta,mu_phi=0,sigma_phi=0.,mu_eps=0,sigma_eps=0.,Fbar=geometric_brownian(.001),p=None,rho_lz=0):

    if len(beta.shape)<2:
        Theta=xr.DataArray(np.diag(beta),dims=('i','ip'))
    else:
        Theta=xr.DataArray(beta,dims=('i','ip'))

    beta=Theta.sum('ip')

    l = lambdas(N,T,M,Fbar=Fbar)

    foo = xr.DataArray(data=[chr(i) for i in range(ord('a'),ord('a')+k)],name='k',dims='k')

    z = xr.concat([characteristics(N,T,M) for i in range(k)],dim=foo)

    l = l*(z**rho_lz).prod('k')

    L = np.reshape(l,(N,T,M))

    if p is None:
        p = prices(T,M,n)

    x = np.log(L)*(-beta)
    x = x + np.log(p) - (Theta*np.log(p)).sum('ip')
    x = x + np.log(z).sum('k')

    x = np.exp(x)

    phi,e=measurement_error(N,T,M,n,mu_phi=mu_phi,sigma_phi=sigma_phi,mu_eps=mu_eps,sigma_eps=sigma_eps)

    truth = xr.Dataset({'beta':beta,'lambdas':l,'characteristics':z,'prices':p,'x0':x})

    x = (x + p*phi)
    x = x*e

    x = x*(x>0)

    return x,truth
#+END_SRC
