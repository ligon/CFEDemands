:SETUP:
#+TITLE: Code and Methods for Estimating Constant Frisch Elasticity Demand Systems
#+AUTHOR: Ethan Ligon
#+OPTIONS: toc:nil
#+PROPERTY: header-args:python :results output :noweb no-export :exports code :comments link :prologue (format "# Tangled on %s" (current-time-string))
#+LATEX_HEADER: \renewcommand{\vec}[1]{\boldsymbol{#1}}
#+LATEX_HEADER: \newcommand{\T}{\top}
#+LATEX_HEADER: \newcommand{\E}{\ensuremath{\mbox{E}}}
#+LATEX_HEADER: \newcommand{\R}{\ensuremath{\mathbb{R}}}
#+LATEX_HEADER: \newcommand{\Cov}{\ensuremath{\mbox{Cov}}}
#+LATEX_HEADER: \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LATEX_HEADER: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
#+LATEX_HEADER: \addbibresource{main.bib}\renewcommand{\refname}{}
#+LATEX_HEADER: \addbibresource{ligon.bib}
#+LATEX_HEADER: \usepackage{stringstrings}\renewcommand{\cite}[1]{\caselower[q]{#1}\citet{\thestring}}
:END:
* Introduction
  This document describes a set of empirical methods for estimating
  demands of a form we'll call "Constant Frisch Elasticity," and
  also provides =python= code which implements these methods.  
  Both this document and the code it contains are licensed in a manner
  which permits non-commercial reuse (see
  https://creativecommons.org/licenses/by-nc-sa/4.0/), subject to a 
  "attribution" requirement that you give appropriate credit to the
  present author.  In the case of uses of the code or methods in an
  written work such as a journal article, I ask that attribution
  include citation of any relevant refereed publications.

* Expenditure Shares

These functions allow the computation and visualization of variation
in expenditure shares.  The principal input is a =pd.DataFrame= with
columns corresponding to different expenditure items, and rows
corresponding to period-households, indexed by $(t,j)$.

#+name: agg_shares_and_mean_shares
#+begin_src python :exports none :tangle ../cfe/estimation.py
  import pylab as pl 
  import pandas as pd
  import numpy as np
  from cfe.df_utils import broadcast_binary_op

  def expenditure_shares(df):

      aggshares=df.groupby(level='t').sum()
      aggshares=aggshares.div(aggshares.sum(axis=1),axis=0).T
      meanshares=df.div(df.sum(axis=1),level='j',axis=0).groupby(level='t').mean().T

      mratio=(np.log(aggshares)-np.log(meanshares))
      sharesdf=pd.Panel({'Mean shares':meanshares,'Agg. shares':aggshares})

      return sharesdf,mratio

  def agg_shares_and_mean_shares(df,figname=None,ConfidenceIntervals=False,ax=None,VERTICAL=False):
      """Figure of log agg shares - log mean shares.

      Input df is a pd.DataFrame of expenditures, indexed by (t,j).

      ConfidenceIntervalues is an optional argument.  
      If True, the returned figure will have 95% confidence intervals.  
      If a float in (0,1) that will be used for the size of the confidence 
      interval instead.
      """

      shares,mratio=expenditure_shares(df)
      meanshares=shares['Mean shares']

      tab=shares.to_frame().unstack()
      tab.sort_values(by=('Agg. shares',meanshares.columns[0]),ascending=False,inplace=True)

      if ax is None:
          fig, ax = pl.subplots()

      mratio.sort_values(by=mratio.columns[0],inplace=True)

      if VERTICAL:
          ax.plot(mratio.as_matrix(),range(mratio.shape[0]), 'o')
          ax.legend(mratio.columns,loc=2)
          ax.set_xlabel('Log Aggregate shares divided by Mean shares')
          ax.set_yticks(range(mratio.shape[0]))
          ax.set_yticklabels(mratio.index.values.tolist(),rotation=0)
          ax.axvline()
          v = ax.axis()
          ax.figure.set_figheight((v[-1]/24)*6)
          pl.tight_layout()
      else:
          ax.plot(range(mratio.shape[0]),mratio.as_matrix(), 'o')
          ax.legend(mratio.columns,loc=2)
          ax.set_ylabel('Log Aggregate shares divided by Mean shares')

          v=ax.axis()

          if  len(mratio)>=12:
              i=0
              for i in range(len(mratio)):
                  name=mratio.ix[i].name # label of expenditure item

                  if mratio.iloc[i,0]>0.2:
                      #pl.text(i,mratio.T.iloc[0][name],name,fontsize='xx-small',ha='right')

                      # The key option here is `bbox`. 
                      ax.annotate(name, xy=(i,mratio.T.iloc[0][name]), xytext=(-20,10), 
                                  textcoords='offset points', ha='right', va='bottom',
                                  bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3),
                                  arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.25', 
                                  color='red'),fontsize='xx-small')

                  if mratio.iloc[i,0]<-0.2:
                      #pl.text(i,mratio.T.iloc[0][name],name,fontsize='xx-small')
                      ax.annotate(name, xy=(i,mratio.T.iloc[0][name]), xytext=(20,-10), 
                                  textcoords='offset points', ha='left', va='top',
                                  bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3),
                                  arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.25', 
                                  color='red'),fontsize='xx-small')
          else: #Put labels on xaxis
              ax.set_xticklabels(mratio.index.values.tolist(),rotation=45)

          ax.axhline()

    

      if ConfidenceIntervals>0: # Bootstrap some confidence intervals
          if ConfidenceIntervals==1: ConfidenceIntervals=0.95
          current=0
          last=1
          M=np.array([],ndmin=3).reshape((mratio.shape[0],mratio.shape[1],0))
          i=0
          mydf=df.loc[:,mratio.index]
          while np.max(np.abs(current-last))>0.001 or i < 1000:
              last=current
              # Sample households in each  round with replacement
              bootdf=mydf.iloc[np.random.random_integers(0,df.shape[0]-1,df.shape[0]),:]
              bootdf.reset_index(inplace=True)
              bootdf['j']=range(bootdf.shape[0])
              bootdf.set_index(['t','j'],inplace=True)
              shares,mr=expenditure_shares(bootdf)
              M=np.dstack((M,mr.as_matrix()))
              M.sort(axis=2)
              a=(1-ConfidenceIntervals)/2.
              lb= mratio.as_matrix() - M[:,:,int(np.floor(M.shape[-1]*a))]
              ub=M[:,:,int(np.floor(M.shape[-1]*(ConfidenceIntervals+a)))] - mratio.as_matrix()
              current=np.c_[lb,ub]
              i+=1

          T=mratio.shape[1]
          for t in range(T):
              if VERTICAL:
                  ax.errorbar(mratio.as_matrix()[:,t],np.arange(mratio.shape[0]),xerr=current[:,[t,t-T]].T.tolist())
              else:
                  ax.errorbar(np.arange(mratio.shape[0]),mratio.as_matrix()[:,t],yerr=current[:,[t,t-T]].T.tolist())

              tab[(df.index.levels[0][t],'Upper Int')]=current[:,t-T]
              tab[(df.index.levels[0][t],'Lower Int')]=current[:,t]

      if figname:
          pl.savefig(figname)

      return tab,ax
#+end_src

#+name: group_expenditures
#+begin_src python :noweb yes :tangle ../cfe/estimation.py
def group_expenditures(df,groups):
    myX=pd.DataFrame(index=df.index)
    for k,v in groups.iteritems():
        myX[k]=df[['$x_{%d}$' % i for i in v]].sum(axis=1)
            
    return myX
#+end_src

* Rank 1 SVD Approximation to Matrix with Missing Data
** Eigenvalue Decomposition Approach to Computing the SVD
Here's an approach that involves estimating a covariance matrix, and
extracting $U$ and $\Sigma$ from that; then backing out estimated $V$,
for columns satisfying a rank condition.  Unlike some alternative
approaches estimates do not depend on the order of observations.  

A parameter =min_obs= governs how  conservative the algorithm is in
estimating the covariance matrix; it's equal to  minimum number of
cross-products required to  estimate an element of that matrix.  Note
that if this is a small integer one is more apt to obtain estimates
of the covariance matrix which are  not positive definite.
#+name: svd_missing
#+BEGIN_SRC python
  import numpy as np

  def missing_inner_product(X,min_obs=None):
    n,m=X.shape

    if n<m: 
        axis=1
        N=m
    else: 
        axis=0
        N=n

    xbar=X.mean(axis=axis)

    if axis:
        C=(N-1)*X.T.cov(min_periods=min_obs)
    else:
        C=(N-1)*X.cov(min_periods=min_obs)

    return C + N*np.outer(xbar,xbar)

  def drop_columns_wo_covariance(X,min_obs=None,VERBOSE=False):
      """Drop columns from pd.DataFrame that lead to missing elements of covariance matrix."""

      m,n=X.shape
      assert(m>n)

      HasMiss=True
      while HasMiss:
          foo = X.cov(min_periods=min_obs).count()
          if np.sum(foo<X.shape[1]):
              badcol=foo.argmin()
              del X[badcol] # Drop  good with  most missing covariances
              if VERBOSE: print("Dropping %s." % badcol)
          else:
              HasMiss=False

      return X

  def svd_missing(A,max_rank=None,min_obs=None):
      """Singular Value Decomposition with missing values

      Returns matrices U,S,V.T, where A~=U*S*V.T.

      Inputs: 
          - A :: matrix or pd.DataFrame, with NaNs for missing data.
          - max_rank :: Truncates the rank of the representation.  
                 Note that this impacts which rows of V will be computed;
                 each row must have at least max_rank non-missing values.
          - min_obs :: Smallest number of non-missing observations for a 
                 row of U to be computed.
      """

      P=missing_inner_product(A,min_obs=min_obs)

      sigmas,u=np.linalg.eig(P)

      order=np.argsort(-sigmas)
      sigmas=sigmas[order]

      # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
      u=u[:,order]
      u=u[:,sigmas>0]
      s=np.sqrt(sigmas[sigmas>0])

      if max_rank is not None and len(s) > max_rank:
          u=u[:,:max_rank]
          s=s[:max_rank]

      r=len(s)
      us=np.matrix(u)*np.diag(s)

      v=np.zeros((len(s),A.shape[1]))
      for j in range(A.shape[1]):
          a=A.iloc[:,j].as_matrix().reshape((-1,1))
          x=np.nonzero(~np.isnan(a))[0] # non-missing elements of vector a
          if len(x)>=r:
              v[:,j]=(np.linalg.pinv(us[x,:])*a[x]).reshape(-1)
          else:
              v[:,j]=np.nan

      return np.matrix(u),s,np.matrix(v).T
#+END_SRC

** Rank 1 Approximation

Once we've computed the SVD of a matrix we can construct an optimal rank one
approximation to that matrix using just the  first left eigenvector,
the first eigenvalue, andn the first right eigenvector.  

#+name: svd_rank1_approximation_with_missing_data
#+begin_src python :noweb no-export :results output :tangle ../cfe/estimation.py
  import pandas as pd
  <<svd_missing>>

  def svd_rank1_approximation_with_missing_data(x,return_usv=False,max_rank=1,min_obs=None,VERBOSE=True):
      """
      Return rank 1 approximation to a pd.DataFrame x, where x may have
      elements which are missing.
      """
      x=x.copy()
      m,n=x.shape

      if n<m:  # If matrix 'thin', make it 'short'
          x=x.T
          TRANSPOSE=True
      else:
          TRANSPOSE=False

      x=x.dropna(how='all',axis=1) # Drop any column which is /all/ missing.
      x=x.dropna(how='all',axis=0) # Drop any row which is /all/ missing.

      x=drop_columns_wo_covariance(x.T,min_obs=min_obs).T
      u,s,v = svd_missing(x,max_rank=max_rank,min_obs=min_obs)
      if VERBOSE:
          print("Estimated singular values: ",)
          print(s)

      xhat=pd.DataFrame(v[:,0]*s[0]*u[:,0].T,columns=x.index,index=x.columns).T

      if TRANSPOSE: xhat=xhat.T

      if return_usv:
          u = pd.Series(u.A.squeeze(),index=xhat.columns)
          v = pd.Series(v.A.squeeze(),index=xhat.index)
          return xhat,u,s,v
      else: return xhat
#+end_src

** Test of Rank 1 SVD Approximation to Matrix with Missing Data

First, some code to check if approximation works for a simple, small
scale example.

#+name: svd_rank1_approximation_with_missing_data_example
#+begin_src python :noweb no-export :results output :tangle ../cfe/test/svd_rank1_approximation_with_missing_data_example.py
import numpy as np
import pandas as pd
<<svd_rank1_approximation_with_missing_data>>

(n,m)=(3,5)
a=np.random.normal(size=(n,1))
b=np.random.normal(size=(1,m))
e=np.random.normal(size=(n,m))*1e-2

X0=np.array([[-0.22,  0.32, -0.43],
             [0.01, 0.00,  0.00],
             [-0.22,  0.31, -0.42],
             [0.01, -0.03,  0.04],
             [-0.21, 0.31, -0.38]])

X0=X0-X0.mean(axis=1).reshape((-1,1))

X=X0.copy()
X[0,0]=np.nan
X[0,1]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

Xhat=svd_rank1_approximation_with_missing_data(X,VERBOSE=False)

print X0
print X
print Xhat
#+end_src

#+results: svd_rank1_approximation_with_missing_data_example
#+begin_example
      0         1     2         3         4
0 -0.11  0.006667 -0.11  0.003333 -0.116667
1  0.43 -0.003333  0.42 -0.036667  0.403333
2 -0.32 -0.003333 -0.31  0.033333 -0.286667
      0         1     2         3         4
0   NaN  0.006667 -0.11  0.003333 -0.116667
1   NaN -0.003333  0.42 -0.036667  0.403333
2 -0.32 -0.003333 -0.31  0.033333 -0.286667
          0         1         2         3         4
0 -0.109610  0.000398 -0.111595  0.010270 -0.106239
1  0.399679 -0.001452  0.406917 -0.037448  0.387385
2 -0.320000  0.001163 -0.325795  0.029982 -0.310158
#+end_example

#+name: svd_rank1_approximation_with_missing_data_test
#+begin_src python :noweb no-export :results output :var n=12 :var m=2000 :var percent_missing=0.5 :var SEED=0 :tangle ../cfe/test/svd_rank1_approximation_with_missing_data_test.py
import numpy as np
import pandas as pd
<<svd_rank1_approximation_with_missing_data>>

if SEED:
    np.random.seed(SEED)

a=np.random.normal(size=(n,1))
b=np.random.normal(size=(1,m))
e=np.random.normal(size=(n,m))*5e-1

X0=np.outer(a,b) + e
X0=X0-X0.mean(axis=0)

X=X0.copy()
X[np.random.random_sample(X.shape)<percent_missing]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

Xhat,u,s,v=svd_rank1_approximation_with_missing_data(X,VERBOSE=False,return_usv=True)

#rho_a=np.corrcoef(np.c_[a,u[:,0]],rowvar=0)[0,1]
rho_a=pd.DataFrame({'a':a.reshape(-1),'u':u}).corr().iloc[0,1]
rho_b=pd.DataFrame({'b':pd.Series(b.reshape(-1)),'v':v}).corr().iloc[0,1]
missing=np.isnan(X.as_matrix()).reshape(-1,1).mean()
print "Proportion missing %g and correlations are %5.4f and %5.4f." % (missing, rho_a,rho_b),
print "Singular value=%g" % s[0],
if SEED: print "Seed=%g" % SEED
else: print
#+end_src

#+results: svd_rank1_approximation_with_missing_data_test


** Test of construction of approximation to CE
#+begin_src python  :noweb no-export :results output :tangle ../cfe/test/test.py
  import numpy as np
  <<estimate_reduced_form>>
  <<artificial_data>>
  <<df_utils>>
  <<svd_rank1_approximation_with_missing_data>>

  y,truth=artificial_data(T=1,N=1000,n=12,sigma_e=1e-1)
  #y,truth=artificial_data(T=2,N=20,n=6,sigma_e=1e-8)
  beta,L,dz,p=truth

  numeraire='x0'

  b0,ce0,d0=estimate_bdce_with_missing_values(y,np.log(dz),return_v=False)
  myce0=ce0.copy()
  cehat=svd_rank1_approximation_with_missing_data(myce0)

  rho=pd.concat([ce0.stack(dropna=False),cehat.stack()],axis=1).corr().iloc[0,1]

  print("Norm of error in approximation of CE: %f; Correlation %f." % (df_norm(cehat,ce0)/df_norm(ce0),rho))
#+end_src

#+results:

* Estimation of reduced form
    This code takes as input time-varying household-level data on log
    expenditures and characteristics, and takes data defining markets
    and perhaps some prices.

    Data on prices is specified by providing a =pd.DataFrame= =P= with
    a MultiIndex of (period,market) indicated as =('t','mkt')=.  If
    provided, the dataframe =P= includes data on actual prices
    observed in different period-markets.  These data need not be
    complete, and in particular it's fine to provide prices for only a
    subset of goods.  However, if one or more prices is provided, one
    of the commodities should be chosen as a numéraire e.g.,
#+BEGIN_SRC python :exports code
    ix=pd.MultiIndex.from_tuples([(1975,'Aurepalle'),(1975,'Shirapur'),(1975,'Kanzara'),
                                  (1976,'Aurepalle'),(1976,'Shirapur'),(1976,'Kanzara'),
                                  (1977,'Aurepalle'),(1977,'Shirapur'),(1977,'Kanzara'),
                                  (1978,'Aurepalle'),(1978,'Shirapur'),(1978,'Kanzara')],names=['t','mkt'])
    P=pd.DataFrame({'Rice':[4,5,4,5,6,5,6,7,6,7,8,7],
                    'Sorghum':[2,3,2,2,3,2,3,4,3,4,5,6]},index=ix)

    numeraire='Rice'
#+END_SRC
Note that not all goods for which household level expenditures are
observed need to have  price supplied.  If prices for one good are
supplied, it should be the numéraire; if prices for two or more goods
are supplied it's possible to identify Frisch elasticities $\beta$ and
to estimate any missing prices.  

#+name: estimate_reduced_form
#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/estimation.py
  import pandas as pd
  import warnings
  import sys
  from collections import OrderedDict
  from cfe.df_utils import drop_missing, ols, arellano_robust_cov, broadcast_binary_op, use_indices, df_norm

  def estimate_reduced_form(y,z,return_v=False,return_se=False,VERBOSE=False):
    """Estimate reduced-form Frisch expenditure/demand system.

    Inputs:
        - y : pd.DataFrame of log expenditures or log quantities, indexed by (j,t,mkt), 
              where j indexes the household, t the period, and mkt the market.  
              Columns are different expenditure items.

        - z : pd.DataFrame of household characteristics; index should match that of y.

    Outputs:
        - a : Estimated good-time-market fixed effects; will sum to  zero.

        - ce : Residuals (can be provided as an input to get_log_lambdas()).

        - d : Estimated coefficients associated with characteristics z.

        - se : (Optional, if return_se) Estimated standard errors for coefficients d.

        - V : (Optional, if return_V) Estimated covariance matrix of coefficients d.

    Ethan Ligon                                            February 2017
    """
    try: # Be a little forgiving if t or mkt index is missing.
        assert y.index.names==['j','t','mkt'], "Indices should be (j,t,mkt)?"
    except AssertionError:
        y = y.reset_index()
        if not 'mkt' in y.columns: y['mkt']=1
        if not 't' in y.columns: y['t']=1
        y = y.set_index(['j','t','mkt'])

    try:
        assert z.index.names==['j','t','mkt'], "Indices should be (j,t,mkt)?"
    except AssertionError:
        z = z.reset_index()
        if not 'mkt' in z.columns: z['mkt']=1
        if not 't' in z.columns: z['t']=1
        z = z.set_index(['j','t','mkt'])

    periods = list(set(y.index.get_level_values('t')))
    mkts = list(set(y.index.get_level_values('mkt')))

    # Time-market dummies
    DateLocD = use_indices(y,['t','mkt'])
    DateLocD = pd.get_dummies(zip(DateLocD['t'],DateLocD['mkt']))
    DateLocD.index = y.index

    sed = pd.DataFrame(columns=y.columns)
    a = pd.Series(index=y.columns)
    b = OrderedDict() #pd.DataFrame(index=y.columns)
    d = OrderedDict() #pd.DataFrame(index=y.columns,columns=z.columns).T
    ce = pd.DataFrame(index=y.index,columns=y.columns)
    V = pd.Panel(items=y.columns,major_axis=z.columns,minor_axis=z.columns)

    for i,Item in enumerate(y.columns):
        if VERBOSE: print(Item)

        lhs,rhs=drop_missing([y.iloc[:,[i]],pd.concat([z,DateLocD],axis=1)])
        rhs=rhs.loc[:,rhs.std()>0] # Drop  any X cols with no variation
        useDateLocs=list(set(DateLocD.columns.tolist()).intersection(rhs.columns.tolist()))

        # Calculate deviations
        lhsbar=lhs.mean(axis=0)
        assert ~np.any(np.isnan(lhsbar)), "Missing data in lhs?"
        lhs=lhs-lhsbar
        lhs=lhs-lhs.mean(axis=0)

        rhsbar=rhs.mean(axis=0)
        assert ~np.any(np.isnan(rhsbar)), "Missing data in rhs?"
        rhs=rhs-rhsbar
        rhs=rhs-rhs.mean(axis=0)

        # Need to make sure time-market effects sum to zero; add
        # constraints to estimate restricted least squares
        ynil=pd.DataFrame([0],index=[(-1,0,0)],columns=lhs.columns)
        znil=pd.DataFrame([[0]*z.shape[1]],index=[(-1,0,0)],columns=z.columns)
        timednil=pd.DataFrame([[1]*DateLocD.shape[1]],index=[(-1,0,0)],columns=DateLocD.columns)

        X=rhs.append(znil.join(timednil))
        X=X.loc[:,X.std()>0] # Drop  any X cols with no variation

        # Estimate d & b
        myb,mye=ols(X,lhs.append(ynil),return_se=False,return_v=False,return_e=True) # Need version of pandas >0.14.0 (?) for this use of join
        ce[Item]=mye.iloc[:-1,:] # Drop constraint that sums time-effects to zero

        if return_v or return_se:
            if z.shape[1]:
                V[Item]=arellano_robust_cov(z,ce[Item])
                sed[Item]=pd.Series(np.sqrt(np.diag(V[Item])), index=z.columns) # reduced form se on characteristics

        d[Item]=myb[z.columns] # reduced form coefficients on characteristics

        b[Item] = myb[useDateLocs].squeeze()  # Terms involving prices
        a[Item] = lhsbar.mean() - d[Item].dot(rhsbar[z.columns]) - np.array(b[Item]).dot(rhsbar[useDateLocs])


    b = pd.DataFrame(b,index=y.groupby(level=['t','mkt']).mean().index)
    b = b.T
    sed = sed.T

    if b.shape[1]==1: # Only a single time-market
      assert np.all(np.isnan(b)), "Only one good-time effect should mean b not identified"
      b[:]=0

    d = pd.concat(d.values())

    out = [b.add(a,axis=0),ce,d]
    if return_se:
        out += [sed]
    if return_v:
        out += [V]
    return out
#+END_SRC

** Test
   If we use a set of fixed parameters to generate artificial data, we
   should be able to recover some of these parameters from =estimate_reduced_form=.
   Below we construct a simple test of this.
#+name: test_estimate_reduced_form
#+BEGIN_SRC python :results output :var T=1 :var N=5000 :var n=6 :tangle ../cfe/test/estimate_reduced_form.py
from scipy.stats.distributions import chi2

<<lambdas_dgp>>
<<characteristics_dgp>>
<<prices_dgp>>
<<expenditures_dgp>>
<<estimate_reduced_form>>

x,parts = expenditures(T,N,n,1,np.array([0.5,1.,1.5,2.,2.5,3.]),sigma_phi=0.0001,sigma_eps=0.0001)
x = x.replace({0.:np.nan}) # Zeros to missing

b,ce,d,V = estimate_reduced_form(np.log(x),np.log(parts['characteristics']),return_v=True)

A=np.matrix(np.diag(V.squeeze())).I
g=np.matrix(d-1)

J=g.T*A*g
p=(1 - chi2.cdf(J,len(g)))
assert p > 0.05, "Shouldn't often reject coefficients on characteristics all equal to 1."

print p
#+END_SRC

#+results: test_estimate_reduced_form
: [[ 0.82896109]]

The preceding creates a random sample with  known parameters =d=;
estimates of =d= should all be equal to 1 in expectation.  We
construct a statistic =J= which should be asymptotically distributed
$\chi^2$.  The code below resamples to  determine whether in fact we
match the correct distribution.  We construct  a =pp_plot= which
should deliver a line close to 45 degrees if all is well.

#+BEGIN_SRC python :var DRAWS=200 :tangle ../cfe/test/monte_carlo_estimate_reduced_form.py
  import pylab as pl
  T=1
  N=1000
  n=6

  def empirical_cdf(x):
      """
      Return the empirical cdf of a univariate vector or series x.
      """
      x=np.array(x)

      return lambda p: (x<p).mean()

  def pp_plot(F,G,interval=(0,1),npts=100):
      """
      Construct p-p plot of cdf F vs CDF G.
      """
      Q=np.linspace(interval[0],interval[1],npts)
      xy=[]
      for q in Q:
          xy.append([F(q),G(q)])

      xy=np.array(xy)
      ax=pl.plot(xy[:,0],xy[:,1])

      return xy

  Jay=[]
  Dee=[]
  Vee=[]
  for i in range(DRAWS):
      <<test_estimate_reduced_form>>
      Dee.append(d.as_matrix().squeeze().tolist())
      Jay.append(J[0,0])
      Vee.append(V.squeeze().as_matrix().tolist())

  Dee=np.array(Dee)
  Jay=np.array(Jay)
  Vee=np.array(Vee)

  F=empirical_cdf(Jay)
  G=lambda x: chi2.cdf(x,len(g))

  xy=pp_plot(F,G,interval=chi2.interval(.999,len(g)))

  assert np.linalg.norm(Dee.std(axis=0) - np.sqrt(Vee.mean(axis=0))) < 0.01
  
#+END_SRC



* Extraction of Frisch Elasticities and Neediness
#+name: get_loglambdas
#+begin_src python :noweb no-export :results output :tangle ../cfe/estimation.py
  import pandas as pd

  try: 
      from joblib import Parallel, delayed
      #import timeit
      PARALLEL=True
  except ImportError:
      PARALLEL=False
      warnings.warn("Install joblib for parallel bootstrap.")

  PARALLEL = False # Not yet working.

  def get_loglambdas(e,TEST=False,time_index='t',max_rank=1,min_obs=None):
      """
      Use singular-value decomposition to compute loglambdas and price elasticities,
      up to an unknown factor of proportionality phi.

      Input e is the residual from a regression of log expenditures purged
      of the effects of prices and household characteristics.   The residuals
      should be arranged as a matrix, with columns corresponding to goods. 
      """ 

      assert e.shape[0]>e.shape[1], "More goods than observations."

      chat = svd_rank1_approximation_with_missing_data(e,VERBOSE=False,max_rank=max_rank,min_obs=min_obs)

      R2 = chat.var()/e.var()

      # Possible that initial elasticity b_i is negative, if inferior goods permitted.
      # But they must be positive on average.
      if chat.iloc[0,:].mean()>0:
          b=chat.iloc[0,:]
      else:
          b=-chat.iloc[0,:]

      loglambdas=(-chat.iloc[:,0]/b.iloc[0])

      # Find phi that normalizes first round loglambdas
      phi=loglambdas.groupby(level=time_index).std().iloc[0]
      loglambdas=loglambdas/phi

      loglambdas=pd.Series(loglambdas,name='loglambda')
      bphi=pd.Series(b*phi,index=e.columns,name=r'\phi\beta')

      if TEST:
          foo=pd.DataFrame(-np.outer(bphi,loglambdas).T,index=loglambdas.index,columns=bphi.index)
          assert df_norm(foo-chat)<1e-4
          #print "blogL norm: %f" % np.linalg.norm(foo-chat)

      return bphi,loglambdas

  def iqr(x):
      """The interquartile range of a pd.Series of observations x."""
      q=x.quantile([0.25,0.75])
 
      try:
          return q.diff().iloc[1]
      except AttributeError:
          return np.nan

  def bootstrap_elasticity_stderrs(e,clusterby=['t','mkt'],tol=1e-2,minits=30,return_samples=False,VERBOSE=False,outfn=None,TRIM=True):
      """Bootstrap estimates of standard errors for \phi\beta.

      Takes pd.DataFrame of residuals as input.

      Default is to `cluster' by (t,mkt) via a block bootstrap.

      If optional parameter TRIM is True, then calculations are
      performed using the interquartile range (IQR) instead of the
      standard deviation, with the standard deviation computed as
      IQR*0.7416 (which is a good approximation provided the
      distribution is normal).

      Ethan Ligon                              January 2017
      """

      def resample(e):
          e = e.iloc[np.random.random_integers(0,e.shape[0]-1,size=e.shape[0]),:]
          e = e - e.mean()
          return e
 
      def new_draw(e,clusterby):      
          if clusterby:
              S=e.reset_index().groupby(clusterby,as_index=True)[e.columns].apply(resample)
          else:
              S=resample(e)

          bs,ls=get_loglambdas(S)

          return bs

      if outfn: outf=open(outfn,'a')

      delta=1.
      old=np.array(1)
      new=np.array(0)
      i=0
      chunksize=2

      assert chunksize>=2, "chunksize must be 2 or more."
      while delta>tol or i < minits:
          delta=np.nanmax(np.abs(old.reshape(-1)-new.reshape(-1)))
          if VERBOSE and i>chunksize: 
              stat = np.nanmax(np.abs((std0.reshape(-1)-std1.reshape(-1))/std0.reshape(-1)))
              print "Draws %d, delta=%5.4f.  Measure of non-normality %6.5f." % (i, delta, stat)
          old=new

          if PARALLEL:
              #start=timeit.timeit()
              bees = Parallel(n_jobs=chunksize)(delayed(new_draw)(e,clusterby) for chunk in range(chunksize))
              #print timeit.timeit() - start
          else:
              #start=timeit.timeit()
              bees = [new_draw(e,clusterby) for chunk in range(chunksize)]
              #print timeit.timeit() - start
  
          if outfn: 
              for bs in bees:
                  if np.any(np.isnan(bs)):
                      warnings.warn("Resampling draw with no data?")
                  outf.write(','.join(['%6.5f' % b for b in bs])+'\n')

          try:
              B=B.append(bees,ignore_index=True)
          except NameError:
              B=pd.DataFrame(bees,index=range(chunksize)) # Create B

          i+=chunksize

          std0=B.std()
          std1=B.apply(iqr)*0.7416 # Estimate of standard deviation, with trimming
          if TRIM:
              new=std1
          else:
              new=std0

      if outfn: outf.close()
      if return_samples:
          B.dropna(how='all',axis=1,inplace=True) # Drop any goods always missing estimate
          return new,B
      else:
          return new
#+end_src

*** Test of get_loglambdas
#+name: test_get_loglambdas
#+begin_src python :noweb no-export :results output :var miss_percent=0.6 :tangle ../cfe/test/test_get_loglambdas.py
import numpy as np
import pandas as pd
<<get_loglambdas>>
<<svd_rank1_approximation_with_missing_data>>
<<df_utils>>

(n,m)=(50,5000)
a=np.random.random_sample((n,1))
b=np.random.random_sample((1,m))
e=np.random.random_sample((n,m))*1e-5

X0=np.outer(a,b)+e

X=X0.copy()
X[np.random.random_sample(X.shape)<miss_percent]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

ahat,bhat=get_loglambdas(X,TEST=True)

Xhat=pd.DataFrame(np.outer(pd.DataFrame(ahat),pd.DataFrame(-bhat).T).T)

print("Norm of error (svd vs. truth): %f" % (df_norm(Xhat,X)/df_norm(X)))
#+end_src

*** Artificial data
We begin by generating some artificial data on expenditures.
#+name: artificial_data
#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/test/artificial_data.py
  import pandas as pd
  <<lambdas_dgp>> #lambdas
  <<prices_dgp>> # prices
  <<characteristics_dgp>> # characteristics

  <<expenditures_dgp>>

  def artificial_data(T=2,N=120,M=1,n=4,sigma_e=0.001):

      # truth=(beta,lambdas,characteristics,prices)
      x,truth=expenditures(T,N,M,n,beta=np.linspace(1,3,n),sigma_eps=sigma_e)

      y=np.log(x)

      return y,truth

#+END_SRC

*** Tests of estimation with missing data

#+name: test_estimate_with_missing
#+begin_src python :noweb no-export :results output :tangle ../cfe/test/estimate_with_missing.py :exports none
  import numpy as np
  <<estimate_reduced_form>>
  <<artificial_data>>
  <<svd_rank1_approximation_with_missing_data>>
  <<get_loglambdas>>
  <<df_utils>>

  y,truth=artificial_data(T=2,N=50,M=2,n=5,sigma_e=1e-8)

  y['mkt']=1
  y=y.reset_index().set_index(['j','t','mkt'])

  #beta,L,dz,p=truth
  dz=truth['characteristics']
  dz['mkt']=1
  dz=dz.reset_index().set_index(['j','t','mkt'])
  dz=np.log(dz)

  numeraire=None #'x0'

  # Try with missing data for contrast
  y.as_matrix()[np.random.random_sample(y.shape)<0.0]=np.nan

  y.replace(-np.inf,np.nan,inplace=True)

  #b,ce,d,V=estimate_bdce_with_missing_values(y,dz,return_v=True)
  b,ce,d=estimate_reduced_form(y,dz,return_v=False)

  bphi,logL=get_loglambdas(ce,TEST=True)
  cehat=np.outer(pd.DataFrame(bphi),pd.DataFrame(-logL).T).T
  cehat=pd.DataFrame(cehat,columns=bphi.index,index=logL.index)

  print "Norm of error in approximation of CE: %f" % df_norm(cehat,ce)

  # Some naive standard errors

  #yhat=b.T.add(cehat + (dz.dot(d.T)),axis=0,level='t')
  yhat = broadcast_binary_op(cehat + dz.dot(d.T),lambda x,y: x+y,b.T)

  e=y.sub(yhat)

  print "Correlation of log lambda with estimate (before normalization): %f" % pd.DataFrame({"L0":np.log(truth['lambdas'][0]),"Lhat":logL}).corr().iloc[0,1]

  if not numeraire is None:
      logL=broadcast_binary_op(logL,lambda x,y: x+y,b.loc[numeraire]) # Add term associated with numeraire good
      b=b-b.loc[numeraire]
  else:
      logL=broadcast_binary_op(logL,lambda x,y: x+y,b.mean()) # Add term associated with numeraire good
      b=b-b.mean()

  # Evaluate estimate of beta:
  print "Norm of (bphi,beta): %f" % np.var(bphi/truth['beta']) # Funny norm deals with fact that b only identified up to a scalar

  foo=logL.reset_index('mkt')
  foo['loglambda0']=np.log(truth['lambdas'][0])
  foo=foo.reset_index().set_index(['j','t','mkt'])
  print "Correlation of log lambda with estimate (after normalization):"
  print foo.groupby(level=['t','mkt']).corr()
  
  print "Mean of errors:"
  print e.mean(axis=0)

#+end_src

#+results: test_estimate_with_missing

* Estimation of Price Elasticities
  Here we develop two distinct estimators for obtaining estimates of
  price elasticities \beta in the demand relationship
  \begin{equation}
  \label{eq:demand}
     \log c_{it}^j = -\beta_i\log p_{itk} + \delta_i^\T z_t^j - \beta_i\log\lambda^j_t,
  \end{equation}
  or the expenditure relationship
  \begin{equation}
  \label{eq:expenditure}
     \log x_{it}^j = (1-\beta_i)\log p_{itk} + \delta_i^\T z_t^j - \beta_i\log\lambda^j_t,
  \end{equation}
  given data on log prices $\log p_{itk}$ for good $i$ at time $t$ in
  market $k$, characteristics $z_t^j$, and either consumption
  $c_{it}^j$ or expenditures $x_{it}^j$.  

** Direct estimation of price elasticities
  We do not assume that $\lambda^j_t$ is observed, but do assume that
  its log is orthogonal to log prices and characteristics.  In this
  case, we can simply use a least squares estimator to directly
  recover an estimate of either $-\beta_i$ (when log quantities are
  the dependent variable) or $1-\beta_i$ (when log expenditures are).

#+name: direct_price_elasticities
#+BEGIN_SRC python :tangle ../cfe/estimation.py

  def direct_price_elasticities(y,p,z,VERBOSE=True,return_se=False,return_v=False):
      """Estimate reduced-form Frisch expenditure/demand system.

         Inputs:
           - y : pd.DataFrame of log expenditures or log quantities, indexed by (j,t,mkt), 
                 where j indexes the household, t the period, and mkt the market.  
                 Columns are different expenditure items.

           - p : pd.DataFrame of log prices, indexed by (t,mkt), with
                 prices for different goods across columns.

           - z : pd.DataFrame of household characteristics; index should match that of y.

        Ethan Ligon                                            March 2017
      """
      assert(y.index.names==['j','t','mkt'])
      assert(z.index.names==['j','t','mkt'])

      periods = list(set(y.index.get_level_values('t')))
      mkts = list(set(y.index.get_level_values('mkt')))
      sed = pd.DataFrame(columns=y.columns)
      a = pd.Series(index=y.columns)
      b = OrderedDict() #pd.DataFrame(index=y.columns)
      d = OrderedDict() #pd.DataFrame(index=y.columns,columns=z.columns).T
      ce = pd.DataFrame(index=y.index,columns=y.columns)
      V = pd.Panel(items=y.columns,major_axis=z.columns,minor_axis=z.columns)

      for i,Item in enumerate(y.columns):
          if VERBOSE: print(Item)
          if np.any(np.isnan(p[Item])): continue # Don't estimate with missing prices

          rhs = z.reset_index('j').join(p[Item]).reset_index().set_index(['j','t','mkt'])
          rhs.rename(columns={Item:'log p'},inplace=True)

          lhs,rhs=drop_missing([y.iloc[:,[i]],rhs])

          rhs['Constant']=1

          myb,mye=ols(rhs,lhs,return_se=False,return_v=False,return_e=True) # Need version of pandas >0.14.0 (?) for this use of join
          ce[Item]=mye

          if return_v or return_se:
              V[Item]=arellano_robust_cov(rhs,mye)
              sed[Item]=pd.Series(np.sqrt(np.diag(V[Item])), index=z.columns) # reduced form se on characteristics

          d[Item]=myb[z.columns] # reduced form coefficients on characteristics

          a[Item] = myb['Constant']
          b[Item] = myb['log p'].values[0]

      b = pd.Series(b)

      d = pd.concat(d.values())

      out = [a,b,ce,d]
      if return_se:
          out += [sed]
      if return_v:
          out += [V]
      return out
#+END_SRC

** Indirect estimation of price elasticities

  A second approach is /indirect/, obtaining estimated elasticities by
  regressing the good-time-market effects obtained from
  =estimated_reduced_form= on $\log p_{itk} - \mbox{Proj}(\log
  p_{itk} | \bar z_{tk})$.  This exploits the relationship between
  these latent variables and implicit prices.  An important virtue of
  this approach is that if we have data for prices only on a subset of
  goods we can nevertheless estimate the first stage even for those
  goods where prices are missing.

#+BEGIN_SRC python :tangle ../cfe/estimation.py
  def indirect_price_elasticities(a,p,zbar):
      """Estimate reduced-form Frisch expenditure/demand system.

         Inputs:
           - a : pd.DataFrame of good-time-market effects estimated by =estimate_reduced_form=,
                 indexed by (t,mkt), where t indexes the period, and mkt the market.  
                 Columns are different expenditure items.

           - p : pd.DataFrame of log prices, indexed by (t,mkt), with
                 prices for different goods across columns.

           - zbar : pd.DataFrame of average household characteristics; index should match that of a.

        Ethan Ligon                                            March 2017
      """
      assert(a.index.names==['t','mkt'])
      assert(zbar.index.names==['t','mkt'])

      # Filter p
      X=zbar.copy()
      X['Constant'] = 1
      y = p.dropna(how='any',axis=1)

      # pe are filtered log prices
      bp,pe = ols(X,y,return_se=False,return_e=True)

      X = pe.copy()

      Xm=np.matrix((X-X.mean()).as_matrix())

      ym=np.matrix((a-a.mean()).as_matrix())
    
      B=OrderedDict()
      SE=OrderedDict()
      for i,Item in enumerate(y.columns):
          B[Item] = np.linalg.lstsq(Xm[:,i],ym[:,i])[0][0,0]
          e = ym[:,i] - Xm[:,i]*B[Item]
          SE[Item] = np.sqrt(np.var(e)/np.var(Xm[:,i]))

      B = pd.Series(B)
      SE = pd.Series(SE)
      return B,SE

#+END_SRC
  

** Test
   The direct and indirect methods  should yield similar results.
   Below we construct a simple test of this.
#+BEGIN_SRC python :var T=20 N=1000 n=6 :tangle ../cfe/test/price_elasticities.py
<<lambdas_dgp>>
<<characteristics_dgp>>
<<prices_dgp>>
<<expenditures_dgp>>

x,parts = expenditures(T,N,n,1,np.array([0.5,1.,1.5,2.,2.5,3.]),sigma_phi=0.01,sigma_eps=0.01)

print x.head()

#+END_SRC

#+results:

* Analysis Omnibus
  This describes a sort of `wrapper' routine which at a minimum takes
  as input a =pd.DataFrame= of log expenditures, indexed by household,
  period, and  market =("j","t","mkt")=, with  columns corresponding
  to different goods.  

  In addition, one may provide a dataframe of household
  characteristics with a similar structure to the dataframe of
  expenditures, save that columns will correspond to different
  household characteristics.  

  Finally, one may provide a dataframe of prices.  The structure of
  this dataframe is described above in Section [[*Estimation of reduced form][Estimation of reduced
  form]]. 

  The analysis omnibus performs a sequence of estimation steps,
  returning an "omnibus" of outputs in a dictionary.  These include
  estimated demand parameters, household IMUEs, and output from an ANOVA
  analysis, among others.

#+name: analysis_omnibus
#+begin_src python :noweb no-export :exports code :tangle ../cfe/estimation.py 
  # -*- coding: utf-8 -*-

  import tempfile
  import numpy as np
  import pandas as pd
  from numpy.linalg import norm

  def analysis_omnibus(y, z=None, prices=None, numeraire=None,
                       VERBOSE=False, BOOTSTRAP=False):

      if BOOTSTRAP is True: # Bootstrap also a tolerance parameter
          BOOTSTRAP = 1e-3

      if z is None:
         z = pd.DataFrame(index=y.index)

      if prices is not None: # Check price indices (t,mkt) consistent with indices in y
          assert set([tuple(x) for x in prices.index.levels]) == set([tuple(x) for x in y.index.levels[1:]]), \
                 "Must have prices for every (t,mkt) in expenditures y."

      firstround=y.reset_index().iloc[0]['t']  

      # Deflate expenditures and prices by prices of numeraire good.
      if numeraire is not None:
          y = broadcast_binary_op(y, lambda foo,bar: foo-bar, np.log(prices[numeraire]))
          logp=np.log(prices).sub(np.log(prices[numeraire]),axis=0)

      use_goods=y.columns.tolist()

      # The criterion below (hh must have observations for at least 1/8 of goods) ad hoc
      using_goods=(y[use_goods].T.count()>=np.floor(len(use_goods)/8.))
      y=y.loc[using_goods,use_goods] # Drop households with too few expenditure observations, keep selected goods
      y = drop_columns_wo_covariance(y,min_obs=30,VERBOSE=False)
      # Only keep goods with observations in each (t,mkt)
      y = y.loc[:,(y.groupby(level=['t','mkt']).count()==0).sum()==0] 

      results={'y':y,'z':z}

      a,ce,d,se,V = estimate_reduced_form(y,z,return_se=True,return_v=True,VERBOSE=VERBOSE)
      ce.dropna(how='all',inplace=True)

      results['ce']=ce
      results['d_covariance'] = V

      bphi,logL = get_loglambdas(ce,TEST=True,min_obs=30)

      assert np.abs(logL.groupby(level='t').std().iloc[0] - 1) < 1e-12, \
             "Problem with normalization of loglambdas"

      cehat=np.outer(pd.DataFrame(bphi),pd.DataFrame(-logL).T).T
      cehat=pd.DataFrame(cehat,columns=bphi.index,index=logL.index)
      results['cehat']=cehat

      if VERBOSE:
          print "Norm of error in approximation of CE divided by norm of CE: %f" % (df_norm(cehat,ce)/df_norm(ce))

      # Some naive standard errors & ANOVA
      miss2nan = ce*0
      anova=pd.DataFrame({'Prices':a.T.var(ddof=0),
                          'Characteristics':z.dot(d.T).var(ddof=0),
                          '$\log\lambda$':(cehat + miss2nan).var(ddof=0),
                          'Residual':(ce-cehat).var(ddof=0)})
      anova=anova.div(y.var(ddof=0),axis=0)
      anova['Total var']=y.var(ddof=0)
      anova.sort_values(by=r'$\log\lambda$',inplace=True,ascending=False)

      results['anova'] = anova

      yhat = broadcast_binary_op(cehat + z.dot(d.T),lambda x,y: x+y,a.T)

      yhat0 = broadcast_binary_op(ce + z.dot(d.T),lambda x,y: x+y,a.T)

      e = y.sub(yhat)
      e0 = y.sub(yhat0)

      goodsdf=d.copy()

      pref_params=[r'$\phi\beta_i$']
      if numeraire is not None:
          # FIXME: Issue here with dividing by a random variable.  What
          # properties do we want estimator of barloglambda_t to have?
          barloglambda_t=-a.loc[numeraire]/bphi[numeraire]
          logL = broadcast_binary_op(logL,lambda x,y: x+y,barloglambda_t) # Add term associated with numeraire good
          a = a - pd.DataFrame(np.outer(bphi,barloglambda_t),index=bphi.index,columns=barloglambda_t.index)

          # FIXME: Should really use weighted mean, since different precisions for a across different  markets
          logalpha = a[firstround].T.mean() 
          goodsdf[r'$\log\alpha_i$']=logalpha
          pref_params += [r'$\log\alpha_i$']
      else:
          pidx=a.mean()
          logL= broadcast_binary_op(logL,lambda x,y: x+y,pidx) # Add term associated with numeraire good
          a = a - pidx

      if VERBOSE:
          print "Mean of errors:"
          print e.mean(axis=0)

      goodsdf[r'$\phi\beta_i$']=bphi
      goodsdf['$R^2$']=1-e.var()/y.var()

      goodsdf=goodsdf[pref_params+d.columns.tolist()+['$R^2$']]
      goodsdf['%Zero']=100-np.round(100*(~np.isnan(y[goodsdf.index])+0.).mean(),1)

      ehat=e.dropna(how='all')
      ehat=ehat-ehat.mean()

      if BOOTSTRAP:
          tmpf = tempfile.mkstemp(suffix='.csv')
          if VERBOSE: print "Bootstrapping.  Interim results written to %s." % tmpf[1]

          sel,Bs = bootstrap_elasticity_stderrs(ce,tol=1e-4,VERBOSE=VERBOSE,return_samples=True,outfn=tmpf[1])
          results['Bs'] = Bs
          se[r'$\phi\beta_i$']=sel
      else:
          sel=[]
          for i in ehat:
              foo=pd.DataFrame({'logL':logL.squeeze(),'e':ehat[i]}).dropna(how='any')
              sel.append(np.sqrt(arellano_robust_cov(foo['logL'],foo['e']).as_matrix()[0,0]))
          se[r'$\phi\beta_i$']=np.array(sel)

      if numeraire is not None:
          se[r'$\log\alpha_i$']=ehat.query('t==%d' % firstround).std()/np.sqrt(ehat.query('t==%d'  % firstround).count())

      se.dropna(how='any',inplace=True)

      results['se'] = se
      goodsdf=goodsdf.T[se.index.tolist()].T # Drop goods that we can't compute std errs for.

      goodsdf.sort_values(by=[r'$\phi\beta_i$'],inplace=True,ascending=False)
      goodsdf.dropna(how='any',inplace=True)
      results['goods'] = goodsdf

      results['a'] = a
      results['loglambda'] = logL
      results['logexpenditures'] = y
      results['logexpenditures_hat'] = yhat

      return results
#+end_src

* Distance between two estimates of \beta
  When we compute the Frisch elasticities, these are only identified
  up to an unknown parameter $\phi$ (which we might call the Pigou
  elasticity, as it relates the price and Frisch elasticities in what
  Deaton calls "Pigou's Law.").  Thus, if we have /two/ different
  estimates of $\beta$, say $\beta^1$ and $\beta^2$, we
  define the difference between these using a norm
  \begin{equation}
  \label{eq:beta_distance}
     \min_\psi ||\psi\beta^1 - \beta^2||_W.
  \end{equation}  
  Note that $\psi$ should not be regarded as an estimate of the Pigou
  elasticity, but as the /ratio/ of the Pigou elasticities
  corresponding to the two different estimates of \beta.

  To implement a test of the hypothesis that $\beta^1=\beta^2$ we
  adopt a sort of $L^2$ distance measure, defining
  \begin{equation}
  \label{eq:norm}
     {} ||\vec{x}||_W = \vec{x}^\T \vec{W}\vec{x},
  \end{equation}
  where $\vec{W}$ is some positive definite matrix.  An /optimal/ choice of
  $\vec{W}$, in  a GMM sense \citep{hansen82}, is to use
  $\vec{W}=\Cov(\vec{x})^{-1}$.  Absent prior knowledge
  regarding this  covariance matrix, if $\beta^1$ and $\beta^2$ are
  estimated using independent  samples, we observe
  that \(\Cov(\psi\beta^1 - \beta^2) = \psi^2\vec{V^1} + \vec{V^2}\), where
  $\vec{V^1}$ and $\vec{V^2}$ are the covariance matrices corresponding to
  $\beta^1$ and $\beta^2$.  More generally, if $\beta^1$ is a "pooled"
  estimate which relies on a matrix of regressors $\vec{X}$, with $N$
  rows, and $\beta^2$ is obtained by estimation on a subset $\vec{X^2}$ with
  $N_2$ rows, then we have 
  \[
     \Cov(\psi\beta^1 - \beta^2) = \psi^2\vec{V^1} + \vec{V^2}\left[\vec{I}-2\frac{N_2}{N}\left(\frac{\vec{X}^\T\vec{X}}{N}\right)^{-1}\left(\frac{\vec{X^2}^\T\vec{X^2}}{N_2}\right)\right]
  \]
  Define the scatter matrices
  $\vec{S}=\vec{X}^\T\vec{X}$ and $\vec{S_2}=\vec{X^2}^\T\vec{X^2}$.
  Then supposing that estimates of the two covariance matrices
  $(\vec{V^1},\vec{V^2})$ can be
  obtained at the same time $\beta^1$ and $\beta^2$ are estimated, we
  choose $\psi$ to minimize 
  \begin{equation}
  \label{eq:min_chi2}
  H(\beta^1,\beta^2,\vec{V^1},\vec{V^2},\vec{S},\vec{S_2}) = \min_\psi \left(\psi\beta^1 - \beta^2\right)^\T\left[\psi^2\vec{V^1} + \vec{V^2}(\vec{I}-2 \vec{S}^{-1}\vec{S_2})\right]^{-1}\left(\psi\beta^1 - \beta^2\right).
  \end{equation}
  If the random variables $\beta^1$ and $\beta^2$ are normally
  distributed, then the (appropriately scaled) estimates $V^1$ and $V^2$ will have a Wishart
  distribution, and the statistic $H$ will be distributed as
  Mahalinobis' $D^2$ statistic.  Scaling this statistic,
  $N_2\left(\frac{N-n-1}{(N-1)(n-1)}\right)D^2$ is distributed $F_{n-1,N-n-1}$;
  as $N\rightarrow\infty$ (holding $n$ fixed this converges to the
  $\chi^2_{n-1}$ distribution).

  For the case in which the vectors $\beta$ are obtained as Frisch
  elasticities in a CFE demand system, then $\vec{X}$ is a vector of
  normalized $\log\lambda$ statistics, and identification assumptions
  on $\beta$ include $\E X=0$ and $\E X^\T X=1$.  Then the weighting
  matrix takes a form which is considerably simpler, but where the
  parameter $\psi$ enters in a more complicated fashion, with
  weighting matrix 
  \[ 
     \vec{W}^{-1}(\psi) = \psi^2\vec{V^1} + \vec{V^2}(1 - 2\frac{N_2}{N}\psi^2).  
  \]

  The following code provides an implementation of this test of
  equality for the CFE case.  We define a function
  =elasticities_equal= which takes as arguments
  $(\beta^1,\beta^2,V^1,V^2,N,N_2)$, and returns the value of $\psi$
  which  minimizes the criterion; the minimized value of the
  criterion, scaled to have the specified $F$ distribution; and
  optionally the \(p\)-value associated with the test.

#+name: elasticities_equal
#+BEGIN_SRC python :exports code :tangle ../cfe/estimation.py
  import numpy as np
  from scipy.optimize import minimize_scalar
  from scipy.stats.distributions import f as F

  def elasticities_equal(b1,b2,v1,v2,N,N2,pvalue=False,criterion=False):

      assert N2<N, "N2 should be size of sub-sample of pooled sample."
      n=len(b1)
  
      assert n==len(b2), "Length of vectors must be equal"

      def Fcriterion(psi):
          try:
              psi=psi[0,0]
          except (TypeError, IndexError):
              pass

          d = np.matrix(psi*b1 - b2)
          if d.shape[0]<d.shape[1]: d = d.T

          #W = np.matrix((psi**2)*v1 + v2*(1-2*(psi**2)*N2/N)).I
          W = np.matrix((psi**2)*v1 + v2).I # Independent case
          #W = np.matrix(v1 + v2).I # Independent case

          F = N2*(N-n-1)/((N-1)*(n-1)) * d.T*W*d

          if ~np.isscalar(F):
              F=F[0,0]

          return F

      #result = minimize_scalar(Fcriterion,method='bounded',bounds=[0,10])
      result = minimize_scalar(Fcriterion)
      psi=np.abs(result['x'])
      Fstat=result['fun']

      assert result['success'], "Minimization failed?"

      outputs = [psi,Fstat]

      if pvalue:
          p = 1 - F.cdf(Fstat,n-1,N-n-1)
          outputs.append(p)

      if criterion:
          outputs.append(Fcriterion)
      
      return tuple(outputs)

#+END_SRC


#+name: test_broadcast_binary_op
#+BEGIN_SRC python :noweb no-export :tangle ../cfe/test/elasticities_equal.py
  <<elasticities_equal>>

  N = 10000
  N2 = 5000
  b0=np.array([1,2,3])
  v0=np.array([[1,0.5,0.25],[0.5,1,.5],[.25,.5,1]])
  B=np.random.multivariate_normal(b0,v0,size=N)

  b1=np.mean(B,axis=0)
  v1=np.cov(B,rowvar=False)

  b2=2*np.mean(B[:N2,:],axis=0) # So true value of psi=2
  v2=4*np.cov(B[:N2,:],rowvar=False)

  def covb1b2(psi=1.,tol=1e-2):
      last=1
      next=0
      b1bar=0
      b2bar=0
      i=0
      while np.linalg.norm(next-last)>tol:
          i+=1
          last=next
          B1=B[np.random.randint(N,size=N),:]
          newb1=psi*np.mean(B1,axis=0)
          newb2=2*np.mean(B1[np.random.randint(N,size=N2),:],axis=0)
          next = next*(1-1./i) + np.outer(newb1,newb2)/i
          b1bar = b1bar*(1-1./i) + newb1/i
          b2bar = b2bar*(1-1./i) + newb2/i
          if i>100: continue

      C = next - np.outer(b1bar,b2bar)
      return (C + C.T)/2.

  def Vmom(psi=1.,tol=1e-2):
      last=1
      next=0
      dbar=0
      i=0
      while np.linalg.norm(next-last)>tol:
          i+=1
          last=next
          newb1=psi*np.mean(B[np.random.randint(N,size=N),:],axis=0)
          newb2=2*np.mean(B[np.random.randint(N,size=N2),:],axis=0)
          d = newb1 - newb2
          next = next*(1-1./i) + np.outer(d,d)/i
          dbar = dbar*(1-1./i) + d/i
          if i>100: continue

      return next - np.outer(dbar,dbar)

  foo = elasticities_equal(b1,b2,v1,v2,N,N2,pvalue=True,criterion=True)
  C=covb1b2()

  print foo
#+END_SRC  

* Monte Carlo Data Generating Process
 Here we construct a simple data-generating process, and then use
 data from this to estimate neediness, checking that we can recover
 the parameters of the data-generating process.

 We randomly generate several different kinds of data: "neediness"
 \lambda_{it}; prices $p_t$; and from these expenditures $x_{it}$.  

** Data-generating process for $\{\lambda^j_{t}\}$
   First we define a function which can generate a panel dataset of
   \(\lambda\)s, featuring both aggregate shocks, idiosyncratic
   shocks, and cross-sectional variation.

   The "aggregate" $\lambda$ is denoted by $\bar\lambda$, and is
   constructed so as to be the geometric mean of individuals'
   \(\lambda\)s in every period. By default these means are
   distributed log-normal.

   There are three different distributions we specify to generate an
   $(N,T)$ dataset of $\lambda_{it}$.  First, the distribution $\bar
   F$ governs the innovations involved in the aggregate 'shocks'
   $\bar\lambda$.  Second, a distribution $G_0$ governs the
   cross-sectional distribution of individual $\lambda$ in the initial
   period; finally, a distribution $F$ governs individual innovations
   /conditional/ on the aggregate shock.  The expected value of an
   geometric innovation is one, by construction, so both individual
   and aggregate \lambda processes are martingales.

#+name: lambdas_dgp
#+BEGIN_SRC python :results silent :exports code
  from scipy.stats.distributions import lognorm
  import numpy as np

  def geometric_brownian(sigma=1.):
      return lognorm(s=sigma,scale=np.exp(-(sigma**2)/2))

  def lambdabar(T,Fbar):
      return np.cumprod(Fbar.rvs(size=(T,1)),axis=0)

  def lambdas(T,N,G0=lognorm(.5),Fbar=geometric_brownian(.1),F=geometric_brownian(.2)):

      L0=G0.rvs(size=(1,N))  # Initial lambdas
      innov=F.rvs(size=(T-1,N))
      L=np.cumprod(np.r_[L0,innov],axis=0)
      
      # Add aggregate shocks L0:
      return L*lambdabar(T,Fbar=Fbar)
#+END_SRC

  In addition, time-varying household characteristics can affect
  demands.
#+name: characteristics_dgp
#+BEGIN_SRC python :noweb no-export :results silent :exports code
  characteristics = lambda T,N : lambdas(T,N,Fbar=geometric_brownian(.05),F=geometric_brownian(0.1))
#+END_SRC


** Data-generating process for $\{p_t\}$
    Next we construct an $n\times T$ matrix of prices for different
    consumption goods.  As with the process generating the
    $\lambda_{it}$, these are also assumed to satisfy a martingale
    process (so we can re-purpose code for generating \(\lambda\)s here):
#+name: prices_dgp
#+BEGIN_SRC python :noweb no-export :results silent :exports code
  prices = lambda T,n : lambdas(T,n,Fbar=geometric_brownian(.05),F=geometric_brownian(0.2))
#+END_SRC

** Data-generating process for measurement error
    As discussed above, there are three sources of measurement error
    in expenditures; an additive error; a multiplicative error, and
    truncation.

    The following routine returns a normally distributed additive
    error, and a log-normally distributed multiplicative error.
    Truncation can only be accomplished after the "true" expenditures
    are generated below.
#+name: measurement_error_dgp
#+BEGIN_SRC python :results value 
  import pandas as pd
  from scipy.stats import distributions
  import numpy as np

  def measurement_error(T,N,n,mu_phi=0.,sigma_phi=0.1,mu_eps=0.,sigma_eps=1.):

      def additive_error(T=T,N=N,n=n,sigma=sigma_phi):
          return distributions.norm.rvs(scale=sigma,size=(T,N,n)) + mu_phi

      def multiplicative_error(T=T,N=N,n=n,sigma=sigma_eps):
          return np.exp(distributions.norm.rvs(loc=-sigma/2.,scale=sigma,size=(T,N,n)) + mu_eps)

      phi=additive_error(T,N,n,sigma=sigma_phi)
      eps=multiplicative_error(T,N,n,sigma=sigma_eps)

      return phi,eps
#+END_SRC

** Data-generating process for expenditures
    
    We assume an addilog preference structure, generalized to allow
    for specific-substitution effects (but note that such effects
    violate symmetry of the Slutsky substitution matrix, and so should
    be regarded as a form of  specification error).  These
    elasticities are taken to be common across households (i.e., the
    curvature parameters in the addilog utilities are assumed equal);
    however, multiplicative terms are allowed to vary across
    households and goods, so that the direct momentary utility
    function for household $j$ can be written
    #
    \[
       U^j(c) = \sum_{i=1}^n\alpha^j_i\prod_{k=1}^n\frac{(c^j_{kt})^{1-1/\theta_{ik}} - 1}{1-1/\theta_{ik}}.
    \]
    # 
    With this structure, log Frischian expenditures are
    #
    \[
       \log x^j_{it} = \log\alpha^j_i + \log p_{it} - \sum_{k=1}^n(\theta_{ik})\log p_{kt} - \beta_i\log\lambda^j_t,
    \]
    #
    where $\beta_i=\sum_{k=1}^n(\theta_{ik})$ is the \(i\)th row-sum
    of the matrix $\Theta$.  Instantiated in code:
#+name: expenditures_dgp
#+BEGIN_SRC python :noweb no-export :results silent :exports code
  <<measurement_error_dgp>>

  def expenditures(T,N,n,k,beta,mu_phi=0,sigma_phi=0.1,mu_eps=0,sigma_eps=1.):

      if len(beta.shape)<2:
          Theta=np.matrix(np.diag(beta))
      else:
          Theta=np.matrix(beta)
          beta=Theta.sum(axis=0).A # Row sum of elasticity matrix

      l=lambdas(T,N)
      dz=characteristics(T,N)
      for i in range(1,k-1):  # add additional characteristics if called for.
          dz=np.c_[dz, characteristics(T,N)]
      L=np.reshape(l,(T,N,1)) 
      p=prices(T,n)

      # Build x in steps
      x = np.kron(np.log(L),-beta)
      x = x + (np.log(p)*(np.eye(n)-Theta)).A.reshape((T,1,n))
      x = x +  np.tile(np.log(dz).reshape((T,N,k)).sum(axis=2).reshape((T,N,1)),(1,1,n))
      x = np.exp(x)

      phi,e=measurement_error(T,N,n,mu_phi=mu_phi,sigma_phi=sigma_phi,mu_eps=mu_eps,sigma_eps=sigma_eps)
    
      x=(x+p.reshape(T,1,n)*phi) # Additive error
      x=x*e # Multiplicative error

      x=x*(x>0) # Truncation

      x=pd.Panel(x.T,items=['x%d' % i for i in range(n)]).to_frame()
      x.index.set_names(['j','t'],inplace=True)

      dz = dz.reshape((T*N,k),order='F')
      dz = {'z%d' % i:dz[:,i] for i in range(k)}
      dz=pd.DataFrame(dz,index=x.index)
      l=pd.DataFrame(pd.DataFrame(l).T.stack(),index=x.index)
      p=pd.DataFrame(p,columns=x.columns,index=x.index.levels[1])

      return x,{'beta':beta,'lambdas':l,'characteristics':dz,'prices':p}
#+END_SRC
* Utility Functions

#+name: df_utils
#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/df_utils.py
  import numpy as np
  from scipy import sparse
  import pandas as pd

  def df_norm(a,b=None,ignore_nan=True,ord=None):
      """
      Provides a norm for numeric pd.DataFrames, which may have missing data.

      If a single pd.DataFrame is provided, then any missing values are replaced with zeros, 
      the norm of the resulting matrix is returned.

      If an optional second dataframe is provided, then missing values are similarly replaced, 
      and the norm of the difference is replaced.

      Other optional arguments:

       - ignore_nan :: If False, missing values are *not* replaced.
       - ord :: Order of the matrix norm; see documentation for numpy.linalg.norm.  
                Default is the Froebenius norm.
      """
      a=a.copy()
      if not b is None:
        b=b.copy()
      else:
        b=pd.DataFrame(np.zeros(a.shape),columns=a.columns,index=a.index)

      if ignore_nan:
          missing=(a.isnull()+0.).replace([1],[np.NaN]) +  (b.isnull()+0.).replace([1],[np.NaN]) 
          a=a+missing
          b=b+missing
      return np.linalg.norm(a.fillna(0).as_matrix() - b.fillna(0).as_matrix())

  def df_to_orgtbl(df,tdf=None,sedf=None,conf_ints=None,float_fmt='%5.3f'):
      """
      Returns a pd.DataFrame in format which forms an org-table in an emacs buffer.
      Note that headers for code block should include ":results table raw".

      Optional inputs include conf_ints, a pair (lowerdf,upperdf).  If supplied, 
      confidence intervals will be printed in brackets below the point estimate.

      If conf_ints is /not/ supplied but sedf is, then standard errors will be 
      in parentheses below the point estimate.

      If tdf is False and sedf is supplied then stars will decorate significant point estimates.
      If tdf is a df of t-statistics stars will decorate significant point estimates.
      """
      if len(df.shape)==1: # We have a series?
         df=pd.DataFrame(df)

      if (tdf is None) and (sedf is None) and (conf_ints is None):
          s = '|  |'+'|   '.join([str(s) for s in df.columns])+'  |\n|-\n'
          for i in df.index:
              s+='| %s  ' % i
              for j in df.columns: # Point estimates
                  entry='| $'+float_fmt+'$  '
                  if np.isnan(df[j][i]):
                      s+='| --- '
                  else:
                      s+=entry % df[j][i]
              s+='|\n'
          return s
      elif not (tdf is None) and (sedf is None) and (conf_ints is None):
          s = '|  |'+'|   '.join([str(s) for s in df.columns])+'\t|\n|-\n'
          for i in df.index:
              s+='| %s  ' % i
              for j in df.columns:
                  try:
                      stars=(np.abs(tdf[j][i])>1.65) + 0.
                      stars+=(np.abs(tdf[j][i])>1.96) + 0.
                      stars+=(np.abs(tdf[j][i])>2.577) + 0.
                      stars = int(stars)
                      if stars>0:
                          stars='^{'+'*'*stars + '}'
                      else: stars=''
                  except KeyError: stars=''
                  entry='| $'+float_fmt+stars+'$  '
                  if np.isnan(df[j][i]):
                      s+='| --- '
                  else:
                      s+=entry % df[j][i]
              s+='|\n'

          return s
      elif not (sedf is None) and (conf_ints is None): # Print standard errors on alternate rows
          if tdf is not False:
              try: # Passed in dataframe?
                  tdf.shape
              except AttributeError:  
                  tdf=df[sedf.columns]/sedf
          s = '|  |'+'|   '.join([str(s) for s in df.columns])+'  |\n|-\n'
          for i in df.index:
              s+='| %s  ' % i
              for j in df.columns: # Point estimates
                  if tdf is not False:
                      try:
                          stars=(np.abs(tdf[j][i])>1.65) + 0.
                          stars+=(np.abs(tdf[j][i])>1.96) + 0.
                          stars+=(np.abs(tdf[j][i])>2.577) + 0.
                          stars = int(stars)
                          if stars>0:
                              stars='^{'+'*'*stars + '}'
                          else: stars=''
                      except KeyError: stars=''
                  else: stars=''
                  entry='| $'+float_fmt+stars+'$  '
                  if np.isnan(df[j][i]):
                      s+='| --- '
                  else:
                      s+=entry % df[j][i]
              s+='|\n|'
              for j in df.columns: # Now standard errors
                  s+='  '
                  try:
                      if np.isnan(df[j][i]): # Pt estimate miss
                          se=''
                      elif np.isnan(sedf[j][i]):
                          se='(---)'
                      else:
                          se='$(' + float_fmt % sedf[j][i] + ')$' 
                  except KeyError: se=''
                  entry='| '+se+'  '
                  s+=entry 
              s+='|\n'
          return s
      elif not (conf_ints is None): # Print confidence intervals on alternate rows
          if tdf is not False and sedf is not None:
              try: # Passed in dataframe?
                  tdf.shape
              except AttributeError:  
                  tdf=df[sedf.columns]/sedf
          s = '|  |'+'|   '.join([str(s) for s in df.columns])+'  |\n|-\n'
          for i in df.index:
              s+='| %s  ' % i
              for j in df.columns: # Point estimates
                  if tdf is not False and tdf is not None:
                      try:
                          stars=(np.abs(tdf[j][i])>1.65) + 0.
                          stars+=(np.abs(tdf[j][i])>1.96) + 0.
                          stars+=(np.abs(tdf[j][i])>2.577) + 0.
                          stars = int(stars)
                          if stars>0:
                              stars='^{'+'*'*stars + '}'
                          else: stars=''
                      except KeyError: stars=''
                  else: stars=''
                  entry='| $'+float_fmt+stars+'$  '
                  if np.isnan(df[j][i]):
                      s+='| --- '
                  else:
                      s+=entry % df[j][i]
              s+='|\n|'
              for j in df.columns: # Now confidence intervals
                  s+='  '
                  try:
                      ci='$[' + float_fmt +','+ float_fmt + ']$'
                      ci= ci % (conf_ints[0][j][i],conf_ints[1][j][i])
                  except KeyError: ci=''
                  entry='| '+ci+'  '
                  s+=entry 
              s+='|\n'
          return s

  def orgtbl_to_df(table, col_name_size=1, format_string=None, index=None):
    """
    Returns a pandas dataframe.
    Requires the use of the header `:colnames no` for preservation of original column names.
    `table` is an org table which is just a list of lists in python.
    `col_name_size` is the number of rows that make up the column names.
    `format_string` is a format string to make the desired column names.
    `index` is a column label or a list of column labels to be set as the index of the dataframe.
    """
    import pandas as pd

    if col_name_size==0:
      return pd.DataFrame(table)
 
    colnames = table[:col_name_size]

    if col_name_size==1:
      if format_string:
        new_colnames = [format_string % x for x in colnames[0]]
      else:
        new_colnames = colnames[0]
    else:
      new_colnames = []
      for colnum in range(len(colnames[0])):
        curr_tuple = tuple([x[colnum] for x in colnames])
        if format_string:
          new_colnames.append(format_string % curr_tuple)
        else:
          new_colnames.append(str(curr_tuple))

    df = pd.DataFrame(table[col_name_size:], columns=new_colnames)
 
    if index:
      df.set_index(index, inplace=True)

    return df

  def balance_panel(df):
      """Drop households that aren't observed in all rounds."""
      pnl=df.to_panel()
      keep=pnl.loc[list(pnl.items)[0],:,:].dropna(how='any',axis=1).iloc[0,:]
      df=pnl.loc[:,:,keep.index].to_frame(filter_observations=False)
      df.index.names=pd.core.base.FrozenList(['Year','HH'])

      return df

  def drop_missing(X):
      """
      Return tuple of pd.DataFrames in X with any 
      missing observations dropped.  Assumes common index.
      """

      foo=pd.concat(X,axis=1).dropna(how='any')
      assert len(set(foo.columns))==len(foo.columns) # Column names must be unique!

      Y=[]
      for x in X:
          Y.append(foo.loc[:,pd.DataFrame(x).columns]) 

      return tuple(Y)

  def use_indices(df,idxnames):
      return df.reset_index()[idxnames].set_index(df.index)

#+END_SRC

** Some econometric routines

#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/df_utils.py

  def arellano_robust_cov(X,u,clusterby=['t','mkt']):
      X,u = drop_missing([X,u])
      clusters = set(zip(*tuple(use_indices(u,clusterby)[i] for i in clusterby)))
      if  len(clusters)>1:
          # Take out time averages
          u = u - u.groupby(level=clusterby).transform(np.mean)
          X = X - X.groupby(level=clusterby).transform(np.mean)
          #u=broadcast_binary_op(u,lambda x,y:x-y, u.groupby(level=clusterby).mean()).squeeze()
          #X=broadcast_binary_op(X,lambda x,y:x-y, X.groupby(level=clusterby).mean()) 
          Xu=X.mul(u.squeeze(),axis=0)
          if len(X.shape)==1:
              XXinv=np.array([1./(X.T.dot(X))])
          else:
              XXinv=np.linalg.inv(X.T.dot(X))
          Vhat = XXinv.dot(Xu.T.dot(Xu)).dot(XXinv)
      else:
          u=(u-u.mean()).squeeze()
          X=X-X.mean()

          Xu=X.mul(u,axis=0)
          if len(X.shape)==1:
              XXinv=np.array([1./(X.T.dot(X))])
          else:
              XXinv=np.linalg.inv(X.T.dot(X))
          Vhat = XXinv.dot(Xu.T.dot(Xu)).dot(XXinv)

      try:
          return pd.DataFrame(Vhat,index=X.columns,columns=X.columns)
      except AttributeError:
          return Vhat


  def ols(x,y,return_se=True,return_v=False,return_e=False):

      x=pd.DataFrame(x) # Deal with possibility that x & y are series.
      y=pd.DataFrame(y)
      # Drop any observations that have missing data in *either* x or y.
      x,y = drop_missing([x,y]) 

      N,n=y.shape
      k=x.shape[1]

      b=np.linalg.lstsq(x,y)[0]

      b=pd.DataFrame(b,index=x.columns,columns=y.columns)

      out=[b.T]
      if return_se or return_v or return_e:

          u=y-x.dot(b)

          # Use SUR structure if multiple equations; otherwise OLS.
          # Only using diagonal of this, for reasons related to memory.  
          S=sparse.dia_matrix((sparse.kron(u.T.dot(u),sparse.eye(N)).diagonal(),[0]),shape=(N*n,)*2) 

          if return_se or return_v:

              # This will be a very large matrix!  Use sparse types
              V=sparse.kron(sparse.eye(n),(x.T.dot(x).dot(x.T)).as_matrix().view(type=np.matrix).I).T
              V=V.dot(S).dot(V.T)

          if return_se:
              se=np.sqrt(V.diagonal()).reshape((x.shape[1],y.shape[1]))
              se=pd.DataFrame(se,index=x.columns,columns=y.columns)

              out.append(se)
          if return_v:
              # Extract blocks along diagonal; return an Nxkxn array
              V={y.columns[i]:pd.DataFrame(V[i*k:(i+1)*k,i*k:(i+1)*k],index=x.columns,columns=x.columns) for i in range(n)} 
              out.append(V)
          if return_e:
              out.append(u)
      return tuple(out)
#+END_SRC

** Utility functions for dealing with some awkward multiindex issues

#+name: broadcast_binary_op 
#+BEGIN_SRC python :noweb no-export :tangle  ../cfe/df_utils.py
  def merge_multi(df1, df2, on):
      """Merge on subset of multiindex.
     
      Idea due to http://stackoverflow.com/questions/23937433/efficiently-joining-two-dataframes-based-on-multiple-levels-of-a-multiindex
      """
      return df1.reset_index().join(df2,on=on).set_index(df1.index.names)

  def broadcast_binary_op(x, op, y):
      """Perform x op y, allowing for broadcasting over a multiindex.

      Example usage: broadcast_binary_op(x,lambda x,y: x*y ,y)
      """
      x = pd.DataFrame(x.copy())
      y = pd.DataFrame(y.copy())
      xix= x.index.copy()
  
      if y.shape[1]==1: # If y a series, expand to match x.
          y=pd.DataFrame([y.iloc[:,0]]*x.shape[1],index=x.columns).T

      cols = list(x.columns)
      xindex = list(x.index.names)
      yindex = list(y.index.names)

      dif = list(set(xindex)-set(yindex))

      z = pd.DataFrame(index=xix)
      z = merge_multi(z,y,on=yindex)

      newdf = op(x[cols],z[cols])

      return newdf
#+END_SRC

#+name: test_broadcast_binary_op
#+BEGIN_SRC python :noweb no-export :tangle ../cfe/test/broadcast_binary_op.py
import pandas as pd
from numpy import array, nan

<<broadcast_binary_op>>

foo=array([[        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan, -0.73396916],
           [-0.14518201,         nan,  0.88238915],
           [        nan,         nan, -0.49532144],
           [        nan,         nan,  0.55594608],
           [ 0.30538166,         nan,         nan],
           [        nan,         nan,  0.22884155],
           [        nan,         nan,  0.10763069],
           [ 0.36909748,         nan, -0.57661338],
           [ 0.        ,         nan, -0.38566246],
           [ 0.31845372,         nan,  0.92198873],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [ 1.2039728 ,         nan,         nan],
           [-0.62860866, -0.69314718,  6.27914662],
           [-0.28768207,  0.        ,  1.60943791]])

idx=[ (u'101005', u'2009Q1', u'BGD'),  (u'101005', u'2011Q1', u'BGD'),
      (u'101007', u'2009Q1', u'BGD'),  (u'101007', u'2011Q1', u'BGD'),
      (u'00L0101', u'2013Q3', u'GHA'), (u'00L0101', u'2014Q3', u'GHA'),
      (u'00L0202', u'2013Q3', u'GHA'), (u'00L0202', u'2014Q3', u'GHA'),
      (u'02L0101', u'2012Q1', u'GHA'), (u'02L0101', u'2012Q3', u'GHA'),
      (u'02L0101', u'2013Q1', u'GHA'), (u'02L0202', u'2012Q1', u'GHA'),
      (u'02L0202', u'2013Q1', u'GHA'), (u'02L0238', u'2012Q3', u'GHA'),
      (1002, u'2014Q3', u'SSD'),       (1006, u'2014Q3', u'SSD'),
      (1002, u'2015Q2', u'SSD'),       (1006, u'2015Q2', u'SSD'),
      (u'3015', u'2010Q4', u'UGA'),    (u'3016', u'2010Q4', u'UGA')]

foo=pd.DataFrame(foo,index=pd.MultiIndex.from_tuples(idx,names=['j','t','mkt']))
foo=foo.sort_index()

bar=array([[ 0.24921932,         nan,         nan],
       [ 0.21093294,         nan,         nan],
       [ 0.33506356,  0.33506356,  0.33506356],
       [ 0.28271356,  0.26914329,  0.26914329],
       [ 0.23344945,  0.10095022,  0.0796501 ],
       [ 0.02299849, -0.25550914, -0.10560527],
       [ 0.0486334 ,  0.34874107,  0.17723716],
       [ 0.28000935,  0.05786949,  0.28000935],
       [ 0.01387462,         nan,         nan],
       [ 0.30318531,         nan,  0.30318531]])

idx=[(u'2009Q1', u'BGD'), (u'2011Q1', u'BGD'), (u'2011Q3', u'UGA'),
       (u'2012Q1', u'GHA'), (u'2012Q3', u'GHA'), (u'2013Q1', u'GHA'),
       (u'2013Q3', u'GHA'), (u'2014Q3', u'GHA'), (u'2014Q3', u'SSD'),
       (u'2015Q2', u'SSD')]

bar = pd.DataFrame(bar,index=pd.MultiIndex.from_tuples(idx,names=['t','mkt']))

baz=broadcast_binary_op(foo, lambda x,y: x+y, bar)

assert baz.shape == foo.shape
print baz

#+END_SRC

#+results: test_broadcast_binary_op
#+begin_example
                           0   1         2
j       t      mkt                        
1002    2014Q3 SSD       NaN NaN       NaN
        2015Q2 SSD       NaN NaN       NaN
1006    2014Q3 SSD       NaN NaN       NaN
        2015Q2 SSD  1.507158 NaN       NaN
00L0101 2013Q3 GHA       NaN NaN -0.556732
        2014Q3 GHA  0.134827 NaN  1.162398
00L0202 2013Q3 GHA       NaN NaN -0.318084
        2014Q3 GHA       NaN NaN  0.835955
02L0101 2012Q1 GHA  0.588095 NaN       NaN
        2012Q3 GHA       NaN NaN  0.308492
        2013Q1 GHA       NaN NaN  0.002025
02L0202 2012Q1 GHA  0.651811 NaN -0.307470
        2013Q1 GHA  0.022998 NaN -0.491268
02L0238 2012Q3 GHA  0.551903 NaN  1.001639
101005  2009Q1 BGD       NaN NaN       NaN
        2011Q1 BGD       NaN NaN       NaN
101007  2009Q1 BGD       NaN NaN       NaN
        2011Q1 BGD       NaN NaN       NaN
3015    2010Q4 UGA       NaN NaN       NaN
3016    2010Q4 UGA       NaN NaN       NaN
#+end_example

#+name: test_broadcast_binary_op2
#+BEGIN_SRC python :noweb no-export :tangle ../cfe/test/broadcast_binary_op2.py
  import pandas as pd
  from numpy import array, nan

  <<broadcast_binary_op>>

  idx=pd.MultiIndex.from_tuples([(0,0,0),(0,0,1),(0,1,0),(0,1,1),
                                 (1,0,0),(1,0,1),(1,1,0),(1,1,1)],names=('a','b','c'))

  foo = pd.DataFrame({'x':range(8)},index=idx)

  idx=pd.MultiIndex.from_tuples([(0,0),(0,1),(1,0),(1,1)],names=('b','c'))

  bar = pd.DataFrame({'y':range(4)},index=idx)

  baz = broadcast_binary_op(foo,lambda x,y:x+y,bar)

  assert baz.iloc[3,0]==6

#+END_SRC

#+results: test_broadcast_binary_op2


