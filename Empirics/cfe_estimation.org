:SETUP:
#+TITLE: Code and Methods for Estimating Constant Frisch Elasticity Demand Systems
#+AUTHOR: Ethan Ligon
#+OPTIONS: toc:nil
#+PROPERTY: header-args:python :results output :noweb no-export :exports code :comments link :prologue (format "# Tangled on %s" (current-time-string))
#+LATEX_HEADER: \renewcommand{\vec}[1]{\boldsymbol{#1}}
#+LATEX_HEADER: \newcommand{\T}{\top}
#+LATEX_HEADER: \newcommand{\E}{\ensuremath{\mbox{E}}}
#+LATEX_HEADER: \newcommand{\R}{\ensuremath{\mathbb{R}}}
#+LATEX_HEADER: \newcommand{\Cov}{\ensuremath{\mbox{Cov}}}
#+LATEX_HEADER: \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LATEX_HEADER: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
#+LATEX_HEADER: \renewcommand{\refname}{}
#+LATEX_HEADER: \usepackage{stringstrings}\renewcommand{\cite}[1]{\caselower[q]{#1}\citet{\thestring}}
:END:
* Introduction
  This document describes a set of empirical methods for estimating
  demands of a form we'll call "Constant Frisch Elasticity," and
  also provides =python= code which implements these methods.  
  Both this document and the code it contains are licensed in a manner
  which permits non-commercial reuse (see
  https://creativecommons.org/licenses/by-nc-sa/4.0/), subject to a 
  "attribution" requirement that you give appropriate credit to the
  present author.  In the case of uses of the code or methods in an
  written work such as a journal article, I ask that attribution
  include citation of any relevant refereed publications.

* Expenditure Shares

These functions allow the computation and visualization of variation
in expenditure shares.  The principal input is a =pd.DataFrame= with
columns corresponding to different expenditure items, and rows
corresponding to period-households, indexed by $(t,j)$.

#+name: agg_shares_and_mean_shares
#+begin_src python :exports none :tangle ../cfe/estimation.py
#name: agg_shares_and_mean_shares

import pylab as pl 
import pandas as pd
import numpy as np
from cfe.df_utils import broadcast_binary_op, is_none
from itertools import cycle

def expenditure_shares(df):

    df.fillna(0,inplace=True)
    aggshares=df.groupby(level='t').sum()
    aggshares=aggshares.div(aggshares.sum(axis=1),axis=0).T
    meanshares=df.div(df.sum(axis=1),level='j',axis=0).groupby(level='t').mean().T

    mratio=(np.log(aggshares)-np.log(meanshares))
    sharesdf={'Mean shares':meanshares,'Agg. shares':aggshares}

    return sharesdf,mratio

def agg_shares_and_mean_shares(df,figname=None,ConfidenceIntervals=False,ax=None,VERTICAL=False,CycleMarkers=False):
    """Figure of log agg shares - log mean shares.

    Required argument is a pd.DataFrame of expenditures, indexed by (t,j).

    Optional arguments
    ------------------
    figname : string; default None.
        If supplied, will save figure to file named figname.

    ConfidenceIntervals : Boolean or float in (0,1);  default False.
        If True, the returned figure will have 95% confidence intervals.  
        If in (0,1) that will be used for the size of the confidence interval instead.

    ax : matplotlib.Axes object; default None.
        If supplied, will draw figure on existing Axes object.

    VERTICAL : Boolean or scalar; default False.
        If True or non-zero scalar produce figure with expenditures arranged in vertical list. 
        If non-zero scalar used to control vertical spacing of figure.
    """

    if CycleMarkers:
        markers = cycle(["-o","-v","-^","-<","->","-*","-+","-d"])
    else:
        markers = cycle(["-o"])

    shares,mratio=expenditure_shares(df)
    meanshares=shares['Mean shares']

    tab = pd.concat(shares,axis=1)
    tab.sort_values(by=('Agg. shares',meanshares.columns[0]),ascending=False,inplace=True)

    if ax is None:
        fig, ax = pl.subplots()

    mratio.sort_values(by=mratio.columns[0],inplace=True)

    if VERTICAL:
        if VERTICAL is not True: # Numeric value supplied
            vertical_scale=VERTICAL
        else:
            vertical_scale=6.
        for i in mratio.columns:
            ax.plot(mratio[i].values, list(range(mratio.shape[0])), next(markers))
        ax.legend(mratio.columns,loc=2,fontsize='small')
        ax.set_xlabel('Log Aggregate shares divided by Mean shares')
        ax.set_yticks(list(range(mratio.shape[0])))
        ax.set_yticklabels(mratio.index.values.tolist(),rotation=0,size='small')
        ax.axvline()
        v = ax.axis()
        ax.figure.set_figheight((v[-1]/24)*vertical_scale)
        pl.tight_layout()
    else:
        for i in mratio.columns:
            ax.plot(list(range(mratio.shape[0])), mratio[i].values, next(markers))
        ax.legend(mratio.columns,loc=2,fontsize='small')
        ax.set_ylabel('Log Aggregate shares divided by Mean shares')

        v=ax.axis()

        if  len(mratio)>=12:
            i=0
            for i in range(len(mratio)):
                name=mratio.index[i] # label of expenditure item

                if mratio.iloc[i,0]>0.2:
                    #pl.text(i,mratio.T.iloc[0][name],name,fontsize='xx-small',ha='right')

                    # The key option here is `bbox`. 
                    ax.annotate(name, xy=(i,mratio.T.iloc[0][name]), xytext=(-20,10), 
                                textcoords='offset points', ha='right', va='bottom',
                                bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3),
                                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.25', 
                                color='red'),fontsize='xx-small')

                if mratio.iloc[i,0]<-0.2:
                    #pl.text(i,mratio.T.iloc[0][name],name,fontsize='xx-small')
                    ax.annotate(name, xy=(i,mratio.T.iloc[0][name]), xytext=(20,-10), 
                                textcoords='offset points', ha='left', va='top',
                                bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3),
                                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.25', 
                                color='red'),fontsize='xx-small')
        else: #Put labels on xaxis
            ax.set_xticklabels(mratio.index.values.tolist(),rotation=45)

        ax.axhline()

  

    if ConfidenceIntervals>0: # Bootstrap some confidence intervals
        if ConfidenceIntervals==1: ConfidenceIntervals=0.95
        current=0
        last=1
        M=np.array([],ndmin=3).reshape((mratio.shape[0],mratio.shape[1],0))
        i=0
        mydf=df.loc[:,mratio.index]
        while np.max(np.abs(current-last))>0.001 or i < 1000:
            last=current
            # Sample households in each  round with replacement
            bootdf=mydf.iloc[np.random.randint(0,df.shape[0],df.shape[0]),:]
            bootdf.reset_index(inplace=True)
            bootdf.loc[:,'j']=list(range(bootdf.shape[0]))
            bootdf.set_index(['t','j'],inplace=True)
            shares,mr=expenditure_shares(bootdf)
            M=np.dstack((M,mr.values))
            M.sort(axis=2)
            a = (1-ConfidenceIntervals)/2.
            lb = mratio.values - M[:,:,int(np.floor(M.shape[-1]*a))]
            ub=M[:,:,int(np.floor(M.shape[-1]*(ConfidenceIntervals+a)))] - mratio.values
            current=np.c_[lb,ub]
            i+=1

        T=mratio.shape[1]
        for t in range(T):
            if VERTICAL:
                ax.errorbar(mratio.values[:,t],np.arange(mratio.shape[0]),xerr=current[:,[t,t-T]].T.tolist())
            else:
                ax.errorbar(np.arange(mratio.shape[0]),mratio.values[:,t],yerr=current[:,[t,t-T]].T.tolist())

            tab[(df.index.levels[0][t],'Upper Int')]=current[:,t-T]
            tab[(df.index.levels[0][t],'Lower Int')]=current[:,t]

    if figname:
        pl.savefig(figname)

    return tab,ax
#+end_src

#+results: agg_shares_and_mean_shares

#+name: group_expenditures
#+begin_src python :noweb yes :tangle ../cfe/estimation.py
#name: group_expenditures
def group_expenditures(df,groups):
    myX=pd.DataFrame(index=df.index)
    for k,v in groups.items():
        if len(k):
            myv = [int(i) for i in v if len(str(i))>0]
            try:
                myX[k]=df[['$x_{%d}$' % int(i) for i in myv]].sum(axis=1)
            except KeyError: pass
            
    return myX
#+end_src

#+results: group_expenditures

* Rank 1 SVD Approximation to Matrix with Missing Data
** Eigenvalue Decomposition Approach to Computing the SVD
Here's an approach that involves estimating a covariance matrix, and
extracting $U$ and $\Sigma$ from that; then backing out estimated $V$,
for columns satisfying a rank condition.  Unlike some alternative
approaches estimates do not depend on the order of observations.  

A parameter =min_obs= governs how  conservative the algorithm is in
estimating the covariance matrix; it's equal to  minimum number of
cross-products required to  estimate an element of that matrix.  Note
that if this is a small integer one is more apt to obtain estimates
of the covariance matrix which are  not positive definite.
#+name: svd_missing
#+BEGIN_SRC python
#name: svd_missing

import numpy as np
import warnings

def missing_inner_product(X,min_obs=None):
    """Compute inner product X.T@X, allowing for possibility of missing data."""
    n,m=X.shape

    if n<m: 
        axis=1
        N=m
    else: 
        axis=0
        N=n

    xbar=X.mean(axis=axis)

    if axis:
        C=(N-1)*X.T.cov(min_periods=min_obs)
    else:
        C=(N-1)*X.cov(min_periods=min_obs)

    return C + N*np.outer(xbar,xbar)

def drop_columns_wo_covariance(X,min_obs=None,VERBOSE=False):
    """Drop columns from pd.DataFrame that lead to missing elements of covariance matrix."""

    m,n=X.shape
    assert m>n, "Fewer rows than columns.  Consider passing the transpose."

    # If good has fewer total observations than min_obs, can't possibly
    # have more cross-products.  Dropping here is faster than iterative procedure below.
    X = X.loc[:,X.count()>=min_obs]

    HasMiss=True
    while HasMiss:
        foo = X.cov(min_periods=min_obs).count()
        if np.sum(foo<X.shape[1]):
            badcol=foo.idxmin()
            del X[badcol] # Drop  good with  most missing covariances
            if VERBOSE: print("Dropping %s, with only %d covariances." % (badcol,foo[badcol]))
        else:
            HasMiss=False

    return X

def svd_missing(A,max_rank=None,min_obs=None,heteroskedastic=False):
    """Singular Value Decomposition with missing values

    Returns matrices U,S,V.T, where A~=U*S*V.T.

    Inputs: 
        - A :: matrix or pd.DataFrame, with NaNs for missing data.

        - max_rank :: Truncates the rank of the representation.  Note
                      that this impacts which rows of V will be
                      computed; each row must have at least max_rank
                      non-missing values.  If not supplied rank may be
                      truncated using the Kaiser criterion.

        - min_obs :: Smallest number of non-missing observations for a 
                     row of U to be computed.

        - heteroskedastic :: If true, use the "heteroPCA" algorithm
                       developed by Zhang-Cai-Wu (2018) which offers a
                       correction to the svd in the case of
                       heteroskedastic errors.  If supplied as a pair,
                       heteroskedastic[0] gives a maximum number of
                       iterations, while heteroskedastic[1] gives a
                       tolerance for convergence of the algorithm.

    Ethan Ligon                                        September 2021

    """
    max_its=50
    tol = 1e-3

    P=missing_inner_product(A,min_obs=min_obs) # P = A.T@A

    def heteropca(C,r=1,max_its=max_its,tol=tol):
        """Estimate r factors and factor weights of covariance matrix C."""

        N = C - np.diag(np.diag(C))

        NLast = 1
        t = 0
        while np.linalg.norm(N-NLast)>tol and t<max_its:
            NLast = N

            u,s,vt = np.linalg.svd(N,full_matrices=False)

            Ntilde = u[:,:r]@np.diag(s[:r])@vt[:r,:]

            N = N - np.diag(np.diag(N)) + np.diag(np.diag(Ntilde))

            t += 1

        if t==max_its:
            warnings.warn("Exceeded maximum iterations (%d)" % max_its)

        s = np.sqrt(s[:r])
        
        u = u[:,:r]

        return u,s

    sigmas,u=np.linalg.eigh(P)

    order=np.argsort(-sigmas)
    sigmas=sigmas[order]

    # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
    u=u[:,order]
    u=u[:,sigmas>0]
    s=np.sqrt(sigmas[sigmas>0])

    if max_rank is not None and len(s) > max_rank:
        u=u[:,:max_rank]
        s=s[:max_rank]

    r=len(s)

    if heteroskedastic:
        try:
            max_its,tol = heteroskedastic
        except TypeError:
            pass
            
        u,s = heteropca(P,r=r,max_its=max_its,tol=tol)
    
    us=u@np.diag(s)

    v=np.zeros((len(s),A.shape[1]))
    for j in range(A.shape[1]):
        a=A.iloc[:,j].values.reshape((-1,1))
        x=np.nonzero(~np.isnan(a))[0] # non-missing elements of vector a
        if len(x)>=r:
            v[:,j]=(np.linalg.pinv(us[x,:])@a[x]).reshape(-1)
        else:
            v[:,j]=np.nan

    return u,s,v.T
#+END_SRC

#+results: svd_missing


Unit test drop_columns_wo_cov function using alternative implementation.

#+name: test_drop_columns_wo_cov
#+begin_src python :noweb no-export :results output :tangle ../cfe/stochastic_test/test_drop_columns_wo_cov.py
#name: test_drop_columns_wo_cov
from cfe.estimation import drop_columns_wo_covariance
import pandas as pd
import numpy as np
import pytest 

# generate random data
N = 5
M = 3
squaredf = pd.DataFrame(np.random.normal(size=(N, N)))
rectandf = pd.DataFrame(np.random.normal(size=(N, M)))

# min_obs values to test: 1, ..., M plus None
min_obs_list = [i for i in range(M)]
min_obs_list.append(None)

# create test cases by introducing na values
line = rectandf.copy()
line.iloc[1, :] = np.NaN

rand = rectandf.copy()
rand.iloc[1, 1] = np.NaN
rand.iloc[3, 2] = np.NaN

single = rectandf.copy()
single.iloc[2,2] = np.NaN

another = rectandf.copy()
another.iloc[0,1] = np.NaN 
another.iloc[1,1] = np.NaN 
another.iloc[2,1] = np.NaN 

@pytest.mark.parametrize("df", [
    line,
    rand,
    single,
    another
])
def test_equal_to_drop_cols(df):
    results = 0
    for m in min_obs_list:
        foo = drop_columns_wo_covariance(df, min_obs=m)
        baz = df.loc[:,df.count()>=m]

        print(foo)

        # get difference 
        diff = foo - baz
        diff = diff.fillna(0)

        if ~((foo.shape == baz.shape) & ((diff < 1e-5).all(axis=None))):
            results += 1

    assert results == 0

# end rest_drop_columns_wo_cov.py

#+end_src

#+RESULTS: test_drop_columns_wo_cov
: Missing dependencies for OracleDemands.

** Rank 1 Approximation

Once we've computed the SVD of a matrix we can construct an optimal rank one
approximation to that matrix using just the  first left eigenvector,
the first eigenvalue, andn the first right eigenvector.  

#+name: svd_rank1_approximation_with_missing_data
#+begin_src python :noweb no-export :results output :tangle ../cfe/estimation.py
#name: svd_rank1_approximation_with_missing_data
import pandas as pd
<<svd_missing>>

def svd_rank1_approximation_with_missing_data(x,return_usv=False,max_rank=1,
                                              min_obs=None,VERBOSE=True):
    """
    Return rank 1 approximation to a pd.DataFrame x, where x may have
    elements which are missing.
    """
    x=x.copy()
    m,n=x.shape

    if min_obs is None: min_obs = 1

    if n<m:  # If matrix 'thin', make it 'short'
        x=x.T
        TRANSPOSE=True
    else:
        TRANSPOSE=False

    x=x.dropna(how='all',axis=1) # Drop any column which is /all/ missing.
    x=x.dropna(how='all',axis=0) # Drop any row which is /all/ missing.

    x=drop_columns_wo_covariance(x.T,min_obs=min_obs).T
    u,s,v = svd_missing(x,max_rank=max_rank,min_obs=min_obs)
    if VERBOSE:
        print("Estimated singular values: ",)
        print(s)

    xhat=pd.DataFrame(s*v@u.T,columns=x.index,index=x.columns).T

    if TRANSPOSE: 
        out = xhat.T
    else:
        out = xhat

    if return_usv:
        u = u.squeeze()
        if u.shape[0] == xhat.shape[1]:
            u = pd.Series(u.squeeze(),index=xhat.columns)
            v = pd.Series(v.squeeze(),index=xhat.index)
        elif u.shape[0] == xhat.shape[0]:
            u = pd.Series(u.squeeze(),index=xhat.index)
            v = pd.Series(v.squeeze(),index=xhat.columns)
        return xhat,u,s,v
    else: return xhat
#+end_src

#+results: svd_rank1_approximation_with_missing_data

** Test of Rank 1 SVD Approximation to Matrix with Missing Data

First, some code to check if approximation works for a simple, small
scale example.

#+name: svd_rank1_approximation_with_missing_data_example
#+begin_src python :noweb no-export :results output :tangle ../cfe/stochastic_test/test_svd_rank1_approximation_with_missing_data_example.py
#name: svd_rank1_approximation_with_missing_data_example
import numpy as np
import pandas as pd
<<svd_rank1_approximation_with_missing_data>>

(n,m)=(1000,500)
a=np.random.normal(size=(n,1))
b=np.random.normal(size=(1,m))
e=np.random.normal(size=(n,m))*1e-5*0

X0=np.array([[-0.22,  0.32, -0.43],
             [0.01, 0.00,  0.00],
             [-0.22,  0.31, -0.42],
             [0.01, -0.03,  0.04],
             [-0.21, 0.31, -0.38]])
X0 = np.outer(a,b) + e

X0=X0-X0.mean(axis=1).reshape((-1,1))

X=X0.copy()
X[0,0]=np.nan
X[0,1]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

def test_symmetry_of_svd_rank1_approximation():
    Xhat=svd_rank1_approximation_with_missing_data(X0,VERBOSE=False)
    XhatT=svd_rank1_approximation_with_missing_data(X0.T,VERBOSE=False)
    assert np.all(Xhat.T == XhatT)

def test_accuracy_of_svd_rank1_approximation():
    Xhat=svd_rank1_approximation_with_missing_data(X,VERBOSE=False)
    error = X0 - Xhat
    assert np.max(np.max(error)<1e-2)
  
Xhat=svd_rank1_approximation_with_missing_data(X,VERBOSE=False)
XhatT=svd_rank1_approximation_with_missing_data(X0.T,VERBOSE=False)

print(X0)
print(X)
print(Xhat)
print((X0-Xhat)/X0)

assert np.linalg.norm((X0-Xhat)/X0,ord=np.inf)//np.sqrt(np.prod(X0.shape)) < 1e-2
#+end_src

#+results: svd_rank1_approximation_with_missing_data_example
#+begin_example
      0     1     2     3     4
0 -0.22  0.01 -0.22  0.01 -0.21
1  0.32  0.00  0.31 -0.03  0.31
2 -0.43  0.00 -0.42  0.04 -0.38
      0     1     2     3     4
0 -0.22  0.01 -0.22  0.01 -0.21
1  0.32  0.00  0.31 -0.03  0.31
2 -0.43  0.00 -0.42  0.04 -0.38
          0         1         2         3         4
0 -0.223913  0.001494 -0.218917  0.019316 -0.206088
1  0.324141 -0.002162  0.316909 -0.027962  0.298337
2 -0.424777  0.002834 -0.415299  0.036644 -0.390962
          0         1         2         3         4
0 -0.017786  0.850623  0.004924 -0.931615  0.018629
1 -0.012941       inf -0.022286  0.067917  0.037622
2  0.012146      -inf  0.011192  0.083900 -0.028847
#+end_example

#+name: svd_rank1_approximation_with_missing_data_test
#+begin_src python :noweb no-export :results output :var n=12 :var m=2000 :var percent_missing=0.5 :var SEED=0 :tangle ../cfe/test/svd_rank1_approximation_with_missing_data_test.py
#name: svd_rank1_approximation_with_missing_data_test
import numpy as np
import pandas as pd

# Tangling may not include :vars from header
try: 
    SEED
except NameError: # :var inputs not set?
    n=12
    m=2000
    percent_missing=0.5
    SEED=0
  
<<svd_rank1_approximation_with_missing_data>>

if SEED:
    np.random.seed(SEED)

a=np.random.normal(size=(n,1))
b=np.random.normal(size=(1,m))
e=np.random.normal(size=(n,m))*5e-1

X0=np.outer(a,b) + e
X0=X0-X0.mean(axis=0)

X=X0.copy()
X[np.random.random_sample(X.shape)<percent_missing]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

Xhat,u,s,v=svd_rank1_approximation_with_missing_data(X,VERBOSE=False,return_usv=True)

#rho_a=np.corrcoef(np.c_[a,u[:,0]],rowvar=0)[0,1]
rho_a=pd.DataFrame({'a':a.reshape(-1),'u':u}).corr().iloc[0,1]
rho_b=pd.DataFrame({'b':pd.Series(b.reshape(-1)),'v':v}).corr().iloc[0,1]
missing=np.isnan(X.values).reshape(-1,1).mean()
print("Proportion missing %g and correlations are %5.4f and %5.4f." % (missing, rho_a,rho_b),)
print("Singular value=%g" % s[0],)
if SEED: print("Seed=%g" % SEED)
else: print()
#+end_src

#+results: svd_rank1_approximation_with_missing_data_test




** Test of construction of approximation to CE
#+begin_src python  :noweb no-export :results output :tangle ../cfe/test/NOtest_approximation.py
import numpy as np
<<estimate_reduced_form>>
<<artificial_data>>
<<df_utils>>
<<svd_rank1_approximation_with_missing_data>>

y,truth=artificial_data(T=1,N=1000,n=12,sigma_e=1e-1)
#y,truth=artificial_data(T=2,N=20,n=6,sigma_e=1e-8)
beta,L,dz,p=(truth.beta,truth.lambdas,truth.characteristics,truth.prices)

numeraire='x0'

b0,ce0,d0=estimate_bdce_with_missing_values(y,np.log(dz),return_v=False)
myce0=ce0.copy()
cehat=svd_rank1_approximation_with_missing_data(myce0)

rho=pd.concat([ce0.stack(dropna=False),cehat.stack()],axis=1).corr().iloc[0,1]

print("Norm of error in approximation of CE: %f; Correlation %f." % (df_norm(cehat,ce0)/df_norm(ce0),rho))
#+end_src

#+results:

* Estimation of reduced form

    This code takes as input time-varying household-level data on log
    expenditures and characteristics, and takes data defining markets
    and perhaps some prices.

    Data on prices is specified by providing a =pd.DataFrame= =P= with
    a MultiIndex of (period,market) indicated as =('t','m')=.  If
    provided, the dataframe =P= includes data on actual prices
    observed in different period-markets.  These data need not be
    complete, and in particular it's fine to provide prices for only a
    subset of goods.  However, if one or more prices is provided, one
    of the commodities should be chosen as a numéraire e.g.,
#+BEGIN_SRC python :exports code
ix=pd.MultiIndex.from_tuples([(1975,'Aurepalle'),(1975,'Shirapur'),(1975,'Kanzara'),
                              (1976,'Aurepalle'),(1976,'Shirapur'),(1976,'Kanzara'),
                              (1977,'Aurepalle'),(1977,'Shirapur'),(1977,'Kanzara'),
                              (1978,'Aurepalle'),(1978,'Shirapur'),(1978,'Kanzara')],names=['t','m'])
P=pd.DataFrame({'Rice':[4,5,4,5,6,5,6,7,6,7,8,7],
                'Sorghum':[2,3,2,2,3,2,3,4,3,4,5,6]},index=ix)

numeraire='Rice'
#+END_SRC

#+RESULTS:

Note that not all goods for which household level expenditures are
observed need to have  price supplied.  If prices for one good are
supplied, it should be the numéraire; if prices for two or more goods
are supplied it's possible to identify Frisch elasticities $\beta$ and
to estimate any missing prices.  

#+name: estimate_reduced_form
#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/estimation.py
#name: estimate_reduced_form
import pandas as pd
import warnings
import sys
from collections import OrderedDict
from cfe.df_utils import drop_missing, ols, arellano_robust_cov, broadcast_binary_op, use_indices, df_norm

def estimate_reduced_form(y,z,return_v=False,return_se=False,VERBOSE=False):
  """Estimate reduced-form Frisch expenditure/demand system.

  Inputs:
      - y : pd.DataFrame of log expenditures or log quantities, indexed by (j,t,m),
            where j indexes the household, t the period, and m the market.
            Columns are different expenditure items, indexed by i.

      - z : pd.DataFrame of household characteristics; index should match that of y.
            Columns are different characteristics, indexed by l.

  Outputs:
      - a : Estimated good-time-market fixed effects.

      - ce : Residuals (can be provided as an input to get_log_lambdas()).

      - d : Estimated coefficients associated with characteristics z.

      - sed : (Optional, if return_se) Estimated standard errors for coefficients d.

      - sea : (Optional, if return_se) Estimated standard errors for coefficients a.

      - V : (Optional, if return_v) Estimated covariance matrix of coefficients d.

  Ethan Ligon                                            February 2017
  """
  try: # Be a little forgiving if t or m index is missing.
      assert y.index.names==['j','t','m'], "Indices should be (j,t,m)?"
      assert y.columns.name == 'i', "Name of column index should be i?"
  except AssertionError:
      y = y.reset_index()
      if not 'm' in y.columns: y['m']=1
      if not 't' in y.columns: y['t']=1
      y = y.set_index(['j','t','m'])
      y.columns.set_names('i',inplace=True)

  try:
      assert z.index.names==['j','t','m'], "Indices should be (j,t,m)?"
      assert z.columns.name == 'k', "Name of column index should be k?"
  except AssertionError:
      z = z.reset_index()
      if not 'm' in z.columns: z['m']=1
      if not 't' in z.columns: z['t']=1
      z = z.set_index(['j','t','m'])

      z.columns.set_names('k',inplace=True)

  assert len(z.index.intersection(y.index))>0, "Indices of z & y don't match."

  periods = list(set(y.index.get_level_values('t')))
  ms = list(set(y.index.get_level_values('m')))

  # Time-market dummies
  DateLocD = use_indices(y,['t','m'])
  DateLocD = pd.get_dummies(list(zip(DateLocD['t'],DateLocD['m'])))
  DateLocD.index = y.index

  sed = pd.DataFrame(columns=y.columns)
  sea = pd.DataFrame(columns=y.columns)
  a = pd.Series(index=y.columns,dtype=float)
  b = OrderedDict()
  d = OrderedDict()
  ce = pd.DataFrame(index=y.index,columns=y.columns)
  V = OrderedDict()

  for i,Item in enumerate(y.columns):
      if VERBOSE: print(Item)

      lhs,rhs=drop_missing([y.iloc[:,[i]],pd.concat([z,DateLocD],axis=1)])
      stdev = rhs.std()
      for constant in stdev[stdev==0].index.tolist():
          warnings.warn("No variation in: %s" % str(constant))
      rhs=rhs.loc[:,rhs.std()>0] # Drop  any X cols with no variation
      useDateLocs=list(set(DateLocD.columns.tolist()).intersection(rhs.columns.tolist()))

      # Calculate deviations
      lhsbar=lhs.mean(axis=0)
      assert ~np.any(np.isnan(lhsbar)), "Missing data in lhs for item %s." % Item
      assert np.all(lhs.std()>0), "No variation in non-missing data for item %s." % Item
      lhs=lhs-lhsbar
      lhs=lhs-lhs.mean(axis=0)

      rhsbar=rhs.mean(axis=0)
      assert ~np.any(np.isnan(rhsbar)), "Missing data in rhs?"
      rhs=rhs-rhsbar
      rhs=rhs-rhs.mean(axis=0)

      # Need to make sure time-market effects sum to zero; add
      # constraints to estimate restricted least squares
      ynil=pd.DataFrame([0],index=[(-1,0,0)],columns=lhs.columns)
      znil=pd.DataFrame([[0]*z.shape[1]],index=[(-1,0,0)],columns=z.columns)
      timednil=pd.DataFrame([[1]*DateLocD.shape[1]],index=[(-1,0,0)],columns=DateLocD.columns)

      # change append to concat: X=rhs.append(znil.join(timednil))
      X=pd.concat([rhs, znil.join(timednil)])
      X=X.loc[:,X.std()>0] # Drop  any X cols with no variation

      # Estimate d & b
      # change append to concat: lhs.append(ynil)
      myb,mye=ols(X,pd.concat([lhs, ynil]) ,return_se=False,return_v=False,return_e=True)
      ce[Item]=mye.iloc[:-1,:] # Drop constraint that sums time-effects to zero

      if return_v or return_se:
          if z.shape[1]:
              V[Item]=arellano_robust_cov(z,ce[Item])
              sed[Item]=pd.Series(np.sqrt(np.diag(V[Item])), index=z.columns) # reduced form se on characteristics

              stderrs = (mye.groupby(['t','m']).std()/np.sqrt(mye.groupby(['t','m']).count()))
              if len(useDateLocs) > 0:
                  sea[Item] = stderrs.squeeze()
              else:
                  sea[Item] = stderrs[Item]

      zvars = z.columns.intersection(myb.index)
      d[Item]= myb.loc[zvars].squeeze() # reduced form coefficients on characteristics

      b[Item] = myb.loc[useDateLocs].squeeze()  # Terms involving prices
      a[Item] = lhsbar.mean() - d[Item].squeeze().dot(rhsbar[zvars]) - np.array(b[Item]).dot(rhsbar[useDateLocs])

  b = pd.DataFrame(b,index=y.groupby(level=['t','m']).mean().index)
  b = b.T
  sed = sed.T
  sea = sea.T

  if b.shape[1]==1: # Only a single time-market
    assert np.all(np.isnan(b)), "Only one good-time effect should mean b not identified"
    b[:]=0

  d = pd.DataFrame(d).T
  d.index.name = 'i'

  out = [b.add(a,axis=0),ce,d]
  if return_se:
      out += [sed,sea]
  if return_v:
      V = xr.Dataset(V).to_array(dim='i')
      out += [V]
  return out
#+END_SRC

#+RESULTS: estimate_reduced_form
: Missing dependencies for OracleDemands.

** Test
   If we use a set of fixed parameters to generate artificial data, we
   should be able to recover some of these parameters from =estimate_reduced_form=.
   Below we construct a simple test of this.
#+name: test_estimate_reduced_form
#+BEGIN_SRC python :results output :var T=1 :var N=5000 :var n=6 :tangle ../cfe/test/NOtest_estimate_reduced_form.py
#name: test_estimate_reduced_form

from scipy.stats.distributions import chi2

# Tangling may not include :vars from header
try:
    N
except NameError: # :var inputs not set?
    N=5000
    T=1
    n=6

<<lambdas_dgp>>
<<characteristics_dgp>>
<<prices_dgp>>
<<expenditures_dgp>>
<<estimate_reduced_form>>

x,parts = expenditures(N,T,1,n,2,np.array([0.5,1.,1.5,2.,2.5,3.]),sigma_phi=0.0,sigma_eps=0.01)
x = x.where(x>0,np.nan)  # Zeros to missing

x = x.to_dataframe('x').unstack('i')
x.columns = x.columns.droplevel(0)

z = parts['characteristics'].to_dataframe('z').unstack('k')
z.columns = z.columns.droplevel(0)
z.columns = [chr(i) for i in range(ord('a'),ord('a')+len(z.columns))]

b,ce,d,se,sea,V = estimate_reduced_form(np.log(x),np.log(z),return_se=True,return_v=True)

z2 = ((d-1)/se)**2

J=z2.sum().sum()
p=(1 - chi2.cdf(J,len(z2)))

try:
    DRAWS
except NameError:
    assert p > 0.01, "Shouldn't often reject coefficients on characteristics all equal to 1: (d,se)=(%s,%s)" % (d,se)

print(p)
#+END_SRC

#+results: test_estimate_reduced_form

The preceding creates a random sample with  known parameters =d=;
estimates of =d= should all be equal to 1 in expectation.  We
construct a statistic =J= which should be asymptotically distributed
$\chi^2$.  The code below resamples to  determine whether in fact we
match the correct distribution.  We construct  a =pp_plot= which
should deliver a line close to 45 degrees if all is well.

#+BEGIN_SRC python :results output :var DRAWS=200  SEED=0 :tangle ../cfe/stochastic_test/monte_carlo_estimate_reduced_form.py
import pylab as pl
import numpy as np

# Tangling may not include :vars from header
try:
    DRAWS
except NameError: # :var inputs not set?
    DRAWS = 200
    SEED = 228

if SEED:
    np.random.seed(seed=SEED)

T=1
N=10000
n=6

def empirical_cdf(x):
    """
    Return the empirical cdf of a univariate vector or series x.
    """
    x=np.array(x)

    return lambda p: (x<p).mean()

def pp_plot(F,G,interval=(0,1),npts=100):
    """
    Construct p-p plot of cdf F vs CDF G.
    """
    Q=np.linspace(interval[0],interval[1],npts)
    xy=[]
    for q in Q:
        xy.append([F(q),G(q)])

    xy=np.array(xy)
    ax=pl.plot(xy[:,0],xy[:,1])

    return xy


Jay=[]
Dee=[]
Vee=[]
for i in range(DRAWS):
    print(i)
    <<test_estimate_reduced_form>>
    Dee.append(d.values.squeeze().tolist())
    Jay.append(J)

    Vee.append((se**2).squeeze().values.tolist())

Dee=np.array(Dee)
Jay=np.array(Jay)
Vee=np.array(Vee)

F=empirical_cdf(Jay)
G=lambda x: chi2.cdf(x, np.prod(d.shape))

xy=pp_plot(F,G,interval=chi2.interval(.999, np.prod(d.shape)))

assert np.linalg.norm(Dee.std(axis=0) - np.sqrt(Vee.mean(axis=0))) < 0.01
#+END_SRC

#+RESULTS:

*** Test with one period, one market

    This is a simple test of the stage one SUR estimation with a
    single period and a single market.

#+name: test_one_period_one_market
#+begin_src python :tangle ../cfe/test/test_one_period_one_market.py
#name: test_one_period_one_market

import cfe
import numpy as np

J = 100
T = 1
M = 1
n = 20
k = 2

def some_result(M,T):
    x, stuff = cfe.dgp.expenditures(J,T,M,n,k,np.linspace(0,1,n),rho_lz=0)

    y = np.log(x)
    z = stuff.characteristics

    result = cfe.Result(y=y,z=z)

    result.get_reduced_form()

    return result

def test_se_a():
    """Compute reduced_form for cases with singular m and t,
       and multiple m and t."""

    result = some_result(1,1)
    assert np.all(result.se_a>0)

    result = some_result(2,1)
    assert np.all(result.se_a>0)

    result = some_result(1,2)
    assert np.all(result.se_a>0)

    result = some_result(2,2)
    assert np.all(result.se_a>0)

#+end_src


* Extraction of Frisch Elasticities and Neediness
#+name: get_loglambdas
#+begin_src python :noweb no-export :results output :tangle ../cfe/estimation.py
#name: get_loglambdas

import pandas as pd

try: 
    from joblib import Parallel, delayed
    #import timeit
    PARALLEL=True
except ImportError:
    PARALLEL=False
    #warnings.warn("Install joblib for parallel bootstrap.")

PARALLEL = False # Not yet working.

def get_loglambdas(e,TEST=False,time_index='t',max_rank=1,min_obs=None,VERBOSE=False):
    """
    Use singular-value decomposition to compute loglambdas and price elasticities,
    up to an unknown factor of proportionality phi.

    Input e is the residual from a regression of log expenditures purged
    of the effects of prices and household characteristics.   The residuals
    should be arranged as a matrix, with columns corresponding to goods. 
    """ 

    assert e.shape[0]>e.shape[1], "More goods than observations."

    chat = svd_rank1_approximation_with_missing_data(e,VERBOSE=VERBOSE,max_rank=max_rank,min_obs=min_obs).T

    R2 = chat.var()/e.var()

    # Possible that initial elasticity b_i is negative, if inferior goods permitted.
    # But they must be positive on average.
    if chat.iloc[0,:].mean()>0:
        b=chat.iloc[0,:]
    else:
        b=-chat.iloc[0,:]

    loglambdas=(-chat.iloc[:,0]/b.iloc[0])

    # Find phi that normalizes first round loglambdas
    phi=loglambdas.groupby(level=time_index).std().iloc[0]
    loglambdas=loglambdas/phi

    loglambdas=pd.Series(loglambdas,name='loglambda')
    bphi=pd.Series(b*phi,index=e.columns,name=r'\phi\beta')

    if TEST:
        foo=pd.DataFrame(-np.outer(bphi,loglambdas).T,index=loglambdas.index,columns=bphi.index)
        assert df_norm(foo-chat)<1e-4
        #print("blogL norm: %f" % np.linalg.norm(foo-chat))

    return bphi,loglambdas

def iqr(x):
    """The interquartile range of a pd.Series of observations x."""
    q=x.quantile([0.25,0.75])

    try:
        return q.diff().iloc[1]
    except AttributeError:
        return np.nan

def bootstrap_elasticity_stderrs(e,clusterby=['t','m'],tol=1e-2,minits=30,return_v=False,return_samples=False,VERBOSE=False,outfn=None,TRIM=True):
    """Bootstrap estimates of standard errors for \\phi\\beta.

    Takes pd.DataFrame of residuals as input.

    Default is to `cluster' by (t,m) via a block bootstrap.

    If optional parameter TRIM is True, then calculations are
    performed using the interquartile range (IQR) instead of the
    standard deviation, with the standard deviation computed as
    IQR*0.7416 (which is a good approximation provided the
    distribution is normal).

    Ethan Ligon                              January 2017
    """

    def resample(e):
        #e = e.iloc[np.random.random_integers(0,e.shape[0]-1,size=e.shape[0]),:]
        e = e.iloc[np.random.randint(0,e.shape[0],size=e.shape[0]),:]
        e = e - e.mean()
        return e

    def new_draw(e,clusterby):      
        if clusterby:
            S=e.reset_index().groupby(clusterby,as_index=True)[e.columns].apply(resample)
        else:
            S=resample(e)

        bs,ls=get_loglambdas(S)

        return bs

    if outfn: outf=open(outfn,'a')

    delta=1.
    old = pd.Series([1]*e.shape[1])
    new = pd.Series([0]*e.shape[1])
    i=0
    chunksize=2

    assert chunksize>=2, "chunksize must be 2 or more."
    while delta>tol or i < minits:
        delta=np.nanmax(np.abs(old.values.reshape(-1)-new.values.reshape(-1)))
        if VERBOSE and i>chunksize: 
            stat = np.nanmax(np.abs((std0.values.reshape(-1)-std1.values.reshape(-1))/std0.values.reshape(-1)))
            print("Draws %d, delta=%5.4f.  Measure of non-normality %6.5f." % (i, delta, stat))
        old=new

        if PARALLEL:
            #start=timeit.timeit()
            bees = Parallel(n_jobs=chunksize)(delayed(new_draw)(e,clusterby) for chunk in range(chunksize))
            #print(timeit.timeit() - start)
        else:
            #start=timeit.timeit()
            bees = [new_draw(e,clusterby) for chunk in range(chunksize)]
            #print(timeit.timeit() - start)

        if outfn: 
            for bs in bees:
                if np.any(np.isnan(bs)):
                    warnings.warn("Resampling draw with no data?")
                outf.write(','.join(['%6.5f' % b for b in bs])+'\n')

        try:
            B=B.append(bees,ignore_index=True)
        except NameError:
            B=pd.DataFrame(bees,index=range(chunksize)) # Create B

        i+=chunksize

        std0=B.std()
        std1=B.apply(iqr)*0.7416 # Estimate of standard deviation, with trimming
        if TRIM:
            new=std1
        else:
            new=std0

    if outfn: outf.close()

    out = [new]
    if return_samples:
        B.dropna(how='all',axis=1,inplace=True) # Drop any goods always missing estimate
        out += [B]

    if return_v:
        B.dropna(how='all',axis=1,inplace=True) # Drop any goods always missing estimate
        out += [B.cov()]

    if len(out)==1:
        return out[0]
    else:
        return out
#+end_src

*** Test of get_loglambdas
#+name: test_get_loglambdas
#+begin_src python :noweb no-export :results output :var miss_percent=0.6 :tangle ../cfe/stochastic_test/test_get_loglambdas.py
#name: test_get_loglambdas

import numpy as np
import pandas as pd
import warnings

# Tangling may not include :vars from header
try: 
    miss_percent
except NameError: # :var inputs not set?
    miss_percent = 0.6

<<get_loglambdas>>
<<svd_rank1_approximation_with_missing_data>>
<<df_utils>>

(n,m)=(50,5000)
a=np.random.random_sample((n,1))
b=np.random.random_sample((1,m))
e=np.random.random_sample((n,m))*1e-5

X0=np.outer(a,b)+e

X=X0.copy()
X[np.random.random_sample(X.shape)<miss_percent]=np.nan

X0=pd.DataFrame(X0).T
X0.index.name='j'
X0['t']=0
X0['m']=0
X0=X0.reset_index().set_index(['j','t','m'])
X=pd.DataFrame(X).T
X.index=X0.index

ahat,bhat=get_loglambdas(X,TEST=True)

Xhat=pd.DataFrame(np.outer(pd.DataFrame(ahat),pd.DataFrame(-bhat).T).T,index=X.index)

def test_svd_vs_truth_error():
    error = df_norm(Xhat,X)/df_norm(X)
    print("%%Norm of error (svd vs. truth): %f" % error)
    assert error < 1e-2
#+end_src

*** Artificial data
We begin by generating some artificial data on expenditures.
#+name: artificial_data
#+BEGIN_SRC python :noweb no-export :results output
#name: artificial_data

import pandas as pd
<<lambdas_dgp>> #lambdas
<<prices_dgp>> # prices
<<characteristics_dgp>> # characteristics

<<expenditures_dgp>>

def artificial_data(T=2,N=120,M=1,k=2,n=4,sigma_e=0.001,sigma_phi=0.1):

    x,truth=expenditures(N,T,M,n,k,beta=np.linspace(1,3,n),sigma_phi=sigma_phi,sigma_eps=sigma_e)

    y=np.log(x)

    return y,truth

#+END_SRC

#+results: artificial_data


#+name: test_artificial_data
#+begin_src python :noweb no-export :results output :tangle ../cfe/stochastic_test/test_artificial_data.py
#name: test_artificial_data

<<artificial_data>>

def test_artificial_data(T=2,N=50,n=5,k=2):
    y,truth=artificial_data(T=T,N=N,k=k,n=n,sigma_e=1e-8)
    
    assert y.shape == (N,T,1,n)
    assert truth['characteristics'].shape == (k,N,T,1)
#+end_src 

*** Alternative approach to estimation using interactive fixed effects
Rather than using an svd to factor residuals, here's an approach that
directly estimates using interactive fixed effects.
#+name: test_interactive_fixed_effects
#+begin_src python :noweb no-export :results output :tangle ../cfe/stochastic_test/test_interactive_fixed_effects.py
#name: test_interactive_fixed_effects

<<artificial_data>>
from cfe.df_utils import use_indices, drop_missing
import numpy as np
import matplotlib.pyplot as plt

n = 40
N = 100
k = 1

y,truth = artificial_data(T=1,N=N,M=1,k=k,n=n,sigma_e=1e-12,sigma_phi=0)

y = y - y.mean(['j','t','m'])

y = y.squeeze(drop=True).to_dataframe('y').replace(-np.inf,np.nan)

idx = use_indices(y,y.index.names)

z = truth.characteristics.squeeze(drop=True).to_dataframe('z')

Z = pd.DataFrame(np.kron(z,np.eye(n)),index=idx.index,columns=y.index.levels[1])

foo = pd.DataFrame({'z':Z.stack()})
Z = foo.unstack(level=2)

x = pd.get_dummies(list(zip(idx.j,idx.i)))
x.index = y.index
#x[('','r')] = 0

x = pd.concat([x,Z],axis=1)

x = x - x.mean() # Demean RHS vars

r1 = pd.DataFrame(np.kron(np.ones((1,N)),np.eye(n)),columns=y.index)
#r1[('','r')] = 0
r1['i'] = y.index.levels[1]
r1['j'] = -1

r1.set_index(['j','i'],inplace=True)

r2 = pd.DataFrame(np.kron(np.eye(N),np.ones((1,n))),columns=y.index)
#r2[('','r')] = -1
r2['j'] = y.index.levels[0]
r2['i'] = -2
r2.set_index(['j','i'],inplace=True)

R = r1 #pd.concat([r1,r2])
zfill = pd.DataFrame(np.zeros((R.shape[0],Z.shape[1])),index=R.index,columns=Z.columns)
R = pd.concat([R,zfill],axis=1)

R = R*1e+6

W = np.r_[np.c_[x.T@x,R.T],
          np.c_[R,np.zeros([R.shape[0]]*2)]]

Y = np.r_[x.T@y,np.zeros((R.shape[0],1))]

B = pd.DataFrame(np.linalg.pinv(W)@Y,index = pd.MultiIndex.from_tuples(x.columns.tolist() + R.index.tolist())).squeeze()
B.index.names=['j','i']

gamma=B.iloc[:4000].unstack('i')

# Rank 1?

u,s,vt = np.linalg.svd(gamma)
print('Singular values:',s)

# gamma embeds beta?
plt.scatter(s[0]*vt[0,:],truth['beta'])

# gamma embeds loglambdas?
plt.scatter(u[:,0],np.log(truth['lambdas']).squeeze())

#+end_src

*** Tests of estimation with missing data

#+name: test_estimate_with_missing
#+begin_src python :noweb no-export :results output :var SEED = 227 :tangle ../cfe/test/test_estimate_with_missing.py :exports none
#name: test_estimate_with_missing

import numpy as np
from cfe.result import to_dataframe

try:
    SEED 
except NameError: # :var inputs not set?
    n=12
    m=2000
    percent_missing=0.5
    SEED=0

if SEED:
    np.random.seed(seed=SEED)

<<estimate_reduced_form>>
<<artificial_data>>
<<svd_rank1_approximation_with_missing_data>>
<<get_loglambdas>>
<<df_utils>>

y,truth=artificial_data(T=2,N=5000,k=2,n=10,sigma_e=1e-10)

y = to_dataframe(y,['j','t','m']).T
#y = y.reset_index().set_index(['j','t','m'])

#beta,L,dz,p=truth
dz = to_dataframe(truth['characteristics'],['j','t','m']).T

#dz=dz.reset_index().set_index(['j','t','m'])
dz=np.log(dz)

numeraire=None #'x0'

# Try with missing data for contrast
y.values[np.random.random_sample(y.shape)<0.0]=np.nan

y.replace(-np.inf,np.nan,inplace=True)

#b,ce,d,V=estimate_bdce_with_missing_values(y,dz,return_v=True)
b,ce,d = estimate_reduced_form(y,dz,return_v=False)

bphi,logL=get_loglambdas(ce,TEST=True)
cehat=np.outer(pd.DataFrame(bphi),pd.DataFrame(-logL).T).T
cehat=pd.DataFrame(cehat,columns=bphi.index,index=logL.index)

print("Norm of error in approximation of CE: %f" % df_norm(cehat,ce))

# Some naive standard errors

#yhat=b.T.add(cehat + (dz.dot(d.T)),axis=0,level='t')
yhat = broadcast_binary_op(cehat + dz.dot(d.T),lambda x,y: x+y,b.T)

e=y.sub(yhat)

C = pd.DataFrame({"L0":to_dataframe(np.log(truth['lambdas'])),"Lhat":logL.squeeze()}).corr()
print("Correlation of log lambda with estimate (before normalization): %f" % C.values[0][-1])
assert C.values[0][-1]>0.97, "loglambda correlation with truth too low."

if not numeraire is None:
    logL=broadcast_binary_op(logL,lambda x,y: x+y,b.loc[numeraire]) # Add term associated with numeraire good
    b=b-b.loc[numeraire]
else:
    logL=broadcast_binary_op(logL,lambda x,y: x+y,b.mean()) # Add term associated with numeraire good
    b=b-b.mean()

# Evaluate estimate of beta:
print("Norm of (bphi,beta): %f" % np.var(bphi/truth['beta'])) # Funny norm deals with fact that b only identified up to a scalar

C = pd.DataFrame({"L0":to_dataframe(np.log(truth['lambdas'])),"Lhat":logL.squeeze()}).corr()
print("Correlation of log lambda with estimate (after normalization): %f" % C.values[0][-1])
assert C.values[0][-1]>0.95, "loglambda correlation with truth too low."

print("Mean of errors:")
print(e.mean(axis=0))

def test_mean():
    assert np.abs(e.stack().mean())/e.stack().std() < 1e-2    

test_mean()
#+end_src

#+results: test_estimate_with_missing
#+begin_example
Norm of error in approximation of CE: 129.991822
Correlation of log lambda with estimate (before normalization): nan
Norm of (bphi,beta): 0.013563
Correlation of log lambda with estimate (after normalization):
                  loglambda  loglambda0
t m                                  
0 1   loglambda    1.000000    0.815898
      loglambda0   0.815898    1.000000
1 1   loglambda    1.000000    0.822589
      loglambda0   0.822589    1.000000
            loglambda  loglambda0
loglambda    1.000000    0.818635
loglambda0   0.818635    1.000000
Mean of errors:
x0    0.031482
x1    0.023607
x2    0.005174
x3    0.042346
x4   -0.022142
x5   -0.008444
x6    0.046394
x7   -0.046577
x8   -0.166377
x9    0.050700
dtype: float64
#+end_example

* Estimation of Price Elasticities
  Here we develop two distinct estimators for obtaining estimates of
  price elasticities \beta in the demand relationship
  \begin{equation}
  \label{eq:demand}
     \log c_{it}^j = -\beta_i\log p_{itk} + \delta_i^\T z_t^j - \beta_i\log\lambda^j_t,
  \end{equation}
  or the expenditure relationship
  \begin{equation}
  \label{eq:expenditure}
     \log x_{it}^j = (1-\beta_i)\log p_{itk} + \delta_i^\T z_t^j - \beta_i\log\lambda^j_t,
  \end{equation}
  given data on log prices $\log p_{itk}$ for good $i$ at time $t$ in
  market $k$, characteristics $z_t^j$, and either consumption
  $c_{it}^j$ or expenditures $x_{it}^j$.  

** Direct estimation of price elasticities
  We do not assume that $\lambda^j_t$ is observed, but do assume that
  its log is orthogonal to log prices and characteristics.  In this
  case, we can simply use a least squares estimator to directly
  recover an estimate of either $-\beta_i$ (when log quantities are
  the dependent variable) or $1-\beta_i$ (when log expenditures are).

#+name: direct_price_elasticities
#+BEGIN_SRC python :tangle ../cfe/estimation.py
#name: direct_price_elasticities

def direct_price_elasticities(y,p,z,VERBOSE=True,return_se=False,return_v=False):
    """Estimate reduced-form Frisch expenditure/demand system.

       Inputs:
         - y : pd.DataFrame of log expenditures or log quantities, indexed by (j,t,m), 
               where j indexes the household, t the period, and m the market.  
               Columns are different expenditure items.

         - p : pd.DataFrame of log prices, indexed by (t,m), with
               prices for different goods across columns.

         - z : pd.DataFrame of household characteristics; index should match that of y.


      Ethan Ligon                                            March 2017
    """
    assert(y.index.names==['j','t','m'])
    assert(z.index.names==['j','t','m'])

    periods = list(set(y.index.get_level_values('t')))
    ms = list(set(y.index.get_level_values('m')))
    sed = pd.DataFrame(columns=y.columns)
    sea = pd.DataFrame(columns=y.columns)
    a = pd.Series(index=y.columns,dtype=float)
    b = OrderedDict() #pd.DataFrame(index=y.columns)
    d = OrderedDict() #pd.DataFrame(index=y.columns,columns=z.columns).T
    ce = pd.DataFrame(index=y.index,columns=y.columns)
    V = pd.Panel(items=y.columns,major_axis=z.columns,minor_axis=z.columns)

    for i,Item in enumerate(y.columns):
        if VERBOSE: print(Item)
        if np.any(np.isnan(p[Item])): continue # Don't estimate with missing prices

        rhs = z.reset_index('j').join(p[Item]).reset_index().set_index(['j','t','m'])
        rhs.rename(columns={Item:'log p'},inplace=True)

        lhs,rhs=drop_missing([y.iloc[:,[i]],rhs])

        rhs['Constant']=1

        myb,mye=ols(rhs,lhs,return_se=False,return_v=False,return_e=True) 
        ce[Item]=mye

        if return_v or return_se:
            V[Item]=arellano_robust_cov(rhs,mye)
            sed[Item]=pd.Series(np.sqrt(np.diag(V[Item])), index=z.columns) # reduced form se on characteristics

        d[Item]=myb[z.columns] # reduced form coefficients on characteristics

        a[Item] = myb['Constant']
        b[Item] = myb['log p'].values[0]

    b = pd.Series(b)

    d = pd.concat(d.values())

    out = [a,b,ce,d]
    if return_se:
        out += [sed]
    if return_v:
        out += [V]
    return out
#+END_SRC

** Indirect estimation of price elasticities

  A second approach is /indirect/, obtaining estimated elasticities by
  regressing the good-time-market effects obtained from
  =estimated_reduced_form= on $\log p_{itk} - \mbox{Proj}(\log
  p_{itk} | \bar z_{tk})$.  This exploits the relationship between
  these latent variables and implicit prices.  An important virtue of
  this approach is that if we have data for prices only on a subset of
  goods we can nevertheless estimate the first stage even for those
  goods where prices are missing.

#+BEGIN_SRC python :tangle ../cfe/estimation.py
def indirect_price_elasticities(a,p,zbar):
    """Estimate reduced-form Frisch expenditure/demand system.

       Inputs:
         - a : pd.DataFrame of good-time-market effects estimated by =estimate_reduced_form=,
               indexed by (t,m), where t indexes the period, and m the market.  
               Columns are different expenditure items.

         - p : pd.DataFrame of log prices, indexed by (t,m), with
               prices for different goods across columns.

         - zbar : pd.DataFrame of average household characteristics; index should match that of a.

      Ethan Ligon                                            March 2017
    """
    assert(a.index.names==['t','m'])
    assert(zbar.index.names==['t','m'])

    # Filter p
    X=zbar.copy()
    X['Constant'] = 1
    y = p.dropna(how='any',axis=1)

    # pe are filtered log prices
    bp,pe = ols(X,y,return_se=False,return_e=True)

    X = pe.copy()

    Xm = (X-X.mean()).values

    ym = (a-a.mean()).values
  
    B=OrderedDict()
    SE=OrderedDict()
    for i,Item in enumerate(y.columns):
        B[Item] = np.linalg.lstsq(Xm[:,i],ym[:,i])[0][0,0]
        e = ym[:,i] - Xm[:,i]@B[Item]
        SE[Item] = np.sqrt(np.var(e)/np.var(Xm[:,i]))

    B = pd.Series(B)
    SE = pd.Series(SE)
    return B,SE
#+END_SRC
  

** Test
   The direct and indirect methods  should yield similar results.
   Below we construct a simple test of this.
#+BEGIN_SRC python :var T=20 N=1000 n=6 :tangle ../cfe/test/price_elasticities.py
# Tangling may not include :vars from header
try: 
    T
except NameError: # :var inputs not set?
    n=6
    N = 1000
    T =20 

<<lambdas_dgp>>
<<characteristics_dgp>>
<<prices_dgp>>
<<expenditures_dgp>>

x,parts = expenditures(T,N,n,1,np.array([0.5,1.,1.5,2.,2.5,3.]),sigma_phi=0.01,sigma_eps=0.01)

print(x.head())

#+END_SRC

#+RESULTS:

* Iterated Regression

#+begin_src python :tangle ../cfe/estimation.py
def iterated_regression(y,z,return_se=False,return_v=False,VERBOSE=False,tol=1e-3,max_its=30,cores=None):
    """Estimate (delta,beta,loglambda).
    """

    # Create location-time dummies
    dm = use_indices(y,['t','m'])
    DateLocD = pd.get_dummies(zip(dm['t'],dm['m']))
    dm = sorted(list(set(zip(dm['t'],dm['m']))))

    DateLocD.index = y.index
    DateLocD.columns = pd.MultiIndex.from_tuples(dm)

    loglambda = pd.Series(np.random.randn(z.shape[0]),index=z.index,name='loglambda')
    X = pd.concat([z,DateLocD],axis=1)
    X['loglambda'] = loglambda

    stdev = X.std()
    for constant in stdev[stdev==0].index.tolist():
        warnings.warn("No variation in: %s" % str(constant))

    X = X.loc[:,stdev>0] # Drop  any X cols with no variation

    coeffs_last = np.inf
    coeffs = 0

    its = 0
    while  (its < 5) or (np.linalg.norm(coeffs_last - coeffs) > tol) and (its < max_its):

        if (its>1) and VERBOSE: 
            print("Iteration %d, Norm: %g" % (its,np.linalg.norm(coeffs_last-coeffs)))
            print(coeffs - coeffs_last)

        coeffs_last = coeffs

        def _regress(ycol): # Inherits recently defined X and y

            x,_y = drop_missing([X,y[ycol]])
            _y = _y.squeeze()

            stdev = x.std()
            for constant in stdev[stdev==0].index.tolist():
                warnings.warn("No variation in: %s" % str(constant))

            x = x.loc[:,stdev>0] # Drop  any X cols with no variation

            b = pd.Series(np.linalg.lstsq(x,_y,rcond=None)[0],index=x.columns,name=_y.name)

            e = y[ycol] - x@b  # Include missings in e

            return b,e

        if cores is not None:
            ests = cores.map(_regress,[i for i in y.columns])
        else:
            ests = map(_regress,[i for i in y.columns])

        b,e = zip(*ests)
        coeffs = pd.DataFrame(b,index=y.columns)
        e = pd.DataFrame(e,index=y.columns).T

        delta = coeffs[z.columns]
        delta.columns.name = 'k'

        ce = y - z@delta.T
        ce = ce - ce.mean()

        X['loglambda'] = get_loglambdas(ce)[1]

        its += 1

    if VERBOSE and its >= max_its: print("Exceeded max_its")

    my_dm = coeffs.columns.intersection(dm)
    a = coeffs[my_dm]
    a.columns = pd.MultiIndex.from_tuples(my_dm)
    a.columns.names = ['t','m']
    b = coeffs['loglambda']

    d = coeffs[z.columns]

    out = [a,b,d,e,X['loglambda']]

    V = {}
    SE = {}
    if return_se or return_v:
        usecols = z.columns.tolist() + ['loglambda']
        for Item in e.columns:
            v = arellano_robust_cov(X[usecols],e[Item])
            V[Item] = v
            se = dict(zip(usecols,np.sqrt(np.diag(V[Item])))) # reduced form se on characteristics
              
            stderrs = (e[Item].groupby(['t','m']).std()/np.sqrt(e[Item].groupby(['t','m']).count())).tolist()
            se.update(dict(zip(my_dm,stderrs)))
            SE[Item] = pd.Series(se)

        SE = pd.DataFrame(SE).T
        SE.index.name = 'i'
        out = out + [SE,V]

    return tuple(out)

#+end_src
* Analysis Omnibus
  This describes a sort of `wrapper' routine which at a minimum takes
  as input a =pd.DataFrame= of log expenditures, indexed by household,
  period, and  market =("j","t","m")=, with  columns corresponding
  to different goods.  

  In addition, one may provide a dataframe of household
  characteristics with a similar structure to the dataframe of
  expenditures, save that columns will correspond to different
  household characteristics.  

  Finally, one may provide a dataframe of prices.  The structure of
  this dataframe is described above in Section [[*Estimation of reduced form][Estimation of reduced
  form]]. 

  The analysis omnibus performs a sequence of estimation steps,
  returning an "omnibus" of outputs in a dictionary.  These include
  estimated demand parameters, household IMUEs, and output from an ANOVA
  analysis, among others.

#+name: analysis_omnibus
#+begin_src python :noweb no-export :exports code :tangle ../cfe/estimation.py
#name: analysis_omnibus

# -*- coding: utf-8 -*-

import tempfile
import numpy as np
import pandas as pd
from numpy.linalg import norm

def analysis_omnibus(y, z=None, prices=None, numeraire=None,min_xproducts=30,min_proportion_items=1./8,
                     VERBOSE=False, BOOTSTRAP=False):

    if BOOTSTRAP is True: # Bootstrap also a tolerance parameter
        BOOTSTRAP = 1e-3

    if z is None:
       z = pd.DataFrame(index=y.index)

    if prices is not None: # Check price indices (t,m) consistent with indices in y
        assert set([tuple(x) for x in prices.index.levels]) == set([tuple(x) for x in y.index.levels[1:]]), \
               "Must have prices for every (t,m) in expenditures y."

    results={'y':y,'z':z}
    if prices is not None: results['prices'] = prices

    firstround=y.reset_index().iloc[0]['t']  

    # Deflate expenditures and prices by prices of numeraire good.
    if numeraire is not None and len(numeraire)>0:
        y = broadcast_binary_op(y, lambda foo,bar: foo-bar, np.log(prices[numeraire]))
        logp=np.log(prices).sub(np.log(prices[numeraire]),axis=0)

    use_goods = y.columns.tolist()

    # The criterion below (hh must have observations for at least min_proportion_items of goods) ad hoc
    using_goods=(y[use_goods].T.count()>=np.floor(len(use_goods) * min_proportion_items))
    y=y.loc[using_goods,use_goods] # Drop households with too few expenditure observations, keep selected goods
    y = drop_columns_wo_covariance(y,min_obs=min_xproducts,VERBOSE=False)
    # Only keep goods with observations in each (t,m)
    y = y.loc[:,(y.groupby(level=['t','m']).count()==0).sum()==0] 

    a,ce,d,sed,sea,V = estimate_reduced_form(y,z,return_se=True,return_v=True,VERBOSE=VERBOSE)
    ce.dropna(how='all',inplace=True)
    se = sed

    results['ce']=ce
    results['delta_covariance'] = V

    bphi,logL = get_loglambdas(ce,TEST=True,min_obs=30)

    assert np.abs(logL.groupby(level='t').std().iloc[0] - 1) < 1e-12, \
           "Problem with normalization of loglambdas"

    cehat=np.outer(pd.DataFrame(bphi),pd.DataFrame(-logL).T).T
    cehat=pd.DataFrame(cehat,columns=bphi.index,index=logL.index)
    results['cehat']=cehat

    if VERBOSE:
        print("Norm of error in approximation of CE divided by norm of CE: %f" % (df_norm(cehat,ce)/df_norm(ce)))

    # Some naive standard errors & ANOVA
    miss2nan = ce*0
    anova=pd.DataFrame({'Prices':a.T.var(ddof=0),
                        'Characteristics':z.dot(d.T).var(ddof=0),
                        r'$\log\lambda$':(cehat + miss2nan).var(ddof=0),
                        'Residual':(ce-cehat).var(ddof=0)})
    anova=anova.div(y.var(ddof=0),axis=0)
    anova['Total var']=y.var(ddof=0)
    anova.sort_values(by=r'$\log\lambda$',inplace=True,ascending=False)

    results['anova'] = anova

    yhat = broadcast_binary_op(cehat + z.dot(d.T),lambda x,y: x+y,a.T)

    e = y.sub(yhat)

    goodsdf=d.copy()

    pref_params=[r'$\phi\beta_i$']
    if numeraire is not None and len(numeraire)>0:
        # FIXME: Issue here with dividing by a random variable.  What
        # properties do we want estimator of barloglambda_t to have?
        try:
            barloglambda_t=-a.loc[numeraire]/bphi[numeraire]
            logL = broadcast_binary_op(logL,lambda x,y: x+y,barloglambda_t) # Add term associated with numeraire good
            a = a - pd.DataFrame(np.outer(bphi,barloglambda_t),index=bphi.index,columns=barloglambda_t.index)
        except KeyError:
            pass

        # FIXME: Should really use weighted mean, since different precisions for a across different  markets
        logalpha = a[firstround].T.mean() 
        goodsdf[r'$\log\alpha_i$'] = logalpha
        pref_params += [r'$\log\alpha_i$']
    else:
        pidx=a.mean()
        logL= broadcast_binary_op(logL,lambda x,y: x+y,pidx) # Add term associated with numeraire good
        a = a - pidx

    if VERBOSE:
        print("Mean of errors:")
        print(e.mean(axis=0))

    goodsdf[r'$\phi\beta_i$']=bphi
    goodsdf['$R^2$']=1-e.var()/y.var()

    goodsdf=goodsdf[pref_params+d.columns.tolist()+['$R^2$']]
    goodsdf['%Zero']=100-np.round(100*(~np.isnan(y[goodsdf.index])+0.).mean(),1)

    ehat=e.dropna(how='all')
    ehat=ehat-ehat.mean()

    if BOOTSTRAP:
        tmpf = tempfile.mkstemp(suffix='.csv')
        if VERBOSE: print("Bootstrapping.  Interim results written to %s." % tmpf[1])

        sel,Bs = bootstrap_elasticity_stderrs(ce,tol=1e-4,VERBOSE=VERBOSE,return_samples=True,outfn=tmpf[1])
        results['Bs'] = Bs
        se[r'$\phi\beta_i$']=sel
    else:
        sel=[]
        for i in ehat:
            foo=pd.DataFrame({'logL':logL.squeeze(),'e':ehat[i]}).dropna(how='any')
            sel.append(np.sqrt(arellano_robust_cov(foo['logL'],foo['e']).values[0,0]))
        se[r'$\phi\beta_i$']=np.array(sel)

    if numeraire is not None:
        se[r'$\log\alpha_i$']=ehat.query('t==%d' % firstround).std()/np.sqrt(ehat.query('t==%d'  % firstround).count())

    se.dropna(how='any',inplace=True)

    results['se'] = sed
    goodsdf=goodsdf.T[se.index.tolist()].T # Drop goods that we can't compute std errs for.

    goodsdf.sort_values(by=[r'$\phi\beta_i$'],inplace=True,ascending=False)
    goodsdf.dropna(how='any',inplace=True)
    results['goods'] = goodsdf

    results['a'] = a
    results['loglambda'] = logL
    results['logexpenditures'] = y
    results['logexpenditures_hat'] = yhat

    return results
#+end_src

* Distance between two estimates of \beta
  When we compute the Frisch elasticities, these are only identified
  up to an unknown parameter $\phi$ (which we might call the Pigou
  elasticity, as it relates the price and Frisch elasticities in what
  Deaton calls "Pigou's Law.").  Thus, if we have /two/ different
  estimates of $\beta$, say $\beta^1$ and $\beta^2$, we
  define the difference between these using a norm
  \begin{equation}
  \label{eq:beta_distance}
     \min_\psi ||\psi\beta^1 - \beta^2||_W.
  \end{equation}  
  Note that $\psi$ should not be regarded as an estimate of the Pigou
  elasticity, but as the /ratio/ of the Pigou elasticities
  corresponding to the two different estimates of \beta.

  To implement a test of the hypothesis that $\beta^1=\beta^2$ we
  adopt a sort of $L^2$ distance measure, defining
  \begin{equation}
  \label{eq:norm}
     {} ||\vec{x}||_W = \vec{x}^\T \vec{W}\vec{x},
  \end{equation}
  where $\vec{W}$ is some positive definite matrix.  An /optimal/ choice of
  $\vec{W}$, in  a GMM sense \citep{hansen82}, is to use
  $\vec{W}=\Cov(\vec{x})^{-1}$.  Absent prior knowledge
  regarding this  covariance matrix, if $\beta^1$ and $\beta^2$ are
  estimated using independent  samples, we observe
  that \(\Cov(\psi\beta^1 - \beta^2) = \psi^2\vec{V^1} + \vec{V^2}\), where
  $\vec{V^1}$ and $\vec{V^2}$ are the covariance matrices corresponding to
  $\beta^1$ and $\beta^2$.  More generally, if $\beta^1$ is a "pooled"
  estimate which relies on a matrix of regressors $\vec{X}$, with $N$
  rows, and $\beta^2$ is obtained by estimation on a subset $\vec{X^2}$ with
  $N_2$ rows, then we have 
  \[
     \Cov(\psi\beta^1 - \beta^2) = \psi^2\vec{V^1} + \vec{V^2}\left[\vec{I}-2\frac{N_2}{N}\left(\frac{\vec{X}^\T\vec{X}}{N}\right)^{-1}\left(\frac{\vec{X^2}^\T\vec{X^2}}{N_2}\right)\right]
  \]
  Define the scatter matrices
  $\vec{S}=\vec{X}^\T\vec{X}$ and $\vec{S_2}=\vec{X^2}^\T\vec{X^2}$.
  Then supposing that estimates of the two covariance matrices
  $(\vec{V^1},\vec{V^2})$ can be
  obtained at the same time $\beta^1$ and $\beta^2$ are estimated, we
  choose $\psi$ to minimize 
  \begin{equation}
  \label{eq:min_chi2}
  H(\beta^1,\beta^2,\vec{V^1},\vec{V^2},\vec{S},\vec{S_2}) = \min_\psi \left(\psi\beta^1 - \beta^2\right)^\T\left[\psi^2\vec{V^1} + \vec{V^2}(\vec{I}-2 \vec{S}^{-1}\vec{S_2})\right]^{-1}\left(\psi\beta^1 - \beta^2\right).
  \end{equation}
  If the random variables $\beta^1$ and $\beta^2$ are normally
  distributed, then the (appropriately scaled) estimates $V^1$ and $V^2$ will have a Wishart
  distribution, and the statistic $H$ will be distributed as
  Mahalinobis' $D^2$ statistic.  Scaling this statistic,
  $N_2\left(\frac{N-n-1}{(N-1)(n-1)}\right)D^2$ is distributed $F_{n-1,N-n-1}$;
  as $N\rightarrow\infty$ (holding $n$ fixed this converges to the
  $\chi^2_{n-1}$ distribution).

  For the case in which the vectors $\beta$ are obtained as Frisch
  elasticities in a CFE demand system, then $\vec{X}$ is a vector of
  normalized $\log\lambda$ statistics, and identification assumptions
  on $\beta$ include $\E X=0$ and $\E X^\T X=1$.  Then the weighting
  matrix takes a form which is considerably simpler, but where the
  parameter $\psi$ enters in a more complicated fashion, with
  weighting matrix 
  \[ 
     \vec{W}^{-1}(\psi) = \psi^2\vec{V^1} + \vec{V^2}(1 - 2\frac{N_2}{N}\psi^2).  
  \]

  The following code provides an implementation of this test of
  equality for the CFE case.  We define a function
  =elasticities_equal= which takes as arguments
  $(\beta^1,\beta^2,V^1,V^2,N,N_2)$, and returns the value of $\psi$
  which  minimizes the criterion; the minimized value of the
  criterion, scaled to have the specified $F$ distribution; and
  optionally the \(p\)-value associated with the test.

#+name: elasticities_equal
#+BEGIN_SRC python :exports code :tangle ../cfe/estimation.py
#name: elasticities_equal

import numpy as np
from scipy.optimize import minimize_scalar
from scipy.stats.distributions import f as F

def elasticities_equal(b1,b2,v1,v2,N,N2,pvalue=False,criterion=False):

    assert N2<N, "N2 should be size of sub-sample of pooled sample."
    b1 = b1.reshape((-1,1))
    b2 = b2.reshape((-1,1))

    n=len(b1)

    assert n==len(b2), "Length of vectors must be equal"

    def Fcriterion(psi):
        try:
            psi=psi[0,0]
        except (TypeError, IndexError):
            pass

        d = psi*b1 - b2
        if d.shape[0]<d.shape[1]: d = d.T

        W = np.linalg.inv((psi**2)*v1 + v2) # Independent case

        F = N2*(N-n-1)/((N-1)*(n-1)) * d.T@W@d

        if ~np.isscalar(F):
            F=F[0,0]

        return F

    #result = minimize_scalar(Fcriterion,method='bounded',bounds=[0,10])
    Fcriterion(1.)
    result = minimize_scalar(Fcriterion)
    psi=np.abs(result['x'])
    Fstat=result['fun']

    assert result['success'], "Minimization failed?"

    outputs = [psi,Fstat]

    if pvalue:
        p = 1 - F.cdf(Fstat,n-1,N-n-1)
        outputs.append(p)

    if criterion:
        outputs.append(Fcriterion)
    
    return tuple(outputs)
#+END_SRC

** Test

#+name: test_elasticities_equal
#+BEGIN_SRC python :noweb no-export :tangle ../cfe/stochastic_test/test_elasticities_equal.py
#name: test_elasticities_equal

<<elasticities_equal>>

N = 10000
N2 = 5000
b0=np.array([1,2,3])
v0=np.array([[1,0.5,0.25],[0.5,1,.5],[.25,.5,1]])
B=np.random.multivariate_normal(b0,v0,size=N)

b1=np.mean(B,axis=0)
v1=np.cov(B,rowvar=False)

b2=2*np.mean(B[:N2,:],axis=0) # So true value of psi=2
v2=4*np.cov(B[:N2,:],rowvar=False)

def covb1b2(psi=1.,tol=1e-2):
    last=1
    next=0
    b1bar=0
    b2bar=0
    i=0
    while np.linalg.norm(next-last)>tol:
        i+=1
        last=next
        B1=B[np.random.randint(N,size=N),:]
        newb1=psi*np.mean(B1,axis=0)
        newb2=2*np.mean(B1[np.random.randint(N,size=N2),:],axis=0)
        next = next*(1-1./i) + np.outer(newb1,newb2)/i
        b1bar = b1bar*(1-1./i) + newb1/i
        b2bar = b2bar*(1-1./i) + newb2/i
        if i>100: continue

    C = next - np.outer(b1bar,b2bar)
    return (C + C.T)/2.

def Vmom(psi=1.,tol=1e-2):
    last=1
    next=0
    dbar=0
    i=0
    while np.linalg.norm(next-last)>tol:
        i+=1
        last=next
        newb1=psi*np.mean(B[np.random.randint(N,size=N),:],axis=0)
        newb2=2*np.mean(B[np.random.randint(N,size=N2),:],axis=0)
        d = newb1 - newb2
        next = next*(1-1./i) + np.outer(d,d)/i
        dbar = dbar*(1-1./i) + d/i
        if i>100: continue

    return next - np.outer(dbar,dbar)

psi,F,p,crit = elasticities_equal(b1,b2,v1,v2,N,N2,pvalue=True,criterion=True)
#C=covb1b2()

assert np.abs(psi-2)<0.05, "Value of psi should be about 2"
assert p>0.01, "Should seldrom reject equality of elasticities."
#+END_SRC  

* Predicted expenditures
  To construct unbiased estimates of /levels/ of expenditures---rather
  than the logarithms that emerge naturally from our estimation---we
  need to take into account the distribution of error terms.  

  One simple approach is to assume that these error terms are
  normally distributed, with means and variances allowed to vary by market or
  period.  This seems to work well in practice, but examining the
  distribution of estimated residuals to check for gross violations of
  this distributional assumption is important.

#+name: predicted_expenditures
#+begin_src python :exports code :tangle ../cfe/estimation.py
#name: predicted_expenditures

import numpy as np

def predicted_expenditures(yhat,e):
    """
    Return levels of predicted expenditures.
   
    =yhat= is a dataframe or xarray of predicted log item expenditures, 
           with columns corresponding to different items.
       =e= is a dataframe or xarray of the residuals from the estimation which
           yielded =yhat=.
    """
    ebar = e.mean('j')
    evar = e.var('j')

    x = np.exp(yhat + ebar + evar/2)

    return x
#+end_src

* Price Indices
  Consider the expenditure function for a consumer expressed as a
  function of \lambda, characteristics $z$ and prices $p$; we write
  this as $x(\lambda,p,z)$, and interpret this as the expenditures
  required for a household or consumer with characteristics $z$ facing
  prices $p$ to achieve a marginal utility of expenditures of \lambda.
  A function which can be used to compute this is defined in
  =cfe.demands.expenditures=.

** Optimal price indices
  Now, suppose that prices aren't $p$, but are instead $p'$.  The
  level of expenditures is now required for the same household or
  consumer to maintain their same level of \lambda will be
  $x(\lambda,p',z)$, so the proportional /change/ in expenditures is
  given by the price index
  \[ 
     R(\lambda,z,p',p) = \frac{x(\lambda,p',z)}{x(\lambda,p,z)}.
  \]
  Since the expenditure functions correspond to the CFE utility
  functions, we say that the index $R$ is /optimal/ for CFE utility.
  Note that since CFE generalized Constant Elasticity of Substitution
  (CES) utility, $R$ also generalizes the index which is optimal for CES
  described by cite:feenstra94. 

  The following code defines a function =optimal_index= designed to take
  three inputs, each either supplied as a =pd.dataframe= or an
  =xr.DataArray=.  In the dataframe case with rows corresponding to
  goods and columns corresponding to $(t,m)$ pairs: First, estimated
  =a=, noting that these can be interpreted as log shadow prices.
  Second, predicted log item expenditures =yhat=; and third the
  residuals =e= associated with the prediction.

#+name: optimal_index
#+begin_src python :exports code :tangle ../cfe/estimation.py
#name: optimal_index

import warnings
with warnings.catch_warnings():  
    warnings.filterwarnings("ignore",category=UserWarning)
    import xarray as xr

import pandas as pd

def optimal_index(a,yhat,e):
    """Return individual optimal price indices for each household in all settings.

    Given log shadow prices =a=, predicted log expenditures =yhat=,
    and residuals from prediction =e= calculate optimal price indices
    for each household =j= in each setting.

    A "setting" is a pair (t,m).  To get the price index for a
    household j=0 observed at (t0,m0)=(1,2) for the counterfactual
    setting (t,m)=(1,0) one can use something like
    R.sel(j=0,t0=1,m0=2,t=1,m=0).

    Ethan Ligon                                                 July 2018
    """

    # Begin by obtaining predicted expenditure shares in null setting.
    # Subtract relevant actual prices for household;
    # yhat missing for all but actual setting, missings propagate.
    x0 = predicted_expenditures(yhat - a,e)

    # (t0,m0) is 'home' setting
    x0 = x0.rename({'t':'t0','m':'m0'}) 

    xsum = x0*np.exp(a)    # Predicted x_i in different settings (t,m)
                           # for households in every setting (t0,m0).

    pidx = xsum.sum('i',skipna=False)   # Total expenditures in different settings.

    R=pidx/pidx.sel(t0=pidx.coords['t'],t=pidx.coords['t'],m0=pidx.coords['m'],m=pidx.coords['m'])

    return R.transpose('j','t0','m0','t','m')
#+end_src


*** Test
 #+name: test_optimal_index
 #+begin_src python :results output :var T=2 :var N=5000 :var n=12 :var M=2 :tangle ../cfe/test/test_optimal_index.py
 #name: test_optimal_index

import cfe
import numpy as np

<<predicted_expenditures>>
<<optimal_index>>

# Tangling may not include :vars from header
try: 
    N
except NameError: # :var inputs not set?
    N=5000
    T=2
    n=12
    M=2

p = cfe.dgp.prices(T,M,n)
for s in range(1,T):  # Just scale prices over time by constant
    p.loc[dict(t=s)] = p.loc[dict(t=0)] #*s
 
x,parts = cfe.dgp.expenditures(N,T,M,n,2,np.linspace(.5,3,n),sigma_phi=0.0,sigma_eps=0.01,p=p)
x = x.where(x>0,np.nan)  # Zeros to missing

z = parts['characteristics']

R = cfe.Result(y=np.log(x),z=np.log(z),min_xproducts=30,verbose='True')

R.drop_useless_expenditures()

yhat = R.get_predicted_log_expenditures()

pidx = optimal_index(R.a,yhat.sel(j=range(3)),R.e.sel(j=range(3)))
 #+END_SRC

* Monte Carlo Data Generating Process
 Here we construct a simple data-generating process, and then use
 data from this to estimate neediness, checking that we can recover
 the parameters of the data-generating process.  The various routines
 for generating data are tangled to a module =cfe.dgp=.

 We randomly generate several different kinds of data: "neediness"
 \lambda_{it}; prices $p_t$; and from these expenditures $x_{it}$.  

** Data-generating process for $\{\lambda^j_{t}\}$
   First we define a function which can generate a panel dataset of
   \(\lambda\)s, featuring both aggregate shocks, idiosyncratic
   shocks, and cross-sectional variation.

   The "aggregate" $\lambda$ is denoted by $\bar\lambda$, and is
   constructed so as to be the geometric mean of individuals'
   \(\lambda\)s within a particular market in every period. By default
   these means are distributed log-normal.

   There are three different distributions we specify to generate a
   dataset of $\lambda_{itm}$, having dimension $(N,T,M)$, where $N$
   is the number of individuals observed in each of $T$ periods and
   each of $M$ markets.  First, the distribution $\bar F$ governs the
   innovations involved in the aggregate 'shocks' $\bar\lambda$.
   Second, a distribution $G_0$ governs the cross-sectional
   distribution of individual $\lambda$ in the initial period;
   finally, a distribution $F$ governs individual innovations
   /conditional/ on the aggregate shock.  The expected value of an
   geometric innovation is one, by construction, so both individual
   and aggregate \lambda processes are martingales.

#+name: lambdas_dgp
#+BEGIN_SRC python :results silent :exports code :tangle ../cfe/dgp.py
#name: lambdas_dgp

from scipy.stats.distributions import lognorm
import numpy as np

import warnings
with warnings.catch_warnings():  
    warnings.filterwarnings("ignore",category=UserWarning)
    import xarray as xr


def geometric_brownian(sigma=1.):
    return lognorm(s=sigma,scale=np.exp(-(sigma**2)/2))

def lambdabar(T,M,Fbar):
    return xr.DataArray(np.cumprod(Fbar.rvs(size=(T,M)),axis=0),
                        dims=('t','m'),
                        coords={'t':range(T),'m':range(M)})

def lambdas(N,T,M=1,G0=lognorm(.5),Fbar=geometric_brownian(.1),F=geometric_brownian(.2)):

    L0 = xr.DataArray(G0.rvs(size=(N,1,M)),dims=('j','t','m'),
                    coords={'j':range(N),'t':range(1),'m':range(M)})  # Initial lambdas
    innov = xr.DataArray(F.rvs(size=(N,T-1,M)),dims=('j','t','m'),
                             coords={'j':range(N),'t':range(1,T),'m':range(M)})

    L = xr.concat((L0,innov),dim='t').transpose('j','t','m')
  
    # Add aggregate shocks Lbar:
    return L*lambdabar(T,M,Fbar=Fbar)
#+END_SRC

  In addition, time-varying household characteristics can affect
  demands.
#+name: characteristics_dgp
#+BEGIN_SRC python :noweb no-export :results silent :exports code :tangle ../cfe/dgp.py
#name: characteristics_dgp

def characteristics(N,T,M=1): 
    z = lambdas(N,T,M,Fbar=geometric_brownian(.05),F=geometric_brownian(0.1))
    return z
#+END_SRC


** Data-generating process for $\{p_t\}$
    Next we construct an $n\times T$ matrix of prices for different
    consumption goods.  As with the process generating the
    $\lambda_{it}$, these are also assumed to satisfy a martingale
    process (so we can re-purpose code for generating \(\lambda\)s here):
#+name: prices_dgp
#+BEGIN_SRC python :noweb no-export :results silent :exports code :tangle ../cfe/dgp.py
#name: prices_dgp

def prices(T,M,n,G0=lognorm(.5),Fbar=geometric_brownian(.05),F=geometric_brownian(.2)):

    P0 = xr.DataArray(G0.rvs(size=(n,1,M)),dims=('i','t','m'),
                      coords={'i':range(n),'t':range(1),'m':range(M)})  # Initial lambdas
    innov = xr.DataArray(F.rvs(size=(n,T-1,M)),dims=('i','t','m'),
                               coords={'i':range(n),'t':range(1,T),'m':range(M)})

    P = xr.concat((P0,innov),dim='t').transpose('t','m','i')
    
    # Add aggregate shocks L0:
    return P*lambdabar(T,M,Fbar=Fbar)
#+END_SRC

** Data-generating process for measurement error
    As discussed above, there are three sources of measurement error
    in expenditures; an additive error; a multiplicative error, and
    truncation.

    The following routine returns a normally distributed additive
    error, and a log-normally distributed multiplicative error.
    Truncation can only be accomplished after the "true" expenditures
    are generated below.
#+name: measurement_error_dgp
#+BEGIN_SRC python :results value
#name: measurement_error_dgp

import pandas as pd
from scipy.stats import distributions
import numpy as np

def measurement_error(N,T,M,n,mu_phi=0.,sigma_phi=0.,mu_eps=0.,sigma_eps=1.):
    """Return samples from two measurement error processes; one additive, the other  multiplicative.
  
    - The additive error (phi) is a normal distribution with mean
      =mu_phi= and standard deviation =sigma_phi=.
    
    - The multiplicative error (eps) is a log-normal distribution with mean
      =mu_eps= and standard deviation =sigma_eps=.
    """

    def additive_error(N=N,T=T,M=M,n=n,sigma=sigma_phi):
        return xr.DataArray(distributions.norm.rvs(scale=sigma,size=(N,T,M,n)) + mu_phi,dims=('j','t','m','i'))

    def multiplicative_error(N=N,T=T,M=M,n=n,sigma=sigma_eps):
        return xr.DataArray(np.exp(distributions.norm.rvs(loc=-sigma/2.,scale=sigma,size=(N,T,M,n)) + mu_eps),dims=('j','t','m','i'))

    phi=additive_error(N,T,M,n,sigma=sigma_phi)
    eps=multiplicative_error(N,T,M,n,sigma=sigma_eps)

    return phi,eps
#+END_SRC

** Data-generating process for expenditures
    
    We assume an addilog preference structure, generalized to allow
    for specific-substitution effects (but note that such effects
    violate symmetry of the Slutsky substitution matrix, and so should
    be regarded as a form of  specification error).  These
    elasticities are taken to be common across households (i.e., the
    curvature parameters in the addilog utilities are assumed equal);
    however, multiplicative terms are allowed to vary across
    households and goods, so that the direct momentary utility
    function for household $j$ can be written
    #
    \[
       U^j(c) = \sum_{i=1}^n\alpha^j_i\prod_{k=1}^n\frac{(c^j_{kt})^{1-1/\theta_{ik}} - 1}{1-1/\theta_{ik}}.
    \]
    # 
    With this structure, log Frischian expenditures are
    #
    \[
       \log x^j_{it} = \log\alpha^j_i + \log p_{it} - \sum_{k=1}^n(\theta_{ik})\log p_{kt} - \beta_i\log\lambda^j_t,
    \]
    #
    where $\beta_i=\sum_{k=1}^n(\theta_{ik})$ is the \(i\)th row-sum
    of the matrix $\Theta$.  Instantiated in code:
#+name: expenditures_dgp
#+BEGIN_SRC python :noweb no-export :results silent :exports code :tangle ../cfe/dgp.py
#name: expenditures_dgp

<<measurement_error_dgp>>

def expenditures(N,T,M,n,k,beta,mu_phi=0,sigma_phi=0.,mu_eps=0,sigma_eps=0.,Fbar=geometric_brownian(.001),p=None,rho_lz=0):
    """Generate artificial expenditures for $N$ households in $M$ markets
    over $T$ periods on $n$ items.  Return dataframe of expenditures
    and a dictionary of "true" underlying variables, the latter as
    type =xarray.DataArray=.

    Households are distinguished by a $k$-vector of characteristics,
    but common Frisch elasticities expressed as an $n$-vector beta.

    If supplied, optional arguments
    (mu_phi,sigma_phi,mu_eps,sigma_eps) describe the parameters of two
    different measurement error processes.  The first is a normally
    distributed additive measurement error process, with mean =mu_phi=
    and standard deviation =sigma_phi=.  The second is a
    multiplicative log-normal error process, with (log) mean =mu_eps=
    and (log) standard deviation =sigma_eps=.

    An optional xarray of prices =p= can also be provided.

    A parameter rho_lz (with default of zero) can be specified to
    induce a correlation between characteristics z and log lambdas.

    Ethan Ligon                                                     January 2018
    """

    if len(beta.shape)<2:
        Theta=xr.DataArray(np.diag(beta),dims=('i','ip'))
    else:
        Theta=xr.DataArray(beta,dims=('i','ip'))

    beta=Theta.sum('ip') # Row sum of elasticity matrix

    l = lambdas(N,T,M,Fbar=Fbar)
    
    foo = xr.DataArray(data=[chr(i) for i in range(ord('a'),ord('a')+k)],name='k',dims='k')

    z = xr.concat([characteristics(N,T,M) for i in range(k)],dim=foo)

    l = l*(z**rho_lz).prod('k') 

    L = np.reshape(l,(N,T,M)) 
    
    if p is None:
        p = prices(T,M,n)

    # Build x in steps
    #x = np.kron(np.log(L),-beta)
    x = np.log(L)*(-beta)
    x = x + np.log(p) - (Theta*np.log(p)).sum('ip') 
    x = x + np.log(z).sum('k')

    x = np.exp(x)

    phi,e=measurement_error(N,T,M,n,mu_phi=mu_phi,sigma_phi=sigma_phi,mu_eps=mu_eps,sigma_eps=sigma_eps)

    truth = xr.Dataset({'beta':beta,'lambdas':l,'characteristics':z,'prices':p,'x0':x})

    x = (x + p*phi) # Additive error
    x = x*e # Multiplicative error

    x = x*(x>0) # Truncation

    return x,truth
#+END_SRC

* Utility Functions

#+name: df_utils
#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/df_utils.py
#name: df_utils

import numpy as np
from scipy import sparse
import pandas as pd
from warnings import warn

def df_norm(a,b=None,ignore_nan=True,ord=None):
    """
    Provides a norm for numeric pd.DataFrames, which may have missing data.

    If a single pd.DataFrame is provided, then any missing values are replaced with zeros, 
    the norm of the resulting matrix is returned.

    If an optional second dataframe is provided, then missing values are similarly replaced, 
    and the norm of the difference is replaced.

    Other optional arguments:

     - ignore_nan :: If False, missing values are *not* replaced.
     - ord :: Order of the matrix norm; see documentation for numpy.linalg.norm.  
              Default is the Froebenius norm.
    """
    a=a.copy()
    if not b is None:
      b=b.copy()
    else:
      b=pd.DataFrame(np.zeros(a.shape),columns=a.columns,index=a.index)

    if ignore_nan:
        missing=(a.isnull()+0.).replace([1],[np.NaN]) +  (b.isnull()+0.).replace([1],[np.NaN]) 
        a=a+missing
        b=b+missing
    return np.linalg.norm(a.fillna(0).values - b.fillna(0).values)

def df_to_orgtbl(df,tdf=None,sedf=None,conf_ints=None,float_fmt='\\(%5.3f\\)',bonus_stats=None):
    """
    Returns a pd.DataFrame in format which forms an org-table in an emacs buffer.
    Note that headers for code block should include ":results table raw".

    Optional inputs include conf_ints, a pair (lowerdf,upperdf).  If supplied, 
    confidence intervals will be printed in brackets below the point estimate.

    If conf_ints is /not/ supplied but sedf is, then standard errors will be 
    in parentheses below the point estimate.

    If tdf is False and sedf is supplied then stars will decorate significant point estimates.
    If tdf is a df of t-statistics stars will decorate significant point estimates.

    if sedf is supplied, this creates some space for =bonus_stats= to be reported on each row.

    BUGS: Dataframes that have multiindex columns can't be nicely represented as orgmode tables, 
    but we do our best.
    """
    if len(df.shape)==1: # We have a series?
        df = pd.DataFrame(df) 

    # Test for duplicates in index
    if df.index.duplicated().sum()>0:
        warn('Dataframe index contains duplicates.')

    # Test for duplicates in columns
    if df.columns.duplicated().sum()>0:
        warn('Dataframe columns contain duplicates.')

    try: # Look for a multiindex
        levels = len(df.index.levels)
        names = ['' if v is None else v for v in df.index.names]
    except AttributeError: # Single index
        levels = 1
        names = [df.index.name if (df.index.name is not None) else '']

    def column_heading(df):
        try: # Look for multiindex columns
            collevels = len(df.columns.levels)
            colnames = ['' if v is None else v for v in df.columns.names]
        except AttributeError: # Single index
            collevels = 1
            colnames = [df.columns.name if (df.columns.name is not None) else '']

        if collevels == 1:
            s = '| ' + ' | '.join(names) + ' | ' + '|   '.join([str(s) for s in df.columns])+'  |\n|-\n'
        else:
            colhead = np.array(df.columns.tolist()).T
            lastcol = ['']*collevels
            for l,j in enumerate(colhead.T.copy()):
                for k in range(collevels):
                    if lastcol[k] == j[k]: colhead[k,l] = ''
                lastcol = j

            colhead = colhead.tolist()
            s = ''
            for k in range(collevels):
                if k < collevels - 1:
                    s += '| '*levels + ' | '
                else:
                    s += '| ' + ' | '.join(names) + ' | '
                s += ' | '.join(colhead[k]) + '  |\n'
            s += '|-\n'

        return s

    def se_linestart(stats,i):
        if stats is None: 
            return '|'*levels
        else:
            stats = stats.loc[i]
            assert levels >= len(stats), "Too many columns of bonus stats"
            line = ['']*(levels-len(stats)+1)
            line += stats.tolist()
            return ' | '.join(line) 

    s = column_heading(df)

    if (tdf is None) and (sedf is None) and (conf_ints is None):
        lastidx = ['']*levels
        for i in df.index:
            if levels == 1: # Normal index
                s += '| %s  ' % i
            else:
                for k in range(levels):
                    if lastidx[k] != i[k]:
                        s += '| %s ' % i[k]
                    else:
                        s += '| '
            lastidx =i 
    
            for j in df.columns: # Point estimates
                try:
                    entry='| '+float_fmt+' '
                    if np.isnan(df[j][i]):
                        s+='| --- '
                    else:
                        s+=entry % df[j][i]
                except TypeError:
                    s += '| %s ' % str(df[j][i])
            s+='|\n'
        return s
    elif not (tdf is None) and (sedf is None) and (conf_ints is None):
        lastidx = ['']*levels
        for i in df.index:
            if levels == 1: # Normal index
                s += '| %s  ' % i
            else:
                for k in range(levels):
                    if lastidx[k] != i[k]:
                        s += '| %s ' % i[k]
                    else:
                        s += '| '
            lastidx = i 

            for j in df.columns:
                try:
                    stars=(np.abs(tdf[j][i])>1.65) + 0.
                    stars+=(np.abs(tdf[j][i])>1.96) + 0.
                    stars+=(np.abs(tdf[j][i])>2.577) + 0.
                    stars = int(stars)
                    if stars>0:
                        stars='^{'+'*'*stars + '}'
                    else: stars=''
                except KeyError: stars=''
                entry='| '+float_fmt+stars+' '
                if np.isnan(df[j][i]):
                    s+='| --- '
                else:
                    s+=entry % df[j][i]
            s+='|\n'

        return s
    elif not (sedf is None) and (conf_ints is None): # Print standard errors on alternate rows
        if tdf is not False:
            try: # Passed in dataframe?
                tdf.shape
            except AttributeError:  
                tdf=df[sedf.columns]/sedf

        lastidx = ['']*levels
        for i in df.index:
            if levels == 1: # Normal index
                s += '| %s  ' % i
            else:
                for k in range(levels):
                    if lastidx[k] != i[k]:
                        s += '| %s ' % i[k]
                    else:
                        s += '| '
            lastidx = i 

            for j in df.columns: # Point estimates
                if tdf is not False:
                    try:
                        stars=(np.abs(tdf[j][i])>1.65) + 0.
                        stars+=(np.abs(tdf[j][i])>1.96) + 0.
                        stars+=(np.abs(tdf[j][i])>2.577) + 0.
                        stars = int(stars)
                        if stars>0:
                            stars='^{'+'*'*stars + '}'
                        else: stars=''
                    except KeyError: stars=''
                else: stars=''
                entry='| '+float_fmt+stars+'  '
                if np.isnan(df[j][i]):
                    s+='| --- '
                else:
                    s+=entry % df[j][i]
            s+='|\n' + se_linestart(bonus_stats,i)
            for j in df.columns: # Now standard errors
                s+='  '
                try:
                    if np.isnan(df[j][i]): # Pt estimate miss
                        se=''
                    elif np.isnan(sedf[j][i]):
                        se='(---)'
                    else:
                        se='(' + float_fmt % sedf[j][i] + ')' 
                except KeyError: se=''
                entry='| '+se+'  '
                s+=entry 
            s+='|\n'
        return s
    elif not (conf_ints is None): # Print confidence intervals on alternate rows
        if tdf is not False and sedf is not None:
            try: # Passed in dataframe?
                tdf.shape
            except AttributeError:  
                tdf=df[sedf.columns]/sedf
        lastidx = ['']*levels
        for i in df.index:
            if levels == 1: # Normal index
                s += '| %s  ' % i
            else:
                for k in range(levels):
                    if lastidx[k] != i[k]:
                        s += '| %s ' % i[k]
                    else:
                        s += '| ' 
            lastidx = i 

            for j in df.columns: # Point estimates
                if tdf is not False and tdf is not None:
                    try:
                        stars=(np.abs(tdf[j][i])>1.65) + 0.
                        stars+=(np.abs(tdf[j][i])>1.96) + 0.
                        stars+=(np.abs(tdf[j][i])>2.577) + 0.
                        stars = int(stars)
                        if stars>0:
                            stars='^{'+'*'*stars + '}'
                        else: stars=''
                    except KeyError: stars=''
                else: stars=''
                entry='| '+float_fmt+stars+' '
                if type(df[j][i]) is not str and np.isnan(df[j][i]):
                    s+='| --- '
                else:
                    s+=entry % df[j][i]
            s+='|\n' + se_linestart(bonus_stats,i)

            for j in df.columns: # Now confidence intervals
                s+='  '
                try:
                    ci='[' + float_fmt +','+ float_fmt + ']'
                    ci= ci % (conf_ints[0][j][i],conf_ints[1][j][i])
                except KeyError: ci=''
                entry='| '+ci+'  '
                s+=entry 
            s+='|\n'
        return s

def orgtbl_to_df(table, col_name_size=1, format_string=None, index=None, dtype=None):
  """
  Returns a pandas dataframe.
  Requires the use of the header `:colnames no` for preservation of original column names.

  - `table` is an org table which is just a list of lists in python.
  - `col_name_size` is the number of rows that make up the column names.
  - `format_string` is a format string to make the desired column names.
  - `index` is a column label or a list of column labels to be set as the index of the dataframe.
  - `dtype` is type of data to return in DataFrame.  Only one type allowed.
  """
  import pandas as pd

  if col_name_size==0:
    return pd.DataFrame(table)

  colnames = table[:col_name_size]

  if col_name_size==1:
    if format_string:
      new_colnames = [format_string % x for x in colnames[0]]
    else:
      new_colnames = colnames[0]
  else:
    new_colnames = []
    for colnum in range(len(colnames[0])):
      curr_tuple = tuple([x[colnum] for x in colnames])
      if format_string:
        new_colnames.append(format_string % curr_tuple)
      else:
        new_colnames.append(str(curr_tuple))

  df = pd.DataFrame(table[col_name_size:], columns=new_colnames)

  if index:
    df.set_index(index, inplace=True)

  return df

def drop_missing(X,infinities=False):
    """
    Return tuple of pd.DataFrames in X with any 
    missing observations dropped.  Assumes common index.

    If infinities is false values of plus or minus infinity are 
    treated as missing values.
    """

    foo=pd.concat(X,axis=1)
    if not infinities:
        foo.replace(np.inf,np.nan)
        foo.replace(-np.inf,np.nan)

    foo = foo.dropna(how='any')

    assert len(set(foo.columns))==len(foo.columns) # Column names must be unique!

    Y=[]
    for x in X:
        Y.append(foo.loc[:,pd.DataFrame(x).columns]) 

    return tuple(Y)

def use_indices(df,idxnames):
    return df.reset_index()[idxnames].set_index(df.index)
#+END_SRC

#+name: test_df_norm
#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/stochastic_test/test_df_norm.py
#name: test_df_norm



import numpy as np
import pandas as pd
from cfe.df_utils import df_norm
from cfe.dgp import expenditures #, lambdas, characteristics, measurement_error  
from cfe.result import to_dataframe
import pytest

def artificial_data(T=2,N=120,M=1,k=2,n=4,sigma_e=0.001,sigma_phi=0.1):
    x,truth=expenditures(N,T,M,n,k,beta=np.linspace(1,3,n),sigma_phi=sigma_phi,sigma_eps=sigma_e)
    y=np.log(x+0.001)
    return y,truth

a,truth=artificial_data(T=2,N=5000,k=2,n=10,sigma_e=1e-10)
a = to_dataframe(a,['j','t','m']).T
# create another random matrix for subtracting 
b = pd.DataFrame(np.random.normal(size = a.shape), index = a.index, columns = a.columns)

# introduce NAs for a test case
a_missing = a.copy()
b_missing = b.copy()
for i in range(5000):
    row_a = np.random.randint(low=0, high = a.shape[0])
    col_a = np.random.randint(low=0, high = a.shape[1])
    a_missing.iloc[row_a, col_a] = np.NaN

    row_b = np.random.randint(low=0, high = b.shape[0])
    col_b = np.random.randint(low=0, high = b.shape[1])
    b_missing.iloc[row_b, col_b] = np.NaN

tol = 1e-4

@pytest.mark.parametrize("m", [
    a,
    a_missing,
])
def test_norm_onemat(m):
    mynorm = (m**2).sum().sum()**.5
    df_norm_norm = df_norm(m)
    assert np.abs(mynorm - df_norm_norm) < tol

@pytest.mark.parametrize("m, n", [
    (a, b),
    (a_missing, b_missing),
    (a, b_missing),
    (a_missing, b)
])
def test_norm_twomat(m, n):
    net = m - n
    mynorm = (net**2).sum().sum()**.5
    df_norm_norm = df_norm(m, n)
    assert np.abs(mynorm - df_norm_norm) < tol
#+END_SRC

#+name: test_df_to_orgtbl_tbl
#name: test_df_to_orgtbl_tbl

| A |  B | C |
|---+----+---|
| a |  1 | m |
| b | 2. | c |

#+name: test_df_to_orgtbl
#+BEGIN_SRC python :noweb no-export :results output raw table :var X=test_df_to_orgtbl_tbl :colnames no :tangle ../cfe/test/test_df_to_orgtbl.py
#name: test_df_to_orgtbl

<<df_utils>>

try:
    df = orgtbl_to_df(X).set_index('A')
except NameError: # Some tangling doesn't include var declarations?
    X=[["A", "B", "C"], ["a", 1, "m"], ["b", 2, "c"]]
    df = orgtbl_to_df(X).set_index('A')


s = df_to_orgtbl(df)
assert(s[-4]=='c')
print(s)
#+END_SRC

#+results: test_df_to_orgtbl
|   | B         | C |
|---+-----------+---|
| a | \(1.000\) | m |
| b | \(2.000\) | c |

** Some econometric routines

#+BEGIN_SRC python :noweb no-export :results output :tangle ../cfe/df_utils.py
import warnings
with warnings.catch_warnings():
    warnings.filterwarnings("ignore",category=UserWarning)
    import xarray as xr

from scipy.linalg import block_diag

def arellano_robust_cov(X,u,clusterby=['t','m'],tol=1e-12):
    """
    Compute clustered estimates of covariance matrix, per Arellano (1987).
    Estimates of variance of fixed effects use OLS estimator.
    """
    X,u = drop_missing([X,u])
    clusters = set(zip(*tuple(use_indices(u,clusterby)[i] for i in clusterby)))
    if  len(clusters)>1:
        # Take out time averages
        ubar = u.groupby(level=clusterby).transform(np.mean)
        Xbar = X.groupby(level=clusterby).transform(np.mean)
    else:
        ubar = u.mean()
        Xbar = X.mean()

    ut = (u - ubar).squeeze()
    assert len(ut.shape)==1, "Errors should be a vector or series"
    Xt = X - Xbar

    # Pull out columns spanned by cluster vars to get var of FEs
    Cvars = Xt.columns[Xt.std()<tol]
    Xvars = Xt.columns[Xt.std()>=tol]
    if len(Cvars):
        _,v = ols(X.loc[:,Cvars],u,return_se=False,return_v=True)

    Xt = Xt.drop(columns=Cvars)

    Xu=Xt.mul(ut,axis=0)

    if len(Xt.shape)==1:
        XXinv=np.array([1./(Xt.T.dot(Xt))])
    else:
        XXinv=np.linalg.inv(Xt.T.dot(Xt))
    Vhat = XXinv.dot(Xu.T.dot(Xu)).dot(XXinv)

    try:
        Allvars = Cvars.values.tolist() + Xvars.values.tolist()
        if len(Cvars):
            V = xr.DataArray(block_diag(v.squeeze('variable').values,Vhat),dims=['k','kp'],coords={'k':Allvars,'kp':Allvars})
        else:
            V = xr.DataArray(Vhat,dims=['k','kp'],coords={'k':Allvars,'kp':Allvars})
        return V
    except AttributeError:
        if len(Cvars):
            return v,Vhat
        else:
            return Vhat


def ols(x,y,return_se=True,return_v=False,return_e=False):
    """Produce OLS estimates of b in $y = xb + u$.

    If standard errors (return_se=True) or covariance matrices
    (return_v=True) are returned, these are Seemingly Unrelated
    Regression (SUR) estimates if y has multiple columns, or the
    simple OLS estimator var(u)(X'X)^{-1} otherwise.
    """

    x=pd.DataFrame(x) # Deal with possibility that x & y are series.
    y=pd.DataFrame(y)
    # Drop any observations that have missing data in *either* x or y.
    x,y = drop_missing([x,y])

    N,n=y.shape
    k=x.shape[1]

    b=np.linalg.lstsq(x,y,rcond=0)[0]

    b=pd.DataFrame(b,index=x.columns,columns=y.columns)

    out=[b]
    if return_se or return_v or return_e:

        u=y-x.dot(b)
        assert u.shape == (N,n), "Dimensions of disturbance not as expected"

        if return_se or return_v:
            Sigma = u.T@u/N
            XXinv = np.linalg.inv(x.T@x)
            V = np.kron(Sigma,XXinv)

        if return_se:
            se=np.sqrt(V.diagonal()).reshape((x.shape[1],y.shape[1]))
            se=pd.DataFrame(se,index=x.columns,columns=y.columns)

            out.append(se)

        if return_v:
            # Extract blocks along diagonal; return a k x kp x n array
            col0 = x.columns
            col1 = col0.rename(name='kp')
            v = {y.columns[i]:pd.DataFrame(V[i*k:(i+1)*k,i*k:(i+1)*k],index=col0,columns=col1) for i in range(n)}
            V = xr.Dataset(v).to_array()
            out.append(V)

        if return_e:
            out.append(u)

    return tuple(out)
#+END_SRC

*** Tests
Start with super basic test of OLS standard errors:
#+begin_src python :noweb no-export :results output :tangle ../cfe/stochastic_test/test_ols_cov.py
import cfe
import numpy as np
import pandas as pd

N = 10000

u = pd.DataFrame({'Constant':1,'u':np.random.normal(size=(N,))})

# Just regress u on constant
b,se,e = cfe.df_utils.ols(u[['Constant']],u[['u']],return_se=True,return_e=True)

np.testing.assert_allclose(0,b,atol=2/np.sqrt(N),err_msg='Point estimate wrong') # Outside two se
np.testing.assert_allclose(1/np.sqrt(N),se,atol=2*np.sqrt(3)/np.sqrt(N),err_msg='SE estimate wrong')
np.testing.assert_allclose(u[['u']] - u['u'].mean(),e,atol=1/np.sqrt(N),err_msg='Residuals wrong')
#+end_src

#+results:

Next try just constructing multiple equation covariance matrix
#+begin_src python :noweb no-export :results output :tangle ../cfe/stochastic_test/test_ols_cov.py
import cfe
import pylab as pl
import numpy as np
import pandas as pd

from cfe.dgp import geometric_brownian
from cfe.result import to_dataframe

J=100
n=12
T=4
x,parts = cfe.dgp.expenditures(J,T,1,n,2,np.linspace(0.25,3,n),sigma_phi=0.,sigma_eps=0.01,Fbar=geometric_brownian(1.))
y = to_dataframe(np.log(x.where(x>0,np.nan)),'i')
z = to_dataframe(np.log(parts['characteristics']),'k')

X = z.copy()
X['Constant'] = 1

b,v = cfe.df_utils.ols(X,y,return_v=True,return_se=False)
#+end_src

Now I test the Arellano robust standard errors using canned R packages.
#+begin_src python :noweb no-export :results output :tangle ../cfe/test/test_arellano_robust.py
import cfe
import pandas as pd
import statsmodels.formula.api as sm
import numpy as np

from cfe.df_utils import arellano_robust_cov, ols, use_indices, drop_missing
from cfe.dgp import expenditures
from cfe.result import to_dataframe

# r packages
try:
    from rpy2.robjects import r, pandas2ri
    from rpy2.robjects.packages import importr
    from rpy2.robjects.conversion import localconverter
    import rpy2.robjects as ro

    def artificial_data(T=2,N=120,M=1,k=2,n=4,sigma_e=0.001,sigma_phi=0.1):
        x,truth=expenditures(N,T,M,n,k,beta=np.linspace(1,3,n),sigma_phi=sigma_phi,sigma_eps=sigma_e)
        y=np.log(x+0.001)
        return y,truth

    def test_arellano():
        # make data that is a panel type (i, m, t) so that the clusters/FE can be over (m, t)
        y,truth=artificial_data(T=2,N=5000,k=2,n=10,sigma_e=1e-10)

        y = to_dataframe(y,['j','t','m']).T
        dz = to_dataframe(truth['characteristics'],['j','t','m']).T
        dz=np.log(dz+0.01)

        DateLocD = use_indices(y,['t','m'])
        DateLocD = pd.get_dummies(list(zip(DateLocD['t'],DateLocD['m'])))
        DateLocD.index = y.index

        for i,Item in enumerate(y.columns):
            lhs,rhs=drop_missing([y.iloc[:,[i]],pd.concat([dz,DateLocD],axis=1)])
            useDateLocs=list(set(DateLocD.columns.tolist()).intersection(rhs.columns.tolist()))

            # demean
            lhsbar=lhs.mean(axis=0)
            lhs=lhs-lhsbar
            lhs=lhs-lhs.mean(axis=0)

            rhsbar=rhs.mean(axis=0)
            rhs=rhs-rhsbar
            rhs=rhs-rhs.mean(axis=0)

            ynil=pd.DataFrame([0],index=[(-1,0,0)],columns=lhs.columns)
            znil=pd.DataFrame([[0]*dz.shape[1]],index=[(-1,0,0)],columns=dz.columns)
            timednil=pd.DataFrame([[1]*DateLocD.shape[1]],index=[(-1,0,0)],columns=DateLocD.columns)

            # change append to concat:  X=rhs.append(znil.join(timednil))
            X=pd.concat([rhs,znil.join(timednil)])
            X=X.loc[:,X.std()>0] # Drop  any X cols with no variation
            # change append to concat:  lhs.append(ynil)
            lhs = pd.concat([lhs,ynil])

            df = lhs.join(X)
            df.columns = ['y', 'a', 'b', 'fe1', 'fe2']

            # # # # #
            # my ols
            cannedols = sm.ols(formula = "y ~ a + b + fe1 + fe2 -1", data = df).fit(cov_type='cluster',
                cov_kwds={'groups': np.array(df[['fe1', 'fe2']])},
                use_t=True)

            canned_b = cannedols.params
            # # # # #
            # try variances in R
            base = importr('base')
            utils = importr('utils')
            sandwich = importr('sandwich')

            with localconverter(ro.default_converter + pandas2ri.converter):
              r_from_pd_df = ro.conversion.py2rpy(df.reset_index())

            # define a fn in R that estimates the model and calculate  2-way arellano SEs
            # this is taken from https://www.jstatsoft.org/article/view/v082i03
            ro.r('''
            # create a function for estimating the model and arellano SE
            estimateArellano <- function(df){
                    mod <- lm(y ~ a + b + m + t, data = df)
                    Vcx <- vcovHC(mod, cluster = "group", method = "arellano")
                    Vct <- vcovHC(mod, cluster = "time", method = "arellano")
                    Vw <- vcovHC(mod, method = "white1")

                    covmat <- Vcx + Vct - Vw
                    covmat <- covmat[2:3, 2:3]
                    return(covmat)
            }
            ''')
            # call the function
            myV = ro.r['estimateArellano'](r_from_pd_df)

            # CFE ols
            myb,mye=ols(X,lhs,return_se=False,return_v=False,return_e=True)
            V = arellano_robust_cov(dz,mye.iloc[:-1,:])

            myb.index = canned_b.index
            # set tolerance
            tol = 0.01
            assert np.sum(np.abs(myb.sub(canned_b, axis= 0).div(myb, axis = 1))).squeeze() < tol, "Betas differ substantially!"
            assert np.all(np.sum(np.abs((V - myV)/V) < tol)), "Variances differ substantially"
except ModuleNotFoundError:
    pass # Can't run test, as R stuff not available.
#+end_src

** Implementation of iterative Frisch-Waugh-Lovell least squares estimator
#+begin_src python :tangle /tmp/fwl_test.py
import numpy as np
import pandas as pd
from cfe.df_utils import drop_missing
import cfe

precision='float64'
def fullrank(X,VERBOSE=False):
    """
    Test if 2d array X has full column rank.

    Examples:
    >>> X=np.array([[0,0],[0,1]])
    >>> fullrank(X)
    False

    >>> X=np.array([[1,0],[0,1]])
    >>> fullrank(X)
    True
    """
    assert len(X.shape)==2
    assert X.shape[1]<=X.shape[0]
    try:
        cX=X.T@X
        cnd=np.linalg.cond(cX) - 1
        if cnd>1e16:
            if VERBOSE:
                print("Condition - 1 = %f" % cnd)
            raise np.linalg.LinAlgError
        elif cnd>1e10:
            print("Warning: Condition %g, columns %d" % (cnd,X.shape[1]))
        return True
    except np.linalg.LinAlgError:
        return False

def reduce_rank_deficiency(X,tol=1e-10,VERBOSE=False):
    """Reduce the rank deficiency of a pd.DataFrame X.

    If X is empty or has only columns of zeros, return [] and labels
    of dropped columns.  If X has some zero columns, eliminate these
    and return [X], along with labels of dropped columns.  Otherwise,
    split X into two parts and return [X1,X2].

    Ethan Ligon                                        July 2019
    """
    X = X.copy()

    k = X.shape[1]
    zerocols = np.abs(X).mean(axis=0) < tol
    zeros = X.loc[:,zerocols]
    X = X.loc[:,~zerocols]
    assert (k-zeros.shape[1])==X.shape[1]


    if X.shape[1]<k: # If X is empty or all zeros...
        if VERBOSE:
            print("Dropping zeros")
        return [X], [zeros.sum()*0]
    elif not fullrank(X,VERBOSE=VERBOSE):
        if VERBOSE:
            print("Partioning X into two.")
        m = int(np.floor(k/2)) # Nb: integer division -> integer

        return [X.iloc[:,0:m],X.iloc[:,m:]],[]
    else: return [X],[]

def fwl_pop(X,VERBOSE=False):
    """b1,(r1,r2,...,rn)=fwl_pop([X0,X1,...,Xn])

    Linear projection of X1,...,Xn-1 on X0.  Returns the coefficients
    of regressing Xn on X0 in b1,...,bn, and the residuals in
    r1,r2,..,rn. Note that the Xi may be Series or DataFrames.  These
    are assumed to have no missing values.

    """

    if len(X)==1:  # Deal with single dataframe
        return pd.Series([]),X

    x = X.pop(0)
    B=[]
    R=[]
    for y in X:
        b,e,rnk,s = np.linalg.lstsq(-x,y,rcond=None)
        if VERBOSE: print("Columns,rank=(%d,%d)" % (x.shape[1],rnk))
        b = pd.DataFrame(b,index=x.columns,columns=y.columns)
        r = y + x@b
        r,zeros = reduce_rank_deficiency(r,VERBOSE=VERBOSE)
        B = B + [pd.concat([b]+zeros,axis=0)]
        R += r

    return B,R

def fwl_regress(y,X,VERBOSE=False):
    """
    Estimate linear regression in the spirit of Frisch-Waugh-Lovell.

    Iteratively project (y,X) on the first dataframe in list X,
    collecting point estimates & residuals.
    """
    Xy = list(drop_missing(X+[y]))

    B=[]
    A=[]
    while len(Xy)>1:
        b,Xy = fwl_pop(Xy,VERBOSE=VERBOSE)
        yb = b.pop()
        # change append to concat: A.append(pd.concat([pd.DataFrame(-np.eye(yb.shape[0]), columns=yb.index,index=yb.index)]+b,axis=1))
        pd.concat([A,pd.concat([pd.DataFrame(-np.eye(yb.shape[0]),
                                    columns=yb.index,index=yb.index)]+b,axis=1)])
        # change append to concat: B.append(yb)
        pd.concat([B,yb])

    A = pd.concat(A,axis=0,sort=False).fillna(0)
    B = pd.concat(B,axis=0,sort=False)

    columns=[]
    for x in X:
        columns = columns + x.columns.tolist()

    x = pd.DataFrame(np.linalg.solve(A,B),index=columns,columns=y.columns)

    return x,Xy[0]
#+end_src

#+begin_src python :tangle /tmp/fwl_test.py
def test_fwl_regress_simple(n=100):
    
    X = pd.DataFrame({'Constant':np.ones(n),
                      'X1':np.random.rand(n)})
    X = [X,pd.DataFrame({'X2':np.random.rand(n)})]
    B = [pd.Series([1,2],index=X[0].columns),
         pd.Series([3],index=X[1].columns)]
    y=0
    for i,b in enumerate(B):
        y = y + X[i]@b

    y = pd.DataFrame({'y':y + pd.Series(np.random.normal(0,.000001,n)),})

    myB,E  = fwl_regress(y,X,VERBOSE=True)

    oneX = pd.concat(X,axis=1)
    b = np.linalg.lstsq(oneX,y,rcond=None)[0]
    b = pd.DataFrame(b,index=oneX.columns,columns=y.columns)

    e = y - oneX@b

    assert np.allclose(b,myB)
    print("Success")
    return b

def test_fwl_regress_realdata():
    r = cfe.from_dataset('~/Research/NotPPP/var/r.ds')
    y = r.y.to_dataframe().dropna()
    z = r.z.to_dataframe('').squeeze().unstack('k').dropna()

    foo = y.reset_index()

    Z = y.join(z,how='outer',on=['j','t','m'])[z.columns]
    y,Z = drop_missing([y,Z])

    # Stacked regression, ordered by (i,t,m)
    X = [Z,      
         pd.get_dummies(pd.Series(list(zip(foo['t'],foo['m'])),index=y.index)),
         pd.get_dummies(pd.Series(foo['i'].values,index=y.index))]

    B,e = fwl_regress(y,X,VERBOSE=True)

    assert np.allclose(y.var(), (pd.concat(X,axis=1)@B).var() + e.var())

    return B,e

if __name__=='__main__':
    print(test_fwl_regress_simple())
    B,e = test_fwl_regress_realdata()

#+end_src

** Utility functions for dealing with some awkward multiindex issues

#+name: broadcast_binary_op 
#+BEGIN_SRC python :noweb no-export :tangle  ../cfe/df_utils.py
#name: broadcast_binary_op

def merge_multi(df1, df2, on):
    """Merge on subset of multiindex.
   
    Idea due to http://stackoverflow.com/questions/23937433/efficiently-joining-two-dataframes-based-on-multiple-levels-of-a-multiindex
    """
    return df1.reset_index().join(df2,on=on).set_index(df1.index.names)

def broadcast_binary_op(x, op, y):
    """Perform x op y, allowing for broadcasting over a multiindex.

    Example usage: broadcast_binary_op(x,lambda x,y: x*y ,y)
    """
    x = pd.DataFrame(x.copy())
    y = pd.DataFrame(y.copy())
    xix= x.index.copy()

    if y.shape[1]==1: # If y a series, expand to match x.
        y=pd.DataFrame([y.iloc[:,0]]*x.shape[1],index=x.columns).T

    cols = list(x.columns)
    xindex = list(x.index.names)
    yindex = list(y.index.names)

    dif = list(set(xindex)-set(yindex))

    z = pd.DataFrame(index=xix)
    z = merge_multi(z,y,on=yindex)

    newdf = op(x[cols],z[cols])

    return newdf
#+END_SRC

Test the merge_multi() function, comparing it to pandas join().

#+name: test_merge_multi
#+BEGIN_SRC python :noweb no-export :tangle ../cfe/stochastic_test/test_merge_multi.py
#name: test_merge_multi


import numpy as np
import pandas as pd
from cfe.df_utils import merge_multi
from cfe.dgp import expenditures #, lambdas, characteristics, measurement_error  
from cfe.result import to_dataframe
import pytest

def artificial_data(T=2,N=120,M=1,k=2,n=4,sigma_e=0.001,sigma_phi=0.1):
    x,truth=expenditures(N,T,M,n,k,beta=np.linspace(1,3,n),sigma_phi=sigma_phi,sigma_eps=sigma_e)
    y=np.log(x+0.001)
    return y,truth

N = 5000
tol = 1e-4

a,truth=artificial_data(T=2,N=N,k=2,n=10,sigma_e=1e-10)
a = to_dataframe(a,['j','t','m']).T

# make a dataframe with a subset of the index of the previous one 
b = pd.DataFrame(np.random.normal(size=(N,1)), columns = ['b'])
b.index.names = ['j']

# make one that has some missing info
b_subset = b.iloc[1:500, :].copy()


@pytest.mark.parametrize("b", [
    b,
    b_subset,
])
def test_merge_multi(b):
    mymerge = a.join(b)
    merge_multi_test = merge_multi(a, b, on='j')

    assert np.sum(np.abs(mymerge - merge_multi_test)).sum() <  tol


#+END_SRC

#+name: test_broadcast_binary_op
#+BEGIN_SRC python :noweb no-export :tangle ../cfe/test/broadcast_binary_op.py
#name: test_broadcast_binary_op

import pandas as pd
from numpy import array, nan

<<broadcast_binary_op>>

foo=array([[        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan, -0.73396916],
           [-0.14518201,         nan,  0.88238915],
           [        nan,         nan, -0.49532144],
           [        nan,         nan,  0.55594608],
           [ 0.30538166,         nan,         nan],
           [        nan,         nan,  0.22884155],
           [        nan,         nan,  0.10763069],
           [ 0.36909748,         nan, -0.57661338],
           [ 0.        ,         nan, -0.38566246],
           [ 0.31845372,         nan,  0.92198873],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [        nan,         nan,         nan],
           [ 1.2039728 ,         nan,         nan],
           [-0.62860866, -0.69314718,  6.27914662],
           [-0.28768207,  0.        ,  1.60943791]])

idx=[ (u'101005', u'2009Q1', u'BGD'),  (u'101005', u'2011Q1', u'BGD'),
      (u'101007', u'2009Q1', u'BGD'),  (u'101007', u'2011Q1', u'BGD'),
      (u'00L0101', u'2013Q3', u'GHA'), (u'00L0101', u'2014Q3', u'GHA'),
      (u'00L0202', u'2013Q3', u'GHA'), (u'00L0202', u'2014Q3', u'GHA'),
      (u'02L0101', u'2012Q1', u'GHA'), (u'02L0101', u'2012Q3', u'GHA'),
      (u'02L0101', u'2013Q1', u'GHA'), (u'02L0202', u'2012Q1', u'GHA'),
      (u'02L0202', u'2013Q1', u'GHA'), (u'02L0238', u'2012Q3', u'GHA'),
      (1002, u'2014Q3', u'SSD'),       (1006, u'2014Q3', u'SSD'),
      (1002, u'2015Q2', u'SSD'),       (1006, u'2015Q2', u'SSD'),
      (u'3015', u'2010Q4', u'UGA'),    (u'3016', u'2010Q4', u'UGA')]

foo=pd.DataFrame(foo,index=pd.MultiIndex.from_tuples(idx,names=['j','t','m']))
foo=foo.sort_index()

bar=array([[ 0.24921932,         nan,         nan],
       [ 0.21093294,         nan,         nan],
       [ 0.33506356,  0.33506356,  0.33506356],
       [ 0.28271356,  0.26914329,  0.26914329],
       [ 0.23344945,  0.10095022,  0.0796501 ],
       [ 0.02299849, -0.25550914, -0.10560527],
       [ 0.0486334 ,  0.34874107,  0.17723716],
       [ 0.28000935,  0.05786949,  0.28000935],
       [ 0.01387462,         nan,         nan],
       [ 0.30318531,         nan,  0.30318531]])

idx=[(u'2009Q1', u'BGD'), (u'2011Q1', u'BGD'), (u'2011Q3', u'UGA'),
       (u'2012Q1', u'GHA'), (u'2012Q3', u'GHA'), (u'2013Q1', u'GHA'),
       (u'2013Q3', u'GHA'), (u'2014Q3', u'GHA'), (u'2014Q3', u'SSD'),
       (u'2015Q2', u'SSD')]

bar = pd.DataFrame(bar,index=pd.MultiIndex.from_tuples(idx,names=['t','m']))

baz=broadcast_binary_op(foo, lambda x,y: x+y, bar)

assert baz.shape == foo.shape
print(baz)

#+END_SRC

#+results: test_broadcast_binary_op
#+begin_example
                           0   1         2
j       t      m                        
1002    2014Q3 SSD       NaN NaN       NaN
        2015Q2 SSD       NaN NaN       NaN
1006    2014Q3 SSD       NaN NaN       NaN
        2015Q2 SSD  1.507158 NaN       NaN
00L0101 2013Q3 GHA       NaN NaN -0.556732
        2014Q3 GHA  0.134827 NaN  1.162398
00L0202 2013Q3 GHA       NaN NaN -0.318084
        2014Q3 GHA       NaN NaN  0.835955
02L0101 2012Q1 GHA  0.588095 NaN       NaN
        2012Q3 GHA       NaN NaN  0.308492
        2013Q1 GHA       NaN NaN  0.002025
02L0202 2012Q1 GHA  0.651811 NaN -0.307470
        2013Q1 GHA  0.022998 NaN -0.491268
02L0238 2012Q3 GHA  0.551903 NaN  1.001639
101005  2009Q1 BGD       NaN NaN       NaN
        2011Q1 BGD       NaN NaN       NaN
101007  2009Q1 BGD       NaN NaN       NaN
        2011Q1 BGD       NaN NaN       NaN
3015    2010Q4 UGA       NaN NaN       NaN
3016    2010Q4 UGA       NaN NaN       NaN
#+end_example

#+name: test_broadcast_binary_op2
#+BEGIN_SRC python :noweb no-export :tangle ../cfe/test/broadcast_binary_op2.py
#name: test_broadcast_binary_op2

import pandas as pd
from numpy import array, nan

<<broadcast_binary_op>>

idx=pd.MultiIndex.from_tuples([(0,0,0),(0,0,1),(0,1,0),(0,1,1),
                               (1,0,0),(1,0,1),(1,1,0),(1,1,1)],names=('a','b','c'))

foo = pd.DataFrame({'x':range(8)},index=idx)

idx=pd.MultiIndex.from_tuples([(0,0),(0,1),(1,0),(1,1)],names=('b','c'))

bar = pd.DataFrame({'y':range(4)},index=idx)

baz = broadcast_binary_op(foo,lambda x,y:x+y,bar)

assert baz.iloc[3,0]==6
#+END_SRC

#+results: test_broadcast_binary_op2



** Utility functions related to transformations between xarray and pandas objects
#+name: broadcast_binary_op 

#+BEGIN_SRC python :noweb no-export :tangle  ../cfe/df_utils.py
#name: broadcast_binary_op

import warnings
with warnings.catch_warnings():
    warnings.filterwarnings("ignore",category=UserWarning)
    import xarray as xr


def is_none(x):
    """
    Tests for None in an array x.
    """
    try:
        if np.any(np.equal(x,None)):
            return True
    except TypeError:
        return is_none(x.data)
    else:
        try:
            if len(x.shape)==0:
                return True
        except AttributeError:
            if isinstance(x,str):
                if len(x)==0: return True
                else: return False
            elif np.isscalar(x): return x is None
            elif isinstance(x,list): return None in x
            else:
                raise(TypeError,"Problematic type.")

def to_dataframe(arr,column_index=None,name=None,dropna_all=True):
    """Convert =xarray.DataArray= into a =pd.DataFrame= with indices etc. usable by =cfe=.
    """

    if name is None:
        dims = arr.dims
        df = arr.to_dataset(name='').to_dataframe(dims).squeeze()
        df.name = None
        df.index.names = dims # Deal with xarray bug in to_dataframe that drops index names?
    else:
        df = arr.to_dataframe(name)

    if column_index is not None:
        df = df.dropna(how='all').unstack(column_index)

    if dropna_all:
        df.dropna(how='all',inplace=True)

    return df

def from_dataframe(df,index_name=None):
    """Convert from dataframe used in cfe.estimation to xarray.DataArray.
    """
    if index_name is not None:
        df.index = df.index.set_names(index_name)

    df = pd.DataFrame(df) # Series to dataframe
    if not is_none(df.columns.names):
        df = df.stack(df.columns.names)

    arr = df.squeeze().to_xarray()

    return arr
#+END_SRC
