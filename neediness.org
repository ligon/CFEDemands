:SETUP:
#+TITLE: Code and Methods for Estimating Neediness
#+AUTHOR: Ethan Ligon
#+PROPERTY: header-args:python :results output :noweb no-export :exports none :comments link :prologue (format "# Tangled on %s" (current-time-string))
:END:

* Utility Functions
#+name: df_utils
#+BEGIN_SRC python :noweb no-export :results output :tangle df_utils.py
    import numpy as np
    from scipy import sparse

    def df_norm(a,b=None,ignore_nan=True):
        a=a.copy()
        if not b is None:
          b=b.copy()
        else:
          b=pd.DataFrame(np.zeros(a.shape),columns=a.columns,index=a.index)

        if ignore_nan:
            missing=(a.isnull()+0.).replace([1],[np.NaN]) +  (b.isnull()+0.).replace([1],[np.NaN]) 
            a=a+missing
            b=b+missing
        return np.linalg.norm(a.fillna(0).as_matrix() - b.fillna(0).as_matrix())

    def df_to_orgtbl(df,tdf=None,sedf=None,conf_ints=None,float_fmt='%5.3f'):
        """
        Print pd.DataFrame in format which forms an org-table.
        Note that headers for code block should include ":results table raw".

        Optional inputs include conf_ints, a pair (lowerdf,upperdf).  If supplied, 
        confidence intervals will be printed in brackets below the point estimate.

        If conf_ints is /not/ supplied but sedf is, then standard errors will be 
        in parentheses below the point estimate.

        If tdf is False and sedf is supplied then stars will decorate significant point estimates.
        If tdf is a df of t-statistics stars will decorate significant point estimates.
        """
        if len(df.shape)==1: # We have a series?
           df=pd.DataFrame(df)

        if (tdf is None) and (sedf is None) and (conf_ints is None):
            return '|'+df.to_csv(sep='|',float_format=float_fmt,line_terminator='|\n|')
        elif not (tdf is None) and (sedf is None) and (conf_ints is None):
            s = '|  |'+'|   '.join([str(s) for s in df.columns])+'\t|\n|-\n'
            for i in df.index:
                s+='| %s  ' % i
                for j in df.columns:
                    try:
                        stars=(np.abs(tdf.loc[i,j])>1.65) + 0.
                        stars+=(np.abs(tdf.loc[i,j])>1.96) + 0.
                        stars+=(np.abs(tdf.loc[i,j])>2.577) + 0.
                        if stars>0:
                            stars='^{'+'*'*stars + '}'
                        else: stars=''
                    except KeyError: stars=''
                    entry='| $'+float_fmt+stars+'$  '
                    s+=entry % df.loc[i,j]
                s+='|\n'

            return s
        elif not (sedf is None) and (conf_ints is None): # Print standard errors on alternate rows
            if tdf is not False:
                try: # Passed in dataframe?
                    tdf.shape
                except AttributeError:  
                    tdf=df[sedf.columns]/sedf
            s = '|  |'+'|   '.join([str(s) for s in df.columns])+'  |\n|-\n'
            for i in df.index:
                s+='| %s  ' % i
                for j in df.columns: # Point estimates
                    if tdf is not False:
                        try:
                            stars=(np.abs(tdf.loc[i,j])>1.65) + 0.
                            stars+=(np.abs(tdf.loc[i,j])>1.96) + 0.
                            stars+=(np.abs(tdf.loc[i,j])>2.577) + 0.
                            if stars>0:
                                stars='^{'+'*'*stars + '}'
                            else: stars=''
                        except KeyError: stars=''
                    else: stars=''
                    entry='| $'+float_fmt+stars+'$  '
                    s+=entry % df.loc[i,j]
                s+='|\n|'
                for j in df.columns: # Now standard errors
                    s+='  '
                    try:
                        se='$(' + float_fmt % sedf.loc[i,j] + ')$' 
                    except KeyError: se=''
                    entry='| '+se+'  '
                    s+=entry 
                s+='|\n'
            return s
        elif not (conf_ints is None): # Print confidence intervals on alternate rows
            if tdf is not False and sedf is not None:
                try: # Passed in dataframe?
                    tdf.shape
                except AttributeError:  
                    tdf=df[sedf.columns]/sedf
            s = '|  |'+'|   '.join([str(s) for s in df.columns])+'  |\n|-\n'
            for i in df.index:
                s+='| %s  ' % i
                for j in df.columns: # Point estimates
                    if tdf is not False and tdf is not None:
                        try:
                            stars=(np.abs(tdf.loc[i,j])>1.65) + 0.
                            stars+=(np.abs(tdf.loc[i,j])>1.96) + 0.
                            stars+=(np.abs(tdf.loc[i,j])>2.577) + 0.
                            if stars>0:
                                stars='^{'+'*'*stars + '}'
                            else: stars=''
                        except KeyError: stars=''
                    else: stars=''
                    entry='| $'+float_fmt+stars+'$  '
                    s+=entry % df.loc[i,j]
                s+='|\n|'
                for j in df.columns: # Now confidence intervals
                    s+='  '
                    try:
                        ci='$[' + float_fmt +','+ float_fmt + ']$'
                        ci= ci % (conf_ints[0].loc[i,j],conf_ints[1].loc[i,j])
                    except KeyError: ci=''
                    entry='| '+ci+'  '
                    s+=entry 
                s+='|\n'
            return s

    def orgtbl_to_df(table, col_name_size=1, format_string=None, index=None):
      """
      Returns a pandas dataframe.
      Requires the use of the header `:colnames no` for preservation of original column names.
      `table` is an org table which is just a list of lists in python.
      `col_name_size` is the number of rows that make up the column names.
      `format_string` is a format string to make the desired column names.
      `index` is a column label or a list of column labels to be set as the index of the dataframe.
      """
      import pandas as pd

      if col_name_size==0:
        return pd.DataFrame(table)
 
      colnames = table[:col_name_size]

      if col_name_size==1:
        if format_string:
          new_colnames = [format_string % x for x in colnames[0]]
        else:
          new_colnames = colnames[0]
      else:
        new_colnames = []
        for colnum in range(len(colnames[0])):
          curr_tuple = tuple([x[colnum] for x in colnames])
          if format_string:
            new_colnames.append(format_string % curr_tuple)
          else:
            new_colnames.append(str(curr_tuple))

      df = pd.DataFrame(table[col_name_size:], columns=new_colnames)
 
      if index:
        df.set_index(index, inplace=True)
    
      return df
    
    def balance_panel(df):
        """Drop households that aren't observed in all rounds."""
        pnl=df.to_panel()
        keep=pnl.loc[list(pnl.items)[0],:,:].dropna(how='any',axis=1).iloc[0,:]
        df=pnl.loc[:,:,keep.index].to_frame(filter_observations=False)
        df.index.names=pd.core.base.FrozenList(['Year','HH'])

        return df

    def drop_missing(X):
        """
        Return tuple of pd.DataFrames in X with any 
        missing observations dropped.  Assumes common index.
        """

        foo=pd.concat(X,axis=1).dropna(how='any')
        assert len(set(foo.columns))==len(foo.columns) # Column names must be unique!

        Y=[]
        for x in X:
            Y.append(foo.loc[:,x.columns])

        return tuple(Y)

    def use_indices(df,idxnames):
        return df.reset_index()[idxnames].set_index(df.index)

    def arellano_robust_cov(X,u):
        rounds=u.index.get_level_values(1).unique() # Periods to cluster by
        if  len(rounds)>1:
            u=u.sub(u.groupby(level='t').mean()) # Take out time averages
            X.sub(X.groupby(level='t').mean())
            Xu=X.mul(u,axis=0)
            if len(X.shape)==1:
                XXinv=np.array([1./(X.T.dot(X))])
            else:
                XXinv=np.linalg.inv(X.T.dot(X))
            Vhat = XXinv.dot(Xu.T.dot(Xu)).dot(XXinv)
        else:
            u=u-u.mean()
            X=X-X.mean()

            Xu=X.mul(u,axis=0)
            if len(X.shape)==1:
                XXinv=np.array([1./(X.T.dot(X))])
            else:
                XXinv=np.linalg.inv(X.T.dot(X))
            Vhat = XXinv.dot(Xu.T.dot(Xu)).dot(XXinv)

        try:
            return pd.DataFrame(Vhat,index=X.columns,columns=X.columns)
        except AttributeError:
            return Vhat


    def ols(x,y,return_se=True,return_v=False,return_e=False):

        x=pd.DataFrame(x) # Deal with possibility that x & y are series.
        y=pd.DataFrame(y)
        N,n=y.shape
        k=x.shape[1]

        # Drop any observations that have missing data in *either* x or y.
        x,y = drop_missing([x,y]) 

        b=np.linalg.lstsq(x,y)[0]

        b=pd.DataFrame(b,index=x.columns,columns=y.columns)

        out=[b.T]
        if return_se or return_v or return_e:

            u=y-x.dot(b)

            # Use SUR structure if multiple equations; otherwise OLS.
            # Only using diagonal of this, for reasons related to memory.  
            S=sparse.dia_matrix((sparse.kron(u.T.dot(u),sparse.eye(N)).diagonal(),[0]),shape=(N*n,)*2) 

            if return_se or return_v:

                # This will be a very large matrix!  Use sparse types
                V=sparse.kron(sparse.eye(n),(x.T.dot(x).dot(x.T)).as_matrix().view(type=np.matrix).I).T
                V=V.dot(S).dot(V.T)

            if return_se:
                se=np.sqrt(V.diagonal()).reshape((x.shape[1],y.shape[1]))
                se=pd.DataFrame(se,index=x.columns,columns=y.columns)

                out.append(se)
            if return_v:
                # Extract blocks along diagonal; return an Nxkxn array
                V={y.columns[i]:pd.DataFrame(V[i*k:(i+1)*k,i*k:(i+1)*k],index=x.columns,columns=x.columns) for i in range(n)} 
                out.append(V)
            if return_e:
                out.append(u)
        return tuple(out)
#+END_SRC

* Expenditure Shares

#+name: agg_shares_and_mean_shares
#+begin_src python :exports none :tangle neediness.py
  import pylab as pl 
  def expenditure_shares(df):

      aggshares=df.groupby(level='t').sum()
      aggshares=aggshares.div(aggshares.sum(axis=1),axis=0).T
      meanshares=df.div(df.sum(axis=1),level='j',axis=0).groupby(level='t').mean().T

      mratio=(np.log(meanshares)-np.log(aggshares))
      sharesdf=pd.Panel({'Mean shares':meanshares,'Agg. shares':aggshares})

      return sharesdf,mratio

  def agg_shares_and_mean_shares(df,figname=None,ConfidenceIntervals=False,ax=None):
      """Figure of log mean shares - log agg shares.

      Input df is a pd.DataFrame of expenditures, ordered by (t,j).

      ConfidenceIntervalues is an optional argument.  
      If True, the returned figure will have 95% confidence intervals.  
      If a float in (0,1) that will be used for the size of the confidence 
      interval instead.
      """

      shares,mratio=expenditure_shares(df)
      meanshares=shares['Mean shares']

      tab=shares.to_frame().unstack()
      tab.sort_values(by=('Agg. shares',meanshares.columns[0]),ascending=False,inplace=True)

      if ax is None:
          fig, ax = pl.subplots()

      mratio.sort_values(by=mratio.columns[0],inplace=True)
      ax.plot(range(mratio.shape[0]),mratio.as_matrix(), 'o')
      ax.legend(mratio.columns,loc=2)
      ax.set_ylabel('Log Mean shares divided by Aggregate shares')

      v=ax.axis()
      i=0
      for i in range(len(mratio)):
          name=mratio.ix[i].name # label of expenditure item

          if mratio.iloc[i,0]>0.2:
              #pl.text(i,mratio.T.iloc[0][name],name,fontsize='xx-small',ha='right')

              # The key option here is `bbox`. 
              ax.annotate(name, xy=(i,mratio.T.iloc[0][name]), xytext=(-20,10), 
                          textcoords='offset points', ha='right', va='bottom',
                          bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3),
                          arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.25', 
                          color='red'),fontsize='xx-small')

          if mratio.iloc[i,0]<-0.2:
              #pl.text(i,mratio.T.iloc[0][name],name,fontsize='xx-small')
              ax.annotate(name, xy=(i,mratio.T.iloc[0][name]), xytext=(20,-10), 
                          textcoords='offset points', ha='left', va='top',
                          bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3),
                          arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.25', 
                          color='red'),fontsize='xx-small')


      if ConfidenceIntervals>0: # Bootstrap some confidence intervals
          if ConfidenceIntervals==1: ConfidenceIntervals=0.95
          current=0
          last=1
          M=np.array([],ndmin=3).reshape((mratio.shape[0],mratio.shape[1],0))
          i=0
          mydf=df.loc[:,mratio.index]
          while np.max(np.abs(current-last))>0.001 or i < 1000:
              last=current
              # Sample households in each  round with replacement
              bootdf=mydf.iloc[np.random.random_integers(0,df.shape[0]-1,df.shape[0]),:]
              bootdf.reset_index(inplace=True)
              bootdf['HH']=range(bootdf.shape[0])
              bootdf.set_index(['Year','HH'],inplace=True)
              shares,mr=expenditure_shares(bootdf)
              M=np.dstack((M,mr.as_matrix()))
              M.sort(axis=2)
              a=(1-ConfidenceIntervals)/2.
              lb= mratio.as_matrix() - M[:,:,np.floor(M.shape[-1]*a)]
              ub=M[:,:,np.floor(M.shape[-1]*(ConfidenceIntervals+a))] - mratio.as_matrix()
              current=np.c_[lb,ub]
              i+=1
          T=mratio.shape[1]
          for t in range(T):
              ax.errorbar(np.arange(mratio.shape[0]),mratio.as_matrix()[:,t],yerr=current[:,[t,t-T]].T.tolist())
              tab[(df.index.levels[0][t],'Upper Int')]=current[:,t-T]
              tab[(df.index.levels[0][t],'Lower Int')]=current[:,t]

      ax.axhline()

      if figname:
          pl.savefig(figname)

      return tab,ax
#+end_src

#+name: group_expenditures
#+begin_src python :noweb yes :tangle neediness.py
def group_expenditures(df,groups):
    myX=pd.DataFrame(index=df.index)
    for k,v in groups.iteritems():
        myX[k]=df[['$x_{%d}$' % i for i in v]].sum(axis=1)
            
    return myX
#+end_src

* Rank 1 SVD with Missing Data

** Rank 1 SVD Approximation to Matrix with Missing Data
This relies on a modification to the interative SVD algorithm =IncPACK=.
#+name: svd_missing
#+begin_src python :noweb no-export :results output :tangle svd_missing.py 
  import numpy as np
  from oct2py import Oct2Py, Oct2PyError
  octave=Oct2Py()
  octave.addpath('../utils/IncPACK/')
  octave.addpath('../utils/nan-3.1.1/')

  def mysvd(X):
      """Wrap np.linalg.svd so that output is "thin" and X=usv.T.
      """
      u,s,vt = np.linalg.svd(X,full_matrices=False)
      s=np.diag(s)
      v = vt.T
      return u,s,v

  def svd_missing(X):
      [u,s,v]=octave.svd_missing(X.as_matrix())
      s=np.matrix(s)
      u=np.matrix(u)
      v=np.matrix(v)
      return u,s,v

#+end_src

Here's an alternative that involves estimating a covariance matrix,
and extracting $U$ and $\Sigma$ from that; then backing out estimated
$V$.  Unlike the previous routine estimates do not depend on the order
of observations.
#+name: eig_svd_missing
#+BEGIN_SRC python
   import numpy as np

   def missing_inner_product(X):
     n,m=X.shape

     if n<m: 
         axis=1
         N=m
     else: 
         axis=0
         N=n

     xbar=X.mean(axis=axis)

     if axis:
         C=(N-1)*X.T.cov()
     else:
         C=(N-1)*X.cov()

     return C + N*np.outer(xbar,xbar)

   def eig_svd_missing(A,max_rank=None):
       P=missing_inner_product(A)

       sigmas,u=np.linalg.eig(P)

       order=np.argsort(-sigmas)
       sigmas=sigmas[order]

       # Truncate rank of representation using Kaiser criterion (positive eigenvalues)
       u=u[:,order]
       u=u[:,sigmas>0]
       s=np.sqrt(sigmas[sigmas>0])

       if max_rank is not None and len(s) > max_rank:
           u=u[:,:max_rank]
           s=s[:max_rank]

       r=len(s)
       us=np.matrix(u)*np.diag(s)

       v=np.zeros((len(s),A.shape[1]))
       for j in xrange(A.shape[1]):
           a=A.iloc[:,j].as_matrix().reshape((-1,1))
           x=np.nonzero(~np.isnan(a))[0] # non-missing elements of vector a
           if len(x)>=r:
               v[:,j]=(np.linalg.pinv(us[x,:])*a[x]).reshape(-1)
           else:
               v[:,j]=np.nan

       return np.matrix(u),s,np.matrix(v).T
#+END_SRC


#+name: svd_rank1_approximation_with_missing_data
#+begin_src python :noweb no-export :results output :tangle neediness.py
  import pandas as pd
  <<svd_missing>>
  <<eig_svd_missing>>

  def eig_svd_rank1_approximation_with_missing_data(x,return_usv=False,max_rank=None,VERBOSE=True):
      """
      Return rank 1 approximation to a pd.DataFrame x, where x may have
      elements which are missing.
      """
      x=x.copy()
      m,n=x.shape

      if n<m:  # If matrix 'thin', make it 'short'
          x=x.T
          TRANSPOSE=True
      else:
          TRANSPOSE=False

      x=x.dropna(how='all',axis=1) # Drop any column which is /all/ missing.
      x=x.dropna(how='all',axis=0) # Drop any row which is /all/ missing.

      u,s,v=eig_svd_missing(x,max_rank=max_rank)
      if VERBOSE:
          print "Estimated singular values: ",
          print s

      xhat=pd.DataFrame(v[:,0]*s[0]*u[:,0].T,columns=x.index,index=x.columns).T

      if TRANSPOSE: xhat=xhat.T

      if return_usv:
          return xhat,u,s,v
      else: return xhat

  def svd_rank1_approximation_with_missing_data(x,return_usv=False,VERBOSE=True,MISSLAST=True,svd_missing=svd_missing): 
      """
      Return rank 1 approximation to a pd.DataFrame x, where x may have
      elements which are missing.
      """
      x=x.copy()
      m,n=x.shape

      if n<m:  # If matrix 'thin', make it 'short'
          x=x.T
          TRANSPOSE=True
      else:
          TRANSPOSE=False

      x=x.dropna(how='all',axis=1) # Drop any column which is /all/ missing.
      x=x.dropna(how='all',axis=0) # Drop any row which is /all/ missing.
    

      if MISSLAST:
          y=x.T.copy() # y "thin"
          if 'xxxxindexxxxx' in y.columns: raise(ValueError)
          y['xxxxindexxxxx']=range(y.shape[0])
          if 'xxxxcountxxxx' in y.columns: raise(ValueError)
          y['xxxxcountxxxx']=y.T.count()
          y=y.sort_values(by='xxxxcountxxxx',ascending=False)
          x=y[y.columns[:-2]].T # x short

      u,s,v=svd_missing(x)
      if VERBOSE:
          print "Estimated singular values: ",
          print s

      xhat=pd.DataFrame(v[:,0]*s[0]*u[:,0].T,columns=x.index,index=x.columns).T

      if MISSLAST: # Restore orginal ordering
          xhat=xhat.T
          xhat['xxxxindexxxxx']=y['xxxxindexxxxx']
          xhat.sort_values(by='xxxxindexxxxx',inplace=True)
          xhat=xhat.iloc[:,:-1]
          xhat=xhat.T

      if TRANSPOSE: xhat=xhat.T

      if return_usv:
          return xhat,u,s,v
      else: return xhat
#+end_src

*** Order matters when data is missing!
By moving observations with  more missing data to the "end" of the
thin matrix, we (invariably?) obtain better approximations if we use
the updating SVD approach.

#+BEGIN_SRC python :var percent_missing=0.2 :var N=20 :tangle foo.py
  <<df_utils>>
  <<svd_rank1_approximation_with_missing_data>>

  def random_rank1_matrix(n=100,m=2):
      a=np.random.random(size=(n,1))
      a=a/np.linalg.norm(a)
      b=np.random.random(size=(m,1)).T
      b=b/np.linalg.norm(b)

      return a.dot(b)

  np.random.seed(0)
  d=[]
  
  for i in range(N):
      #print i
      X0=pd.DataFrame(random_rank1_matrix(n=100,m=4))
      X=X0.copy()
      X.iloc[np.random.random_sample(X.shape)<percent_missing]=np.nan

      Xhat0=svd_rank1_approximation_with_missing_data(X,return_usv=False,VERBOSE=False,MISSLAST=False)
      Xhat1=svd_rank1_approximation_with_missing_data(X,return_usv=False,VERBOSE=False,MISSLAST=True)

      d.append(df_norm(Xhat0-X0)/np.linalg.norm(X0) - df_norm(Xhat1-X0)/np.linalg.norm(X0))

  d=np.array(d)
  print "Proportion (out of %d) for which approximation is better with missing values last is %5.4f." % (N,np.mean(d>0))

#+END_SRC

#+results:
: Proportion (out of 20) for which approximation is better with missing values last is 1.0000.

    
*** Test of Rank 1 SVD Approximation to Matrix with Missing Data

First, some code to check if approximation works for a simple, small
scale example.

#+name: svd_rank1_approximation_with_missing_data_example
#+begin_src python :noweb no-export :results output :var MISSLAST=1 :tangle svd_rank1_approximation_with_missing_data_example.py
import numpy as np
import pandas as pd
<<svd_rank1_approximation_with_missing_data>>

(n,m)=(3,5)
a=np.random.normal(size=(n,1))
b=np.random.normal(size=(1,m))
e=np.random.normal(size=(n,m))*1e-2

X0=np.array([[-0.22,  0.32, -0.43],
             [0.01, 0.00,  0.00],
             [-0.22,  0.31, -0.42],
             [0.01, -0.03,  0.04],
             [-0.21, 0.31, -0.38]])

X0=X0-X0.mean(axis=1).reshape((-1,1))

X=X0.copy()
X[0,0]=np.nan
X[0,1]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

Xhat=svd_rank1_approximation_with_missing_data(X,VERBOSE=False,MISSLAST=MISSLAST)

print X
print X0
print Xhat
print "MISSLAST=%s" % MISSLAST
#+end_src

#+results: svd_rank1_approximation_with_missing_data_example
#+begin_example
      0     1     2     3     4
0   NaN  0.01 -0.22  0.01 -0.21
1   NaN  0.00  0.31 -0.03  0.31
2 -0.43  0.00 -0.42  0.04 -0.38
      0     1     2     3     4
0 -0.22  0.01 -0.22  0.01 -0.21
1  0.32  0.00  0.31 -0.03  0.31
2 -0.43  0.00 -0.42  0.04 -0.38
          0         1         2         3         4
0 -0.223967  0.001319 -0.217683  0.019239 -0.204965
1  0.323213 -0.001904  0.314145 -0.027764  0.295791
2 -0.429781  0.002532 -0.417723  0.036918 -0.393317
MISSLAST=1
#+end_example

#+name: svd_rank1_approximation_with_missing_data_test
#+begin_src python :noweb no-export :results output :var n=12 :var m=2000 :var percent_missing=0.5 :var SEED=0 :var MISSLAST=0 :tangle svd_rank1_approximation_with_missing_data_test.py
import numpy as np
import pandas as pd
<<svd_rank1_approximation_with_missing_data>>

if SEED:
    np.random.seed(SEED)

a=np.random.normal(size=(n,1))
b=np.random.normal(size=(1,m))
e=np.random.normal(size=(n,m))*1e1

X0=np.outer(a,b)
X0=X0-X0.mean(axis=0)

X=X0.copy()
X[np.random.random_sample(X.shape)<percent_missing]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

Xhat,u,s,v=svd_rank1_approximation_with_missing_data(X,VERBOSE=False,MISSLAST=MISSLAST,return_usv=True)
Xhat1,u1,s1,v1=eig_svd_rank1_approximation_with_missing_data(X,VERBOSE=False,return_usv=True)

rho=pd.concat([X.stack(dropna=False),Xhat.stack()],axis=1).corr().iloc[0,1]
rho1=pd.concat([X.stack(dropna=False),Xhat1.stack()],axis=1).corr().iloc[0,1]
missing=np.isnan(X.as_matrix()).reshape(-1,1).mean()
print "Proportion missing %g and correlations are %5.4f and %5.4f" % (missing, rho,rho1),
print "MISSLAST=%s" % MISSLAST,
print "Singular value=%g" % s,
print "Singular value=%g" % s1[0],
if SEED: print "Seed=%g" % SEED
else: print
#+end_src

#+results: svd_rank1_approximation_with_missing_data_test

#+CALL: svd_rank1_approximation_with_missing_data_test(percent_missing=0.85,SEED=21,MISSLAST=0) :results prepend

#+results:


#+BEGIN_SRC python :var percent_missing=0.6 :var MISSLAST=1 :tangle baz.py
(n,m)=(12,1000)

SEED=0
rho=1
lowrho=(2,SEED)
while rho>0: 
    SEED+=1
    <<svd_rank1_approximation_with_missing_data_test>>
    if rho<lowrho[0]: lowrho=(rho,SEED)

#+END_SRC

#+BEGIN_SRC python :var percent_missing=0.6 :var MISSLAST=1 :tangle foo.py

SEED=17

(n,m)=(4,8)

<<svd_rank1_approximation_with_missing_data_test>>

#+END_SRC

#+results:
: Proportion missing 0.5625 and correlation 0.4696 MISSLAST=1 Singular value=8.15501 Seed=17

: Proportion missing 0.53125 and correlation 0.0389 MISSLAST=1 Singular value=13.2579 Seed=80





*** Test of construction of approximation to CE
#+begin_src python  :noweb no-export :results output :tangle test.py
  import numpy as np
  <<estimate_reduced_form>>
  <<artificial_data>>
  <<df_utils>>
  <<svd_rank1_approximation_with_missing_data>>

  y,truth=artificial_data(T=1,N=1000,n=12,sigma_e=1e-1)
  #y,truth=artificial_data(T=2,N=20,n=6,sigma_e=1e-8)
  beta,L,dz,p=truth

  numeraire='x0'

  b0,ce0,d0=estimate_bdce_with_missing_values(y,np.log(dz),return_v=False)
  myce0=ce0.copy()
  cehat=eig_svd_rank1_approximation_with_missing_data(myce0)

  rho=pd.concat([ce0.stack(dropna=False),cehat.stack()],axis=1).corr().iloc[0,1]

  print "Norm of error in approximation of CE: %f; Correlation %f." % (df_norm(cehat,ce0)/df_norm(ce0),rho)
#+end_src

#+results:

* Estimation of reduced form
#+name: estimate_reduced_form
#+BEGIN_SRC python :noweb no-export :results output :tangle neediness.py
  import pandas as pd
  import warnings
  import sys
  from collections import OrderedDict

  <<df_utils>>

  def estimate_bdce_with_missing_values(y,z,market=None,prices=None,return_v=False,return_se=False):
      """Estimate reduced form objects b, d, and ce.  

      Inputs are log expenditures and household characteristics (both in
      logs).  Both must be pd.DataFrames.

      The optional variable market is a series which identifies locations
      (e.g, rural/urban)  which may be thought to have different prices.
      In this case different latent price variables are estimated for
      different regions. 

      The optional variable prices is a df of prices for (possibly
      selected) goods.  Where supplied these (logged) price data will be
      used in lieu of a latent variable approach.

      Ethan Ligon                                            April,  2016
      """
      n,N,T=y.to_panel().shape

      b=OrderedDict()
      d=OrderedDict()
      a=OrderedDict()
      myE=OrderedDict()
      sed=OrderedDict()
      V=OrderedDict()

      years=[year for year in y.index.levels[1]]

      Timed=pd.get_dummies(use_indices(z,['t'])['t'])

      for i in range(n):
          myy,myz=drop_missing([y.iloc[:,[i]],z])
          # Calculate a within transformation
          Wy=myy-myy.mean()
          Wy=Wy-Wy.mean()

          Wz=myz-myz.mean(axis=0)
          Wz=Wz-Wz.mean(axis=0)

          if not market is None:
              foo=pd.Series([tuple(x) for x in pd.concat([use_indices(Wz,['t'])['t'],market],axis=1,join='inner').as_matrix().tolist()],index=Wz.index,name='Period-Market')
              timed=pd.get_dummies(foo)
          else:
              timed=pd.get_dummies(use_indices(Wz,['t'])['t'])

          years = [x for x in timed.columns]

          USE_PRICE=False
          if (prices is not None) and (y.columns[i] in prices.index):
              USE_PRICE=True
              timed=np.log(prices.iloc[i,:])

          Wtimed=timed-timed.mean() # Don't forget within transformation of time dummies! 
          Wtimed=Wtimed-Wtimed.mean()  # First de-meaning can be improved upon

          if not USE_PRICE:
              # Need to make sure time-market effects sum to zero
              ynil=pd.DataFrame([0],index=[(-1,0)],columns=Wy.columns)
              znil=pd.DataFrame([[0]*Wz.shape[1]],index=[(-1,0)],columns=Wz.columns)
              timednil=pd.DataFrame([[1]*timed.shape[1]],index=[(-1,0)],columns=timed.columns)

              X=Wz.append(znil).join(Wtimed.append(timednil))
              # Estimate d & b
              myb,mye=ols(X,Wy.append(ynil),return_se=False,return_v=False,return_e=True) # Need version of pandas >0.14.0 (?) for this use of join
          else:
              X=Wz.join(Wtimed)
              myb,mye=ols(X,Wy,return_se=False,return_v=False,return_e=True) # Need version of pandas >0.14.0 (?) for this use of join

          #mye=mye.iloc[:-1,:] # Drop constraint
          if return_v or return_se:
              myV=arellano_robust_cov(X,mye.iloc[:,0])
              myse=np.sqrt(np.diag(myV))

          for year in years:
              if year not in myb.columns:
                  myb[year]=np.NaN 

          myb=myb[z.columns.tolist()+years]

          d[y.columns[i]]=myb.iloc[:,:Wz.shape[1]].as_matrix()[0] # reduced form coefficients on characteristics
          if return_se: # Get std. errs for characteristics
              sed[y.columns[i]]=myse # reduced form se on characteristics

          b[y.columns[i]]=myb.iloc[:,Wz.shape[1]:].as_matrix()[0] # Terms involving prices
          a[y.columns[i]] = (myy.mean() - d[y.columns[i]].dot(myz.mean(axis=0)) - b[y.columns[i]].dot(timed.mean().as_matrix())).as_matrix()[0]

          #myce[y.columns[i]] = pd.Series((myy - a[y.columns[i]]).as_matrix().reshape(-1) - myz.as_matrix().dot(d[y.columns[i]]) - timed.as_matrix().dot(b[y.columns[i]]),index=myy.index)
          myE[y.columns[i]] = mye.iloc[:-1,:]  # Drop constraint
          V[y.columns[i]] = myV


      d=pd.DataFrame(d,index=z.columns).T

      if return_se: # Get std. errs for characteristics
          sed=pd.DataFrame(sed,index=X.columns).T[z.columns]

      b=pd.DataFrame(b,index=years)
      b.T.index.name='t'
      b=b.T

      a=pd.DataFrame(a,columns=y.columns,index=['Constant']).T['Constant']

      #ce0 = y - a - z.dot(d.T) - Timed.dot(b.T) #  Should be equal to ce if no prices
      ce=pd.concat(myE.values(),axis=1)

      assert np.abs(ce.unstack().mean()).sum() < 1e-10

      out = [b.add(a,axis=0),ce,d]
      if return_se:
          out += [sed]
      if return_v:
          V = pd.Panel(V,major_axis=X.columns,minor_axis=X.columns)
          out += [V]
      return out
#+end_src

#+results: estimate_reduced_form

* TODO Extraction of Elasticities and Neediness
#+name: get_loglambdas
#+begin_src python :noweb no-export :results output :tangle neediness.py
  import pandas as pd

  def get_loglambdas(e,TEST=False,max_rank=1):
      """
      Use singular-value decomposition to compute loglambdas and price elasticities,
      up to an unknown factor of proportionality phi.

      Input e is the residual from a regression of log expenditures purged
      of the effects of prices and household characteristics.   The residuals
      should be arranged as a matrix, with columns corresponding to goods. 
      """ 

      assert(e.shape[0]>e.shape[1]) # Fewer goods than observations

      #chat = svd_rank1_approximation_with_missing_data(e,VERBOSE=False,MISSLAST=True)
      chat = eig_svd_rank1_approximation_with_missing_data(e,VERBOSE=False,max_rank=max_rank)

      R2 = chat.var()/e.var()

      # Possible that initial elasticity b_i is negative, if inferior goods permitted.
      # But they must be positive on average.
      if chat.iloc[0,:].mean()>0:
          b=chat.iloc[0,:]
      else:
          b=-chat.iloc[0,:]

      loglambdas=(-chat.iloc[:,0]/b.iloc[0])

      # Find phi that normalizes first round loglambdas
      phi=loglambdas.groupby(level='t').std().iloc[0]
      loglambdas=loglambdas/phi

      bphi=pd.Series(b*phi,index=e.columns)

      if TEST:
          foo=-np.outer(bphi,loglambdas).T
          assert np.linalg.norm(foo-chat)<1e-4
          #print "blogL norm: %f" % np.linalg.norm(foo-chat)

      return bphi,loglambdas

  def iqr(x):
      """The interquartile range of a pd.Series of observations x."""
      import numpy as np
      return x.quantile([0.25,0.75]).diff().iloc[1]

  def bootstrap_elasticity_stderrs(e,tol=1e-4,minits=30,return_samples=False,VERBOSE=False,outfn=None,TRIM=True):
      """Bootstrap estimates of standard errors for \phi\beta.

      Takes pd.DataFrame of residuals as input.

      If optional parameter TRIM is True, then calculations are
      performed using the interquartile range (IQR) instead of the
      standard deviation, with the standard deviation computed as
      IQR*0.7416 (which is a good approximation provided the
      distribution is normal).

      Ethan Ligon                              January 2017
      """
      bhat,Lhat=get_loglambdas(e)

      if outfn: outf=open(outfn,'a')

      delta=1.
      old=np.array(0)
      new=np.array(1)
      i=1
      L=[]
      while delta>tol or i < minits:
          delta=np.nanmean(np.abs(old.reshape(-1)-new.reshape(-1)))
          if VERBOSE and (i % 2)==0: 
              print i, delta
          if delta>0.1:
              print "Large delta=%g" % delta
          old=new
          S=e.iloc[np.random.random_integers(0,e.shape[0]-1,size=e.shape[0]),:]
          S=S-S.mean() 

          try:
              bs,ls=get_loglambdas(S)
              try:
                  B=B.append(bs,ignore_index=True)
              except NameError:
                  B=pd.DataFrame(bs).T # Create B

              L.append(ls)

              if TRIM:
                  new=B.apply(iqr)*0.7416 # Estimate of standard deviation, with trimming
              else:
                  new=B.std()
            
              if outfn: outf.write(','.join(['%6.5f' % b for b in bs])+'\n')
              i+=1
          except Oct2PyError: # Octpy sometimes throws mysterious error?
              warnings.warn("Mysterious octpy problem.")
              new=np.array(-1)
              pass

      outf.close()
      if return_samples:
          return new,B
      else:
          return new


#+end_src
*** Test of get_loglambdas
#+name: test_get_loglambdas
#+begin_src python :noweb no-export :results output :var miss_percent=0.6 :tangle test_get_loglambdas.py
import numpy as np
import pandas as pd
<<get_loglambdas>>
<<svd_rank1_approximation_with_missing_data>>
<<df_utils>>

(n,m)=(50,5000)
a=np.random.random_sample((n,1))
b=np.random.random_sample((1,m))
e=np.random.random_sample((n,m))*1e-5

X0=np.outer(a,b)+e

X=X0.copy()
X[np.random.random_sample(X.shape)<miss_percent]=np.nan

X0=pd.DataFrame(X0).T
X=pd.DataFrame(X).T

ahat,bhat=get_loglambdas(X,TEST=True)

Xhat=pd.DataFrame(np.outer(pd.DataFrame(ahat),pd.DataFrame(-bhat).T).T)

print "Norm of error (svd vs. truth): %f" % (df_norm(Xhat,X)/df_norm(X))
#+end_src

#+results: test_get_loglambdas


: blogL norm: 0.000000
: Norm of error (svd vs. truth): 0.000089
: Norm of error (averaging vs. truth): 0.000101
: Norm of error (averaging vs. svd): 0.000048

#+CALL: test_get_loglambdas(miss_percent=0.)

#+results:
: blogL norm: 0.000000
: Norm of error (svd vs. truth): 0.000010
: Norm of error (averaging vs. truth): 0.000011
: Norm of error (averaging vs. svd): 0.000004

#+CALL: test_get_loglambdas(miss_percent=0.1)

#+results:
: blogL norm: 0.000000
: Norm of error (svd vs. truth): 0.000010
: Norm of error (averaging vs. truth): 0.058786
: Norm of error (averaging vs. svd): 0.059439

#+CALL: test_get_loglambdas(miss_percent=0.2)

#+results:
: blogL norm: 0.000000
: Norm of error (svd vs. truth): 0.000008
: Norm of error (averaging vs. truth): 0.073570
: Norm of error (averaging vs. svd): 0.074247

*** Tuning truncation parameter for missing-value svds

By extracting estimates of \phi\beta from the covariance matrix, we
ensure that those estimates depend on missing data only to the extent
that the covariance matrix itself depends on  those data.  If we
assume that data is missing at random, then the naive estimator
implemented in =pandas= will be consistent, though in finite samples
it may not be positive definite.  

Although estimates of \phi\beta are thus likely to be reasonably
robust to missing data, the same cannot be said of our estimates of
\log\lambda. 
#+BEGIN_SRC python :tangle missing_svd_tuning.py
  <<svd_rank1_approximation_with_missing_data>>
  <<get_loglambdas>>

  x=pd.read_pickle('../Results/Uganda/Eig/ce.df') # Residuals from reduced form

  B=[pd.read_pickle('../Results/Uganda/goods.df')[r'$\phi\beta_i$']]
  L=[pd.read_pickle('../Results/Uganda/loglambda.df').stack()]
  for r in range(1,11):
      print "Max rank for missing inference: %d" % r
      b,l=get_loglambdas(x,TEST=True,max_rank=r)
      B.append(b)
      L.append(l)
  
  L=pd.concat(L,axis=1,names=range(11))
  B=pd.concat(B,axis=1,names=range(11))

#+END_SRC
